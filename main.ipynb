{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08427a84-31f9-4bcf-875e-22b2e6497608",
   "metadata": {
    "user_expressions": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/IsoonAI/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import os\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "879be4cb",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkout git\n",
      "kernel is working\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Paths and model settings\\ nZIP_PATH = 'I-Soon-data.zip'      # Path to your downloaded zip file\n",
    "ZIP_PATH = '0.zip'    # Directory to extract contents\n",
    "EXTRACT_DIR = 'I-Soon-data'        # Directory to extract contents\n",
    "print(\"Checkout git\")\n",
    "print(\"kernel is working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e6feb79",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted archive to 'I-Soon-data'\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(EXTRACT_DIR):\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_DIR)\n",
    "    print(f\"Extracted archive to '{EXTRACT_DIR}'\")\n",
    "else:\n",
    "    print(f\"Extraction directory '{EXTRACT_DIR}' already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a01d8",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# **DATA TYPE CATEGORIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dec65000",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: I-Soon-data/__MACOSX\n"
     ]
    }
   ],
   "source": [
    "# Define the parent directory\n",
    "parent_directory = \"I-Soon-data\"\n",
    "\n",
    "# Full path to the __MACOSX folder\n",
    "macosx_folder = os.path.join(parent_directory, \"__MACOSX\")\n",
    "\n",
    "# Check if __MACOSX exists and remove it\n",
    "if os.path.exists(macosx_folder) and os.path.isdir(macosx_folder):\n",
    "    shutil.rmtree(macosx_folder)\n",
    "    print(f\"Deleted: {macosx_folder}\")\n",
    "else:\n",
    "    print(f\"Folder not found: {macosx_folder}\")\n",
    "\n",
    "# Organize files by extension into subfolders\n",
    "for root, dirs, files in os.walk(parent_directory):\n",
    "    for file in files:\n",
    "        # Skip hidden files and __MACOSX if any reappear\n",
    "        if file.startswith('.') or '__MACOSX' in root:\n",
    "            continue\n",
    "\n",
    "        # Get the file extension (in lowercase, without the dot)\n",
    "        file_extension = os.path.splitext(file)[1].lower().lstrip('.')\n",
    "        if not file_extension:\n",
    "            file_extension = \"no_extension\"\n",
    "\n",
    "        # Define the new subfolder path\n",
    "        subfolder_path = os.path.join(parent_directory, file_extension)\n",
    "\n",
    "        # Create the subfolder if it doesn't exist\n",
    "        os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "        # Define source and destination paths\n",
    "        source_path = os.path.join(root, file)\n",
    "        destination_path = os.path.join(subfolder_path, file)\n",
    "\n",
    "        # Move the file if source and destination are not the same\n",
    "        if os.path.abspath(source_path) != os.path.abspath(destination_path):\n",
    "            shutil.move(source_path, destination_path)\n",
    "\n",
    "# Remove any empty folders within the parent directory\n",
    "for dirpath, dirnames, filenames in os.walk(parent_directory, topdown=False):\n",
    "    if not dirnames and not filenames:\n",
    "        try:\n",
    "            os.rmdir(dirpath)\n",
    "            print(f\"Removed empty folder: {dirpath}\")\n",
    "        except OSError:\n",
    "            pass  # Ignore errors (e.g., if directory is not empty due to permissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75417f54",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# **Markdown File Classification Using Local LLM (Ollama + LangChain)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9397264",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying files: 100%|██████████| 70/70 [05:53<00:00,  5.04s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import concurrent.futures\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Step 1: Set up LLM and Prompt ===\n",
    "\n",
    "llm = Ollama(model=\"llama3.1:8b\")\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"\"\"\n",
    "You are analyzing the content of a Markdown (.md) file.\n",
    "\n",
    "Markdown content:\n",
    "\\\"\\\"\\\"\n",
    "{content}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "1. Classify the content into one of the following categories ONLY: chats, images, other.\n",
    "2. State your confidence in the classification as one of: high, medium, or low.\n",
    "\n",
    "Respond in the following format:\n",
    "Category: <chats|images|other>\n",
    "Confidence: <high|medium|low>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# === Step 2: Preprocessing Function ===\n",
    "\n",
    "def preprocess_first_20_lines(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [line.strip() for _, line in zip(range(20), f)]\n",
    "            content = \" \".join(lines)\n",
    "        return os.path.basename(file_path), content\n",
    "    except Exception:\n",
    "        return os.path.basename(file_path), \"\"\n",
    "\n",
    "# === Step 3: Load .md Files ===\n",
    "\n",
    "md_dir = \"I-Soon-data/md\"\n",
    "md_files = glob.glob(os.path.join(md_dir, \"*.md\"))\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    file_data = list(executor.map(preprocess_first_20_lines, md_files))\n",
    "\n",
    "# === Step 4: Classify Each File and Move ===\n",
    "\n",
    "valid_categories = {\"chats\", \"images\", \"other\"}\n",
    "results_log = []\n",
    "\n",
    "for file_name, content in tqdm(file_data, desc=\"Classifying files\"):\n",
    "    if not content:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        response = chain.run(content=content).strip().lower()\n",
    "        lines = response.splitlines()\n",
    "\n",
    "        category = next((line.replace(\"category:\", \"\").strip() for line in lines if line.startswith(\"category:\")), \"\")\n",
    "        confidence = next((line.replace(\"confidence:\", \"\").strip() for line in lines if line.startswith(\"confidence:\")), \"\")\n",
    "\n",
    "        if category not in valid_categories:\n",
    "            category = \"other\"\n",
    "\n",
    "        # Destination folder *within* the md_dir\n",
    "        category_path = os.path.join(md_dir, category)\n",
    "        os.makedirs(category_path, exist_ok=True)\n",
    "\n",
    "        # Move file into category folder\n",
    "        src_path = os.path.join(md_dir, file_name)\n",
    "        dst_path = os.path.join(category_path, file_name)\n",
    "\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.move(src_path, dst_path)\n",
    "\n",
    "        results_log.append({\n",
    "            \"file\": file_name,\n",
    "            \"category\": category,\n",
    "            \"confidence\": confidence\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_name}: {e}\")\n",
    "\n",
    "# === Optional: Save results to file ===\n",
    "\n",
    "# import json\n",
    "# with open(\"classification_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(results_log, f, indent=2)\n",
    "\n",
    "# import csv\n",
    "# with open(\"classification_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "#     writer = csv.DictWriter(f, fieldnames=[\"file\", \"category\", \"confidence\"])\n",
    "#     writer.writeheader()\n",
    "#     writer.writerows(results_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fe507e",
   "metadata": {},
   "source": [
    "# **Stage 2: LLM-Based Categorization and File Organization by Description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29857281",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.chains import LLMChain\n",
    "# from langchain.llms import Ollama\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Initialize second LLM\n",
    "# llm2 = Ollama(model=\"taozhiyuai/llama-3-refueled:q4_k_m\")\n",
    "\n",
    "# # Prompt for classification based on description\n",
    "# prompt_template_stage2 = PromptTemplate(\n",
    "#     input_variables=[\"description\"],\n",
    "#     template=\"\"\"\n",
    "# You are a strict content classifier.\n",
    "\n",
    "# Given the following short description of a Markdown (.md) file:\n",
    "\n",
    "# \\\"\\\"\\\"\n",
    "# {description}\n",
    "# \\\"\\\"\\\"\n",
    "\n",
    "# Classify the content into one of these categories only:\n",
    "# - chats\n",
    "# - images\n",
    "# - other\n",
    "\n",
    "# Return only one of those three exact words (in lowercase). Do not use synonyms or explanations. Do not make up new categories.\n",
    "# \"\"\"\n",
    "# )\n",
    "\n",
    "# # Set up LangChain chain\n",
    "# chain2 = LLMChain(llm=llm2, prompt=prompt_template_stage2)\n",
    "\n",
    "# # Directories\n",
    "# original_dir = \"I-Soon-data/md\"\n",
    "# filtered_dir = \"Filtered-markdowns\"\n",
    "# os.makedirs(filtered_dir, exist_ok=True)\n",
    "\n",
    "# # Allowed categories\n",
    "# valid_categories = {\"chats\", \"images\", \"other\"}\n",
    "\n",
    "# # Reclassify and copy files\n",
    "# for file_name, description in tqdm(results.items()):\n",
    "#     try:\n",
    "#         category = chain2.run(description=description).strip().lower()\n",
    "#         if category not in valid_categories:\n",
    "#             category = \"other\"  # fallback to default\n",
    "\n",
    "#         # Create destination folder\n",
    "#         category_path = os.path.join(filtered_dir, category)\n",
    "#         os.makedirs(category_path, exist_ok=True)\n",
    "\n",
    "#         # Copy the file\n",
    "#         src_path = os.path.join(original_dir, file_name)\n",
    "#         dst_path = os.path.join(category_path, file_name)\n",
    "\n",
    "#         if os.path.exists(src_path):\n",
    "#             shutil.copy2(src_path, dst_path)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to classify or copy {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c72754",
   "metadata": {},
   "source": [
    "# **Finding connetions between the MD files - reduced size due to performance issues**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d0463",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Idea: Have the LLM search the markdown files and look for any files linked outside the chats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf991dca",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# *RegEx based identification of linked files within the chat*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff0e93fe",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 LLM-generated file extensions list:\n",
      "['docx', 'ods', 'odt', 'pdf', 'jpg', 'jpeg', 'png', 'gif', 'bmp', 'txt', 'log', 'cfg', 'ini', 'reg', 'dbf', 'mdb', 'accdb', 'sqlite', 'sql', 'bak', 'mdf', 'ndf', 'ldf', 'db', 'mdb', 's3db', 'xml', 'json', 'yaml', 'csv', 'xls', 'xlsx', 'xlsm', 'xlt', 'ppt', 'pptx', 'doc', 'dot', 'rtf', 'msg', 'eml', 'vcf', 'jpg', 'jpeg', 'tiff', 'tif', 'gif', 'bmp', 'pcapng', 'pcap', 'csv', 'json', 'ods', 'xlsx', 'xlsm', 'zip', 'rar', '7z', 'tar', 'gz', 'bz2']\n",
      "\n",
      "📄 Extracted filenames from chats:\n",
      "1.md:\n",
      "  - 安洵项目-投资意向书-200730.DOCX\n",
      "  - 课程表2020_西安_内版.docx\n",
      "  - c404_indictment_0.docx\n",
      "  - wong_indictment_redacted_0.docx\n",
      "  - zhr_indictment_redacted_0.docx\n",
      "  - 产教融合建设项目申报书20201028.docx\n",
      "  - tpyrced_打击整治涉网犯罪技术服务合同0201-大理.docx\n",
      "  - 202063简报-模板展示.pdf\n",
      "  - 比特信安数据脱敏系统saas服务业务模式主打胶片.pdf\n",
      "  - 个人简历表-徐子译.docx\n",
      "  - 太极公司基本情况介绍20210809.ppt\n",
      "  - 20220110.doc\n",
      "  - -2021年1月版.pdf\n",
      "  - 市场对外2022-海南安洵.ppt\n",
      "  - 四川两地项目合作框架协议.pdf\n",
      "  - 四川两地项目合作框架协议.pdf\n",
      "  - 安洵信息20220228.pdf\n",
      "  - 前端销售反馈问题.doc\n",
      "  - 一体化安全项目初步设计文档2b723eaa7742869ff370fd42f5b572f8.pdf\n",
      "  - 销售体系队伍调整方案.docx\n",
      "  - 特殊人群智慧管理产品简介2022.pdf\n",
      "  - 司法智慧矫正产品介绍2022.pdf\n",
      "  - v1.0.ppt.pdf\n",
      "  - 智慧矫正产品报价单202206-海南-市场指导价.pdf\n",
      "19.md:\n",
      "  - 6848748d-2881-4c26-b153-fcd5373d2f1c.png\n",
      "  - 0-6848748d-2881-4c26-b153-fcd5373d2f1c.png\n",
      "4.md:\n",
      "  - 330f554f-a3e6-4bd3-8b1b-d5949e1f30e8.png\n",
      "  - 0-330f554f-a3e6-4bd3-8b1b-d5949e1f30e8.png\n",
      "  - dd5b6a38-dc17-4122-a242-32006b381b3a.png\n",
      "  - 0-dd5b6a38-dc17-4122-a242-32006b381b3a.png\n",
      "  - 62ff30cf-de5f-4388-82aa-b69b0fd0f07c.png\n",
      "  - 0-62ff30cf-de5f-4388-82aa-b69b0fd0f07c.png\n",
      "  - 越南人民公安电视ANTV.docx\n",
      "  - f41b7574-57b4-4c9f-907c-2a3c48a56157.png\n",
      "  - 0-f41b7574-57b4-4c9f-907c-2a3c48a56157.png\n",
      "  - bcad4fdf-3771-4873-92fa-23240654118a.png\n",
      "  - 0-bcad4fdf-3771-4873-92fa-23240654118a.png\n",
      "  - fc27ce32-9c96-416c-9c38-84977255e0ba.png\n",
      "  - 0-fc27ce32-9c96-416c-9c38-84977255e0ba.png\n",
      "22.md:\n",
      "  - b0a4acaa-d768-4f6d-8e54-6d20f271bb7c.png\n",
      "  - 0-b0a4acaa-d768-4f6d-8e54-6d20f271bb7c.png\n",
      "  - 493542fc-495f-4756-8451-c4ed084d8bf7.png\n",
      "  - 0-493542fc-495f-4756-8451-c4ed084d8bf7.png\n",
      "26.md:\n",
      "  - 前端销售反馈问题.doc\n",
      "  - 前端销售反馈问题.doc\n",
      "36.md:\n",
      "  - 手机购彩分析简报.docx\n",
      "13.md:\n",
      "  - 987ba39a-cc1c-4367-8d6d-f5a49a940198.png\n",
      "  - 0-987ba39a-cc1c-4367-8d6d-f5a49a940198.png\n",
      "  - adaf869e-920a-4a17-91bd-e2ef3125c10e.png\n",
      "  - 0-adaf869e-920a-4a17-91bd-e2ef3125c10e.png\n",
      "  - 5ae9bdca-fdf9-4948-8c11-a9e400b331aa.png\n",
      "  - 0-5ae9bdca-fdf9-4948-8c11-a9e400b331aa.png\n",
      "  - 1b0dc208-d2bb-43ea-b744-534f3b759394.png\n",
      "  - 0-1b0dc208-d2bb-43ea-b744-534f3b759394.png\n",
      "  - 3556e54c-d418-447d-bb2a-43ac0408cc7a.png\n",
      "  - 0-3556e54c-d418-447d-bb2a-43ac0408cc7a.png\n",
      "37.md:\n",
      "  - 太极公司基本情况介绍20210809.ppt\n",
      "23.md:\n",
      "  - 微软邮件密取平台.7z\n",
      "  - 202301090144.pdf\n",
      "24.md:\n",
      "  - 96af60b3-299c-4e26-bca3-d9eb3e113b94.png\n",
      "  - 0-96af60b3-299c-4e26-bca3-d9eb3e113b94.png\n",
      "  - fcf90a92-794c-40c6-aa4f-8ea82f8bed51.png\n",
      "  - 0-fcf90a92-794c-40c6-aa4f-8ea82f8bed51.png\n",
      "  - 1afcf93d-50f1-4f1e-896d-87b0da7519f7.png\n",
      "  - 0-1afcf93d-50f1-4f1e-896d-87b0da7519f7.png\n",
      "  - fe221e78-67e4-4d88-b73d-e58a9943a036.png\n",
      "  - 0-fe221e78-67e4-4d88-b73d-e58a9943a036.png\n",
      "41.md:\n",
      "  - 恩佐娱乐-14.2E-220726.docx\n",
      "10.md:\n",
      "  - e705d192-90ee-4fd1-9dcd-061958d1817f.png\n",
      "  - 0-e705d192-90ee-4fd1-9dcd-061958d1817f.png\n",
      "  - 5a84cde3-7175-4044-8c88-d4c883a8fd38.png\n",
      "  - 0-5a84cde3-7175-4044-8c88-d4c883a8fd38.png\n",
      "  - 4ea07c23-a1a6-411b-bcfb-552d095b66c9.png\n",
      "  - 0-4ea07c23-a1a6-411b-bcfb-552d095b66c9.png\n",
      "  - 79d9b7f2-cfe4-4615-9b75-8fea33fc0c9d.png\n",
      "  - 0-79d9b7f2-cfe4-4615-9b75-8fea33fc0c9d.png\n",
      "  - 6bcc0131-e4ad-421e-bb1f-d8ebe5eeec7b.png\n",
      "  - 0-6bcc0131-e4ad-421e-bb1f-d8ebe5eeec7b.png\n",
      "9.md:\n",
      "  - 32eb7662-f212-4811-a7c1-1cfeb121cd99.png\n",
      "  - 0-32eb7662-f212-4811-a7c1-1cfeb121cd99.png\n",
      "  - 5d4e3e02-1dfc-469e-8af9-8dbe2b9f1564.png\n",
      "  - 0-5d4e3e02-1dfc-469e-8af9-8dbe2b9f1564.png\n",
      "  - 70c63791-2797-4bf0-a778-ea08819aa9de.png\n",
      "  - 0-70c63791-2797-4bf0-a778-ea08819aa9de.png\n",
      "  - 0baba509-5e81-4b88-b509-843822d09e21.png\n",
      "  - 0-0baba509-5e81-4b88-b509-843822d09e21.png\n",
      "  - 129ac70f-8942-4ca7-b1f2-ddeaa3d984b5.png\n",
      "  - 0-129ac70f-8942-4ca7-b1f2-ddeaa3d984b5.png\n",
      "  - b9d9c584-5e21-4a49-952b-ffecca4eb91e.png\n",
      "  - 0-b9d9c584-5e21-4a49-952b-ffecca4eb91e.png\n",
      "  - ee47dfea-2626-4107-8ab3-4663167e0493.png\n",
      "  - 0-ee47dfea-2626-4107-8ab3-4663167e0493.png\n",
      "  - aa99f763-6849-4f6b-adf2-58f0cc2ed545.png\n",
      "  - 0-aa99f763-6849-4f6b-adf2-58f0cc2ed545.png\n",
      "  - b6eb1b15-cf99-475c-921f-f06e5c1019d4.png\n",
      "  - 0-b6eb1b15-cf99-475c-921f-f06e5c1019d4.png\n",
      "20.md:\n",
      "  - 785cc8c9-1225-4f93-b633-349bc5113512.png\n",
      "  - 0-785cc8c9-1225-4f93-b633-349bc5113512.png\n",
      "  - af93eff8-2973-4746-9041-b2223016b117.png\n",
      "  - 0-af93eff8-2973-4746-9041-b2223016b117.png\n",
      "  - b3ce4d51-6024-4b43-b0d2-d3faaf3c2879.png\n",
      "  - 0-b3ce4d51-6024-4b43-b0d2-d3faaf3c2879.png\n",
      "  - f313f521-80a1-4db5-a8a7-53d29ee09890.png\n",
      "  - 0-f313f521-80a1-4db5-a8a7-53d29ee09890.png\n",
      "15.md:\n",
      "  - 20211103-1.txt\n",
      "  - b8cea3b1-4dde-4438-9b1a-6faf690bbad0.png\n",
      "  - 0-b8cea3b1-4dde-4438-9b1a-6faf690bbad0.png\n",
      "21.md:\n",
      "  - 9c8c9989-2293-4e68-9ffe-6f7a5f14562f.png\n",
      "  - 0-9c8c9989-2293-4e68-9ffe-6f7a5f14562f.png\n",
      "11.md:\n",
      "  - 2bankinfo-5.zip\n",
      "7.md:\n",
      "  - 94b16e53-f035-4aa9-a76e-80bc6e936d10.png\n",
      "  - 0-94b16e53-f035-4aa9-a76e-80bc6e936d10.png\n",
      "  - 645dfc97-3268-4e1d-920d-4138545456fa.png\n",
      "  - 0-645dfc97-3268-4e1d-920d-4138545456fa.png\n",
      "  - 1cc570d8-cddb-401e-8c37-ef10c0e4841f.png\n",
      "  - 0-1cc570d8-cddb-401e-8c37-ef10c0e4841f.png\n",
      "2.md:\n",
      "  - 9a8077f5-ac41-491f-b192-6b4609324bda.png\n",
      "  - 0-9a8077f5-ac41-491f-b192-6b4609324bda.png\n",
      "  - de359f8d-0745-4a93-959a-d1a6c361e326.png\n",
      "  - 0-de359f8d-0745-4a93-959a-d1a6c361e326.png\n",
      "  - 383d824e-7588-4a92-84b7-fd953dd91cba.png\n",
      "  - 0-383d824e-7588-4a92-84b7-fd953dd91cba.png\n",
      "  - f0ce8a7b-909d-4fc5-ba13-ea66b2dc6448.png\n",
      "  - 0-f0ce8a7b-909d-4fc5-ba13-ea66b2dc6448.png\n",
      "  - b8b76b6d-a50e-4246-82ee-3c8a5dcd523e.png\n",
      "  - 0-b8b76b6d-a50e-4246-82ee-3c8a5dcd523e.png\n",
      "  - 4c74b697-0681-4223-9982-5ffaf4e98ed0.png\n",
      "  - 0-4c74b697-0681-4223-9982-5ffaf4e98ed0.png\n",
      "  - 300450bf-221e-4eeb-bdda-dc1115c947ea.png\n",
      "  - 0-300450bf-221e-4eeb-bdda-dc1115c947ea.png\n",
      "  - 0f319bf6-e667-4bac-a974-dfda1142e9ff.png\n",
      "  - 0-0f319bf6-e667-4bac-a974-dfda1142e9ff.png\n",
      "  - 1a20ded1-50fc-4153-9a95-e158eeb7199e.png\n",
      "  - 0-1a20ded1-50fc-4153-9a95-e158eeb7199e.png\n",
      "  - 5ef1d666-e19d-4570-b800-6693a4f680ee.png\n",
      "  - 0-5ef1d666-e19d-4570-b800-6693a4f680ee.png\n",
      "  - e07a9457-86f1-4f0f-86d7-8ea816b8d8d3.png\n",
      "  - 0-e07a9457-86f1-4f0f-86d7-8ea816b8d8d3.png\n",
      "  - 08a6bcd3-6477-4252-8f35-4f8f80d114f9.png\n",
      "  - 0-08a6bcd3-6477-4252-8f35-4f8f80d114f9.png\n",
      "  - 7150f512-e7a2-4f2c-86bc-58b671b25ba9.png\n",
      "  - 0-7150f512-e7a2-4f2c-86bc-58b671b25ba9.png\n",
      "  - 62583414-9e32-4d09-8989-b5fa32a98a81.png\n",
      "  - 0-62583414-9e32-4d09-8989-b5fa32a98a81.png\n",
      "  - 4ae9bf34-c16c-4684-aa92-fec65a151275.png\n",
      "  - 0-4ae9bf34-c16c-4684-aa92-fec65a151275.png\n",
      "  - c5f1d959-39d1-4176-9cb1-1fb6e8baedc3.png\n",
      "  - 0-c5f1d959-39d1-4176-9cb1-1fb6e8baedc3.png\n",
      "  - 6e9aced1-df28-4e57-b7c8-641609ff4450.png\n",
      "  - 0-6e9aced1-df28-4e57-b7c8-641609ff4450.png\n",
      "  - 6cbb3eeb-17e9-4af6-8da1-36eb6437f7bc.png\n",
      "  - 0-6cbb3eeb-17e9-4af6-8da1-36eb6437f7bc.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Step 1: Initialize LLM\n",
    "llm2 = Ollama(model=\"llama3.1:8b\")\n",
    "\n",
    "# Step 2: Define the prompt template\n",
    "prompt_template_file_types = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"\"\"\n",
    "List the top 50 file extensions most commonly found in cybersecurity leaked data, including data from breaches, ransomware leaks, and dark web dumps. THINK LIKE A CYBERSECURITY EXPERT.\n",
    "Focus on file types that typically contain credentials, configurations, databases, personal data, internal documentation, archives, or images (e.g., screenshots of sensitive material). \n",
    "\n",
    "OUTPUT INSTRUCTIONS:\n",
    "ONLY OUTPUT the extensions as a clean Python list format, like [<'file_extension'>, <'file_extension'>, etc.] \n",
    "Don't include \".\" and ALWAYS use \"'\" in the list. \n",
    "Do not include any explanations, comments, or extra text. \n",
    "JUST GIVE THE LIST.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 3: Run the chain to get extensions\n",
    "chain = LLMChain(llm=llm2, prompt=prompt_template_file_types)\n",
    "llm_response = chain.run(content=\"\")\n",
    "\n",
    "# Step 4: Print the LLM output\n",
    "print(\"🔍 LLM-generated file extensions list:\")\n",
    "print(llm_response)\n",
    "\n",
    "# Step 5: Parse LLM response into a Python list\n",
    "try:\n",
    "    # Extract only the list portion using regex\n",
    "    match = re.search(r\"\\[(.*?)\\]\", llm_response, re.DOTALL)\n",
    "    if match:\n",
    "        list_str = \"[\" + match.group(1) + \"]\"\n",
    "        common_extensions = ast.literal_eval(list_str)\n",
    "    else:\n",
    "        raise ValueError(\"❌ No list found in LLM response.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(\"❌ Failed to parse LLM response into a list.\") from e\n",
    "\n",
    "# Step 6: Build dynamic regex\n",
    "ext_pattern = '|'.join(common_extensions)\n",
    "file_pattern = re.compile(r'[\\w\\-/\\.]{8,}\\.(?:' + ext_pattern + r')', re.IGNORECASE)\n",
    "\n",
    "# Step 7: Define chats path and extract files\n",
    "chats_path = 'I-Soon-data/md/chats'\n",
    "extracted_files_from_chats = {}\n",
    "\n",
    "for filename in os.listdir(chats_path):\n",
    "    if filename.endswith('.md'):\n",
    "        full_path = os.path.join(chats_path, filename)\n",
    "        with open(full_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            matches = file_pattern.findall(content)\n",
    "            if matches:\n",
    "                extracted_files_from_chats[filename] = matches\n",
    "\n",
    "# Step 8: Display results\n",
    "print(\"\\n📄 Extracted filenames from chats:\")\n",
    "for chat, files in extracted_files_from_chats.items():\n",
    "    print(f\"{chat}:\")\n",
    "    for f in files:\n",
    "        print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5857059",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# *Search which files are in the leaked data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d6162",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import Ollama\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# Prepare data\n",
    "markdown_file_path = 'I-Soon-data/md/chats/2.md'\n",
    "with open(markdown_file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# Prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"\"\"\n",
    "You are a cybersecurity analyst with expert knowledge of file types and syntax used to reference files in markdown and related contexts.\n",
    "Your task is to analyze the given markdown content and extract all explicit file names with extensions (e.g., .exe, .pdf, .docx).\n",
    "Follow these guidelines:\n",
    "\t•\tOnly include file names that explicitly contain a valid extension.\n",
    "\t•\tConsider common patterns such as filename.ext, [text](filename.ext), src=\"filename.ext\", path/to/filename.ext, etc.\n",
    "\t•\tRecognize contextual clues like source, reference, include, or links that point to files.\n",
    "\t•\tUse your knowledge of the top 50 most common file extensions (e.g., .txt, .pdf, .jpg, .docx, .exe, .zip, etc.) to guide detection.\n",
    "\t•\tDo not infer or fabricate file names based on ambiguous text. Do not extract names without a clear extension.\n",
    "\t•\tOutput only: a single line list of the detected file names with extensions, separated by commas. No explanation or commentary.\n",
    "Content:\n",
    "{content}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# File name regex\n",
    "file_regex = re.compile(r'\\b[\\w\\-]+(?:\\.[\\w\\-]+)*\\.\\w{2,6}\\b')\n",
    "\n",
    "# Worker function\n",
    "def process_line(line_text):\n",
    "    try:\n",
    "        # Create LLM inside the process\n",
    "        llm = Ollama(model=\"llama3.1:70b\")\n",
    "        chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "        response = chain.run(content=line_text)\n",
    "        return file_regex.findall(response)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run with multiprocessing\n",
    "found_files = []\n",
    "max_workers = 4  # Based on CPU/RAM; try 2–8 depending on load\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(process_line, line) for line in lines]\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        matches = future.result()\n",
    "        found_files.extend([m.lower() for m in matches])\n",
    "\n",
    "# Final output\n",
    "unique_files = sorted(set(found_files))\n",
    "print(\"\\n🎯 Unique filenames found:\")\n",
    "for f in unique_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a55a603e-103a-46c0-9dcc-8c72023cacb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'I-Soon-data' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Directory to delete\n",
    "# EXTRACT_DIR = 'I-Soon-data'\n",
    "\n",
    "# # Check if the directory exists and delete it\n",
    "# if os.path.isdir(EXTRACT_DIR):\n",
    "#     shutil.rmtree(EXTRACT_DIR)\n",
    "#     print(f\"Directory '{EXTRACT_DIR}' has been deleted.\")\n",
    "# else:\n",
    "#     print(f\"Directory '{EXTRACT_DIR}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c242f8e-af9c-488e-9a4a-92bdc06a8ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (IsoonAI)",
   "language": "python",
   "name": "isoonai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
