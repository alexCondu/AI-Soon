{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6d6a8f",
   "metadata": {},
   "source": [
    "# **IMPORTS & PATHS** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "879be4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Paths and model settings\\ nZIP_PATH = 'I-Soon-data.zip'      # Path to your downloaded zip file\n",
    "ZIP_PATH = '0.zip'    # Directory to extract contents\n",
    "EXTRACT_DIR = 'I-Soon-data'        # Directory to extract contents\n",
    "ORGANIZED_DIR = 'organized_data'  # Directory to group files by extension\n",
    "MODEL_NAME = 'llama3.2'             # Local Ollama model identifier\n",
    "OUTPUT_JSON = 'parsed_md.json'    # Aggregated JSON output\n",
    "OUTPUT_CSV = 'parsed_md.csv'      # CSV output for DataFrame\n",
    "\n",
    "# Initialize Ollama LLM via LangChain\n",
    "model = OllamaLLM(model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f9f3fd",
   "metadata": {},
   "source": [
    "# **ZIP FILE EXTRACTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e6feb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction directory 'I-Soon-data' already exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(EXTRACT_DIR):\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_DIR)\n",
    "    print(f\"Extracted archive to '{EXTRACT_DIR}'\")\n",
    "else:\n",
    "    print(f\"Extraction directory '{EXTRACT_DIR}' already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a01d8",
   "metadata": {},
   "source": [
    "# **DATA TYPE CATEGORIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dec65000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organized files under 'organized_data':\n",
      " • Markdown with HTML → md/html/\n",
      " • Markdown without HTML → md/non-html/\n",
      " • Other extensions → <extension>/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# regex to detect any HTML tag\n",
    "html_re = re.compile(r'<[A-Za-z/][^>]*>')\n",
    "\n",
    "for root, _, files in os.walk(EXTRACT_DIR):\n",
    "    for fname in files:\n",
    "        ext = os.path.splitext(fname)[1].lower().lstrip('.') or 'no_extension'\n",
    "        src_path = os.path.join(root, fname)\n",
    "\n",
    "        if ext == 'md':\n",
    "            # classify Markdown by content into md/html or md/non-html\n",
    "            with open(src_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "            subfolder = 'html' if html_re.search(text) else 'non-html'\n",
    "            dest_folder = os.path.join(ORGANIZED_DIR, 'md', subfolder)\n",
    "        else:\n",
    "            # preserve original extension grouping for non-MD\n",
    "            dest_folder = os.path.join(ORGANIZED_DIR, ext)\n",
    "\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        dst_path = os.path.join(dest_folder, fname)\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "print(f\"Organized files under '{ORGANIZED_DIR}':\\n\"\n",
    "      \" • Markdown with HTML → md/html/\\n\"\n",
    "      \" • Markdown without HTML → md/non-html/\\n\"\n",
    "      \" • Other extensions → <extension>/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a6aea",
   "metadata": {},
   "source": [
    "# **MD DATA TRANSFORMATIO TO JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "321522dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "converter = DocumentConverter()\n",
    "result    = converter.convert(\"organized_data/md/html/34.md\")\n",
    "\n",
    "# In Pydantic v2 the public API is .model_dump()\n",
    "doc_dict  = result.document.model_dump()\n",
    "\n",
    "# Now serialize however you like:\n",
    "with open(\"34_docling.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(doc_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232dc4db",
   "metadata": {},
   "source": [
    "# **JSON TO DATAFRAME WITH AI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01e08cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposed DataFrame headers:\n",
      "[\"Time\", \"From\", \"To\", \"Message\"]\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "with open(\"34_docling.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    doc_json = json.load(f)\n",
    "\n",
    "# Extract a manageable portion of the table cells\n",
    "cells = doc_json[\"tables\"][0][\"data\"][\"table_cells\"]\n",
    "sample_cells = cells[:20]  # reduce size for prompt\n",
    "\n",
    "# Format as JSON string for input to LLM\n",
    "cell_str = json.dumps(sample_cells, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Prompt for LLM: only ask for DataFrame headers\n",
    "prompt = f\"\"\"\n",
    "You are a law inforcement officer that specializez in investigating cybersecurity data leaks. Analyze the following JSON table cell data and infer what column headers would be appropriate\n",
    "if this were converted into a pandas DataFrame. \n",
    "\n",
    "Only return a list of column names that are relevant to the data. Do not include any other text or explanation.\n",
    "\n",
    "JSON data:\n",
    "{cell_str}\n",
    "\"\"\"\n",
    "\n",
    "# Run the prompt through Ollama\n",
    "headers_response = model(prompt)\n",
    "\n",
    "# Output the suggested headers\n",
    "print(\"Proposed DataFrame headers:\")\n",
    "print(headers_response.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7551b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "def load_docling_tables_with_llm_headers(json_path: str, headers_response: str) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads a Docling JSON export and returns a dict mapping\n",
    "    \"table_1\", \"table_2\", … → pandas.DataFrame for each table found.\n",
    "    Uses LLM-generated headers instead of inferring from content.\n",
    "    \"\"\"\n",
    "    # Parse LLM headers from string\n",
    "    try:\n",
    "        headers = ast.literal_eval(headers_response.strip())\n",
    "        if not isinstance(headers, list):\n",
    "            raise ValueError(\"LLM response did not evaluate to a list\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to parse headers from LLM output: {e}\")\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        doc = json.load(f)\n",
    "\n",
    "    tables = doc.get(\"tables\", [])\n",
    "    dfs: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    for idx, tbl in enumerate(tables, start=1):\n",
    "        # Flatten to DataFrame\n",
    "        cells = tbl[\"data\"][\"table_cells\"]\n",
    "        df_cells = pd.DataFrame(cells)\n",
    "\n",
    "        # Pivot into grid\n",
    "        grid = df_cells.pivot(\n",
    "            index=\"start_row_offset_idx\",\n",
    "            columns=\"start_col_offset_idx\",\n",
    "            values=\"text\"\n",
    "        )\n",
    "\n",
    "        # Remove the first row (assumed to be header row in JSON, already handled by LLM)\n",
    "        body = grid.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "        # Apply LLM-inferred headers\n",
    "        body.columns = headers\n",
    "\n",
    "        # Store result\n",
    "        dfs[f\"table_{idx}\"] = body\n",
    "\n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba1c00c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== table_1 (shape: (426, 4)) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-08 01:36:58</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>我昨天一天都在忙....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-08 01:37:12</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>你好久回呢？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-08 01:37:14</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>没事</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-08 01:37:19</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>我可能要下周</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-11-08 01:37:23</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>我擦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>2018-11-08 16:22:57</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>你不要跟别的人说起这些哈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>2018-11-08 16:23:06</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>嗯嗯不得</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>2018-11-08 16:23:19</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>尤其是一楼的女的......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>2018-11-08 16:23:21</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>哈哈哈哈哈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>2018-11-08 16:23:52</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>这种事肯定不得，张欢在这里都不晓得这些事，只是晓得今天客户这个事我怎么问你</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>426 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Time                From                  To  \\\n",
       "0    2018-11-08 01:36:58          qq78263462  wxid_5390224027312   \n",
       "1    2018-11-08 01:37:12          qq78263462  wxid_5390224027312   \n",
       "2    2018-11-08 01:37:14  wxid_5390224027312          qq78263462   \n",
       "3    2018-11-08 01:37:19  wxid_5390224027312          qq78263462   \n",
       "4    2018-11-08 01:37:23          qq78263462  wxid_5390224027312   \n",
       "..                   ...                 ...                 ...   \n",
       "421  2018-11-08 16:22:57          qq78263462  wxid_5390224027312   \n",
       "422  2018-11-08 16:23:06  wxid_5390224027312          qq78263462   \n",
       "423  2018-11-08 16:23:19          qq78263462  wxid_5390224027312   \n",
       "424  2018-11-08 16:23:21          qq78263462  wxid_5390224027312   \n",
       "425  2018-11-08 16:23:52  wxid_5390224027312          qq78263462   \n",
       "\n",
       "                                   Message  \n",
       "0                             我昨天一天都在忙....  \n",
       "1                                   你好久回呢？  \n",
       "2                                       没事  \n",
       "3                                   我可能要下周  \n",
       "4                                       我擦  \n",
       "..                                     ...  \n",
       "421                           你不要跟别的人说起这些哈  \n",
       "422                                   嗯嗯不得  \n",
       "423                         尤其是一楼的女的......  \n",
       "424                                  哈哈哈哈哈  \n",
       "425  这种事肯定不得，张欢在这里都不晓得这些事，只是晓得今天客户这个事我怎么问你  \n",
       "\n",
       "[426 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "JSON_PATH = \"34_docling.json\"\n",
    "\n",
    "# Example LLM output from earlier cell\n",
    "headers_response = '[\"Time\", \"From\", \"To\", \"Message\"]'\n",
    "\n",
    "# Load and display\n",
    "dataframes = load_docling_tables_with_llm_headers(JSON_PATH, headers_response)\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n=== {name} (shape: {df.shape}) ===\")\n",
    "    display(df)  # Displayed properly in Jupyter if this is the last line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11955b5",
   "metadata": {},
   "source": [
    "# **DATAFRAME TO CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "877f5868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: parsed_html_to_csv/table_1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define output directory\n",
    "OUTPUT_DIR = \"parsed_html_to_csv\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save each table to a CSV inside the directory\n",
    "for name, df in dataframes.items():\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"{name}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISoonAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
