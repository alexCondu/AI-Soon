{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6559c645-340c-4646-b646-39b14bcd9932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'csvs' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Directory to delete\n",
    "EXTRACT_DIR = 'csvs'\n",
    "\n",
    "# Check if the directory exists and delete it\n",
    "if os.path.isdir(EXTRACT_DIR):\n",
    "    shutil.rmtree(EXTRACT_DIR)\n",
    "    print(f\"Directory '{EXTRACT_DIR}' has been deleted.\")\n",
    "else:\n",
    "    print(f\"Directory '{EXTRACT_DIR}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5776e2db-42ed-4aa2-a497-1a545810a7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores:  144\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "print(\"CPU cores: \", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2befe2f0-40aa-4ffa-8a73-7443ff5760b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted to: 0\n"
     ]
    }
   ],
   "source": [
    "def extract_zip(zip_path):\n",
    "\n",
    "    if not zipfile.is_zipfile(zip_path):\n",
    "        raise ValueError(f\"The file at {zip_path} is not a valid ZIP archive.\")\n",
    "    \n",
    "    # Determine the output directory name from the zip file name\n",
    "    base_dir = os.path.dirname(zip_path)\n",
    "    zip_filename = os.path.basename(zip_path)\n",
    "    folder_name = os.path.splitext(zip_filename)[0]\n",
    "    extract_to = os.path.join(base_dir, folder_name)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(extract_to):\n",
    "        os.makedirs(extract_to)\n",
    "\n",
    "    # Extract the ZIP file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(path=extract_to)\n",
    "\n",
    "    return extract_to\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    zip_file_path = '0.zip' \n",
    "    extracted_dir = extract_zip(zip_file_path)\n",
    "    print(f\"Extracted to: {extracted_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "950a34b4-50f3-486e-b781-a1dc9e43c458",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted - 0/__MACOSX\n"
     ]
    }
   ],
   "source": [
    "leak_directory = \"0\"\n",
    "\n",
    "# Path to the __MACOSX folder\n",
    "macosx_folder = os.path.join(leak_directory, \"__MACOSX\")\n",
    "\n",
    "# Check if __MACOSX exists and remove it - creates issues when analyzing the data, and its not needed, made automatically by MacOS\n",
    "if os.path.exists(macosx_folder) and os.path.isdir(macosx_folder):\n",
    "    shutil.rmtree(macosx_folder)\n",
    "    print(f\"Deleted - {macosx_folder}\")\n",
    "else:\n",
    "    print(f\"Folder not found - {macosx_folder}\")\n",
    "\n",
    "# Organize files by extension into subfolders\n",
    "for root, dirs, files in os.walk(leak_directory):\n",
    "    for file in files:\n",
    "        # Skip hidden files and __MACOSX if any reappear\n",
    "        if file.startswith('.') or '__MACOSX' in root:\n",
    "            continue\n",
    "\n",
    "        # Get the file extension (in lowercase, without the dot)\n",
    "        file_extension = os.path.splitext(file)[1].lower().lstrip('.')\n",
    "        if not file_extension:\n",
    "            file_extension = \"no_extension\"\n",
    "\n",
    "        # Define the new subfolder path\n",
    "        subfolder_path = os.path.join(leak_directory, file_extension)\n",
    "\n",
    "        # Create the subfolder if it doesn't exist\n",
    "        os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "        # Define source and destination paths\n",
    "        source_path = os.path.join(root, file)\n",
    "        destination_path = os.path.join(subfolder_path, file)\n",
    "\n",
    "        # Move the file if source and destination are not the same\n",
    "        if os.path.abspath(source_path) != os.path.abspath(destination_path):\n",
    "            shutil.move(source_path, destination_path)\n",
    "\n",
    "# Remove any empty folders within the parent directory\n",
    "for dirpath, dirnames, filenames in os.walk(leak_directory, topdown=False):\n",
    "    if not dirnames and not filenames:\n",
    "        try:\n",
    "            os.rmdir(dirpath)\n",
    "            print(f\"Removed empty folder: {dirpath}\")\n",
    "        except OSError:\n",
    "            pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab45fc5-01f0-4dd6-9c70-1ad25b1212a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '0/0'deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete Duplicate of 0 which is empty\n",
    "\n",
    "# Directory to delete\n",
    "EXTRACT_DIR = '0/0'\n",
    "\n",
    "# Check if the directory exists and delete it\n",
    "if os.path.isdir(EXTRACT_DIR):\n",
    "    shutil.rmtree(EXTRACT_DIR)\n",
    "    print(f\"Directory '{EXTRACT_DIR}'deleted\")\n",
    "else:\n",
    "    print(f\"Directory '{EXTRACT_DIR}' does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f80802-63db-4c12-9401-d9d6246f556a",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Dataframe with all file location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88ee5d4f-ee82-49b3-bb69-9b052f1952f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_folder_file_dataframe(root_dir):\n",
    "    folders = []\n",
    "    for file_name in os.listdir(root_dir):\n",
    "        full_path = os.path.join(root_dir, file_name)\n",
    "        if os.path.isdir(full_path):\n",
    "            folders.append(file_name)\n",
    "\n",
    "    series_list = []\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        files = [\n",
    "            os.path.join(folder_path, file)\n",
    "            for file in os.listdir(folder_path)\n",
    "            if os.path.isfile(os.path.join(folder_path, file)) and not file.startswith('.')\n",
    "        ]\n",
    "        s = pd.Series(files, name=folder)\n",
    "        series_list.append(s)\n",
    "\n",
    "    df = pd.concat(series_list, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94163880-07e2-4205-8302-b5ead2a1f488",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>md</th>\n",
       "      <th>png</th>\n",
       "      <th>log</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md</td>\n",
       "      <td>0/png/64bba692-d430-440c-9f1e-2575f45770af_6.png</td>\n",
       "      <td>0/log/77010155050.log</td>\n",
       "      <td>0/txt/IDNET.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0/md/28.md</td>\n",
       "      <td>0/png/12756724-394c-4576-b373-7c53f1abbd94_0.png</td>\n",
       "      <td>0/log/77753527617.log</td>\n",
       "      <td>0/txt/IDTV.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0/md/5.md</td>\n",
       "      <td>0/png/f179eb06-0c53-44df-a13f-570be23355bb_1.png</td>\n",
       "      <td>0/log/tele2-lbs.log</td>\n",
       "      <td>0/txt/beeline-77774042222.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0/md/38.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_24.png</td>\n",
       "      <td>0/log/tele2-cdr.log</td>\n",
       "      <td>0/txt/beeline-77051056626.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_30.png</td>\n",
       "      <td>0/log/tele2-crm.log</td>\n",
       "      <td>0/txt/beeline-crm.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0/md/18.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_18.png</td>\n",
       "      <td>0/log/77783030133.log</td>\n",
       "      <td>0/txt/UBSCRIBER.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md</td>\n",
       "      <td>0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/beeline-cdr.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md</td>\n",
       "      <td>0/png/5387a301-0af8-4e24-a197-20189f87b9ef_8.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/CRM.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0/md/1.md</td>\n",
       "      <td>0/png/0-32eb7662-f212-4811-a7c1-1cfeb121cd99.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/LAC.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0/md/19.md</td>\n",
       "      <td>0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_11.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/beeline-lbs.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md</td>\n",
       "      <td>0/png/0-7150f512-e7a2-4f2c-86bc-58b671b25ba9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/Φ»¥σìò.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0/md/29.md</td>\n",
       "      <td>0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_8.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md</td>\n",
       "      <td>0/png/eda5b003-9250-4913-b724-74cca86240af_13.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0/md/585875ff-f8c5-4a02-acd7-fef37dc9ff11.md</td>\n",
       "      <td>0/png/eda5b003-9250-4913-b724-74cca86240af_12.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0/md/4.md</td>\n",
       "      <td>0/png/adaf869e-920a-4a17-91bd-e2ef3125c10e.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0/md/39.md</td>\n",
       "      <td>0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0/md/16.md</td>\n",
       "      <td>0/png/32eb7662-f212-4811-a7c1-1cfeb121cd99.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0/md/64bba692-d430-440c-9f1e-2575f45770af.md</td>\n",
       "      <td>0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_10.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0/md/178e3898-903d-47cf-bfbe-061e7dc18895.md</td>\n",
       "      <td>0/png/5387a301-0af8-4e24-a197-20189f87b9ef_9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0/md/22.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_19.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0/md/32.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_31.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0/md/26.md</td>\n",
       "      <td>0/png/f179eb06-0c53-44df-a13f-570be23355bb_0.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0/md/9fe6b262-9944-417d-a0c4-9f2de1de2994.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_25.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0/md/f7205881-3904-42ec-ab2c-04f36fa24785.md</td>\n",
       "      <td>0/png/7150f512-e7a2-4f2c-86bc-58b671b25ba9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0/md/54990932-71af-48dd-9a7a-2617b1407c54.md</td>\n",
       "      <td>0/png/12756724-394c-4576-b373-7c53f1abbd94_1.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0/md/12.md</td>\n",
       "      <td>0/png/64bba692-d430-440c-9f1e-2575f45770af_7.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0/md/36.md</td>\n",
       "      <td>0/png/6848748d-2881-4c26-b153-fcd5373d2f1c.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0/md/eda5b003-9250-4913-b724-74cca86240af.md</td>\n",
       "      <td>0/png/12756724-394c-4576-b373-7c53f1abbd94_3.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0/md/27.md</td>\n",
       "      <td>0/png/64bba692-d430-440c-9f1e-2575f45770af_5.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0/md/13.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_33.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              md  \\\n",
       "0   0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md   \n",
       "1                                     0/md/28.md   \n",
       "2                                      0/md/5.md   \n",
       "3                                     0/md/38.md   \n",
       "4   0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md   \n",
       "5                                     0/md/18.md   \n",
       "6   0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md   \n",
       "7   0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md   \n",
       "8                                      0/md/1.md   \n",
       "9                                     0/md/19.md   \n",
       "10  0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md   \n",
       "11                                    0/md/29.md   \n",
       "12  0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md   \n",
       "13  0/md/585875ff-f8c5-4a02-acd7-fef37dc9ff11.md   \n",
       "14                                     0/md/4.md   \n",
       "15                                    0/md/39.md   \n",
       "16                                    0/md/16.md   \n",
       "17  0/md/64bba692-d430-440c-9f1e-2575f45770af.md   \n",
       "18  0/md/178e3898-903d-47cf-bfbe-061e7dc18895.md   \n",
       "19                                    0/md/22.md   \n",
       "20                                    0/md/32.md   \n",
       "21                                    0/md/26.md   \n",
       "22  0/md/9fe6b262-9944-417d-a0c4-9f2de1de2994.md   \n",
       "23  0/md/f7205881-3904-42ec-ab2c-04f36fa24785.md   \n",
       "24  0/md/54990932-71af-48dd-9a7a-2617b1407c54.md   \n",
       "25                                    0/md/12.md   \n",
       "26                                    0/md/36.md   \n",
       "27  0/md/eda5b003-9250-4913-b724-74cca86240af.md   \n",
       "28                                    0/md/27.md   \n",
       "29                                    0/md/13.md   \n",
       "\n",
       "                                                  png                    log  \\\n",
       "0    0/png/64bba692-d430-440c-9f1e-2575f45770af_6.png  0/log/77010155050.log   \n",
       "1    0/png/12756724-394c-4576-b373-7c53f1abbd94_0.png  0/log/77753527617.log   \n",
       "2    0/png/f179eb06-0c53-44df-a13f-570be23355bb_1.png    0/log/tele2-lbs.log   \n",
       "3   0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_24.png    0/log/tele2-cdr.log   \n",
       "4   0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_30.png    0/log/tele2-crm.log   \n",
       "5   0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_18.png  0/log/77783030133.log   \n",
       "6    0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e.png                    NaN   \n",
       "7    0/png/5387a301-0af8-4e24-a197-20189f87b9ef_8.png                    NaN   \n",
       "8    0/png/0-32eb7662-f212-4811-a7c1-1cfeb121cd99.png                    NaN   \n",
       "9   0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_11.png                    NaN   \n",
       "10   0/png/0-7150f512-e7a2-4f2c-86bc-58b671b25ba9.png                    NaN   \n",
       "11   0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_8.png                    NaN   \n",
       "12  0/png/eda5b003-9250-4913-b724-74cca86240af_13.png                    NaN   \n",
       "13  0/png/eda5b003-9250-4913-b724-74cca86240af_12.png                    NaN   \n",
       "14     0/png/adaf869e-920a-4a17-91bd-e2ef3125c10e.png                    NaN   \n",
       "15   0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_9.png                    NaN   \n",
       "16     0/png/32eb7662-f212-4811-a7c1-1cfeb121cd99.png                    NaN   \n",
       "17  0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_10.png                    NaN   \n",
       "18   0/png/5387a301-0af8-4e24-a197-20189f87b9ef_9.png                    NaN   \n",
       "19  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_19.png                    NaN   \n",
       "20  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_31.png                    NaN   \n",
       "21   0/png/f179eb06-0c53-44df-a13f-570be23355bb_0.png                    NaN   \n",
       "22  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_25.png                    NaN   \n",
       "23     0/png/7150f512-e7a2-4f2c-86bc-58b671b25ba9.png                    NaN   \n",
       "24   0/png/12756724-394c-4576-b373-7c53f1abbd94_1.png                    NaN   \n",
       "25   0/png/64bba692-d430-440c-9f1e-2575f45770af_7.png                    NaN   \n",
       "26     0/png/6848748d-2881-4c26-b153-fcd5373d2f1c.png                    NaN   \n",
       "27   0/png/12756724-394c-4576-b373-7c53f1abbd94_3.png                    NaN   \n",
       "28   0/png/64bba692-d430-440c-9f1e-2575f45770af_5.png                    NaN   \n",
       "29  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_33.png                    NaN   \n",
       "\n",
       "                              txt  \n",
       "0                 0/txt/IDNET.txt  \n",
       "1                  0/txt/IDTV.txt  \n",
       "2   0/txt/beeline-77774042222.txt  \n",
       "3   0/txt/beeline-77051056626.txt  \n",
       "4           0/txt/beeline-crm.txt  \n",
       "5             0/txt/UBSCRIBER.txt  \n",
       "6           0/txt/beeline-cdr.txt  \n",
       "7                   0/txt/CRM.txt  \n",
       "8                   0/txt/LAC.txt  \n",
       "9           0/txt/beeline-lbs.txt  \n",
       "10               0/txt/Φ»¥σìò.txt  \n",
       "11                            NaN  \n",
       "12                            NaN  \n",
       "13                            NaN  \n",
       "14                            NaN  \n",
       "15                            NaN  \n",
       "16                            NaN  \n",
       "17                            NaN  \n",
       "18                            NaN  \n",
       "19                            NaN  \n",
       "20                            NaN  \n",
       "21                            NaN  \n",
       "22                            NaN  \n",
       "23                            NaN  \n",
       "24                            NaN  \n",
       "25                            NaN  \n",
       "26                            NaN  \n",
       "27                            NaN  \n",
       "28                            NaN  \n",
       "29                            NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_folder_file_dataframe(leak_directory)\n",
    "df.head(30)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b011fbc-09b2-4ccd-b075-62306871df42",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Use LLM to classify the source files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c46d6b38-6ff3-4a79-860c-a37054e03cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ba6064-3151-4166-a432-142d9e610d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying files: 100%|██████████| 70/70 [02:51<00:00,  2.45s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>other</th>\n",
       "      <th>chats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md</td>\n",
       "      <td>0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md</td>\n",
       "      <td>0/md/28.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md</td>\n",
       "      <td>None</td>\n",
       "      <td>0/md/5.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md</td>\n",
       "      <td>None</td>\n",
       "      <td>0/md/38.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md</td>\n",
       "      <td>None</td>\n",
       "      <td>0/md/18.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md</td>\n",
       "      <td>None</td>\n",
       "      <td>0/md/1.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         images  \\\n",
       "0  0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md   \n",
       "1  0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md   \n",
       "2  0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md   \n",
       "3  0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md   \n",
       "4  0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md   \n",
       "\n",
       "                                          other       chats  \n",
       "0  0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md  0/md/28.md  \n",
       "1                                          None   0/md/5.md  \n",
       "2                                          None  0/md/38.md  \n",
       "3                                          None  0/md/18.md  \n",
       "4                                          None   0/md/1.md  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3.1:8b\")\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"\"\"\n",
    "You are analyzing the content of a file.\n",
    "\n",
    "File content:\n",
    "\\\"\\\"\\\"\n",
    "{content}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "1. Classify the content into one of the following categories ONLY: chats, images, other.\n",
    "2. State your confidence in the classification as one of: high, medium, or low.\n",
    "3. I will have to make a csv, please give me a list of headers based on the content. E.g. \"[<header_name>, <heaer_name2>, etc.]\n",
    "Respond in the following format:\n",
    "Category: <chats|images|other>\n",
    "Confidence: <high|medium|low>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Preprocess the files to the first 20 lines to shorten analysis times\n",
    "def preprocess_first_20_lines(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [line.strip() for _, line in zip(range(20), f)]\n",
    "            content = \" \".join(lines)\n",
    "        return file_path, content\n",
    "    except Exception:\n",
    "        return file_path, \"\"\n",
    "\n",
    "# Use only the first column of the DataFrame\n",
    "first_column = df.columns[0]\n",
    "file_paths = df[first_column].dropna().unique().tolist()\n",
    "\n",
    "# Preprocess the file for faster classification\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    file_data = list(executor.map(preprocess_first_20_lines, file_paths))\n",
    "\n",
    "# Classify and collect by category\n",
    "valid_categories = {\"chats\", \"images\", \"other\"}\n",
    "categorized_files = {cat: [] for cat in valid_categories}\n",
    "\n",
    "for file_path, content in tqdm(file_data, desc=\"Classifying files\"):\n",
    "    if not content:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        response = chain.run(content=content).strip().lower()\n",
    "        lines = response.splitlines()\n",
    "\n",
    "        category = \"\"\n",
    "        for line in lines:\n",
    "            if line.startswith(\"category:\"):\n",
    "                category = line.replace(\"category:\", \"\").strip()\n",
    "                break\n",
    "        if category not in valid_categories:\n",
    "            category = \"other\"\n",
    "\n",
    "        categorized_files[category].append(file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert to DataFrame (columns = categories)\n",
    "# Determine the maximum list length among all categories\n",
    "max_len = 0\n",
    "for file_list in categorized_files.values():\n",
    "    if len(file_list) > max_len:\n",
    "        max_len = len(file_list)\n",
    "\n",
    "# Pad each list with None to match max length\n",
    "padded = {}\n",
    "for category, file_list in categorized_files.items():\n",
    "    padding_needed = max_len - len(file_list)\n",
    "    padded[category] = file_list + [None] * padding_needed\n",
    "result_df = pd.DataFrame(padded)\n",
    "result_df.to_csv(\"classified_by_category.csv\", index=False)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae9479b-52e3-48d0-867b-55aa68bf1acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e96cc02-e59d-4113-b35e-857490c25736",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Script to check for cross refernced files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2960f30-2535-4cc4-b4ec-ac57315e764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e94a6ab-a1d9-4c11-b275-a753fc8182f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    source_file  \\\n",
      "0  0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md   \n",
      "1                                    0/md/10.md   \n",
      "2                                    0/md/13.md   \n",
      "3                                    0/md/15.md   \n",
      "4  0/md/178e3898-903d-47cf-bfbe-061e7dc18895.md   \n",
      "5                                    0/md/19.md   \n",
      "6                                     0/md/2.md   \n",
      "7                                    0/md/20.md   \n",
      "8                                    0/md/21.md   \n",
      "9                                    0/md/22.md   \n",
      "\n",
      "                                            mentions  \n",
      "0  [0/png/01cdc26f-e773-4ad7-8808-d04abf16aae7_1_...  \n",
      "1  [0/png/0-6bcc0131-e4ad-421e-bb1f-d8ebe5eeec7b....  \n",
      "2  [0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e....  \n",
      "3  [0/png/0-b8cea3b1-4dde-4438-9b1a-6faf690bbad0....  \n",
      "4  [0/png/178e3898-903d-47cf-bfbe-061e7dc18895_8....  \n",
      "5  [0/png/6848748d-2881-4c26-b153-fcd5373d2f1c.pn...  \n",
      "6  [0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md,...  \n",
      "7  [0/png/0-785cc8c9-1225-4f93-b633-349bc5113512....  \n",
      "8  [0/png/9c8c9989-2293-4e68-9ffe-6f7a5f14562f.pn...  \n",
      "9  [0/png/0-b0a4acaa-d768-4f6d-8e54-6d20f271bb7c....  \n",
      "Unique file paths in original DataFrame: 577\n"
     ]
    }
   ],
   "source": [
    "# Extract all unique file paths from the DataFrame \n",
    "file_paths = df.stack().dropna().unique().tolist()\n",
    "\n",
    "# Read file content\n",
    "def read_file(path):\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except:\n",
    "        return \"Failed to open file\"\n",
    "\n",
    "# Read file contents into a dictionary \n",
    "file_contents = {path: read_file(path) for path in file_paths}\n",
    "\n",
    "# Build a reference map (which files mention which) \n",
    "reference_map = defaultdict(list)\n",
    "\n",
    "for source_path, content in file_contents.items():\n",
    "    if content is None:\n",
    "        continue  # skip if content is None\n",
    "\n",
    "    for target_path in file_paths:\n",
    "        if target_path == source_path:\n",
    "            continue  # skip comparing file to itself\n",
    "\n",
    "        target_filename = os.path.basename(target_path)\n",
    "        pattern = re.escape(target_filename)\n",
    "\n",
    "        if re.search(rf'\\b{pattern}\\b', content):\n",
    "            reference_map[source_path].append(target_path)\n",
    "\n",
    "# Convert reference map to a DataFrame\n",
    "ref_df = pd.DataFrame([\n",
    "    {\"source_file\": src, \"mentions\": tgt}\n",
    "    for src, tgts in reference_map.items()\n",
    "    for tgt in tgts\n",
    "])\n",
    "\n",
    "# Group mentions into lists per source_file\n",
    "ref_summary = ref_df.groupby(\"source_file\")[\"mentions\"].apply(list).reset_index()\n",
    "\n",
    "# Some source_file names are also in mentions, therefore they aren't a source file anymore\n",
    "source_files = set(ref_summary[\"source_file\"])\n",
    "mentioned_files = set(file for mention_list in ref_summary[\"mentions\"] for file in mention_list)\n",
    "common_files = source_files.intersection(mentioned_files)\n",
    "\n",
    "# Filter out common files from the DataFrame\n",
    "filtered_ref_summary = ref_summary[~ref_summary[\"source_file\"].isin(common_files)].reset_index(drop=True)\n",
    "\n",
    "filtered_ref_summary.to_csv(\"file_reference_map.csv\", index=False)\n",
    "print(filtered_ref_summary.head(10))\n",
    "print(\"Unique file paths in original DataFrame:\", df.stack().nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0bce5a-16de-48e6-83df-240b732f1a20",
   "metadata": {},
   "source": [
    "# Check for more linkages in the files + add the missing source files from the chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f92af7-5075-4d8d-9251-113f6e360f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from ast import literal_eval\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe46121c-4ff4-4e45-b248-8edd4d12be74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['source_file', 'mentions', 'mentions2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def is_readable_text_file(path):\n",
    "    return any(path.endswith(ext) for ext in ['txt', 'md', 'rst', 'tex', 'nfo', 'readme', 'rtf', 'doc', 'docx',\n",
    "    'cfg', 'conf', 'config', 'ini', 'json', 'yaml', 'yml', 'toml',\n",
    "    'log', 'lst', 'cnf', 'properties', 'prefs',\n",
    "    'csv', 'tsv', 'dat', 'db', 'dbf', 'sql', 'xml',\n",
    "    'ssv', 'psv', 'jsonl', 'parquet', 'orc',\n",
    "    'html', 'htm', 'xhtml', 'xht', 'css', 'js',\n",
    "    'jsx', 'ts', 'tsx', 'vue', 'erb', 'ejs', 'jsp',\n",
    "    'liquid', 'handlebars', 'hbs', 'mustache',\n",
    "    'py', 'pyw', 'ipynb', 'java', 'c', 'cpp', 'h', 'hpp', 'cs',\n",
    "    'sh', 'bash', 'zsh', 'ksh', 'bat', 'cmd', 'ps1',\n",
    "    'r', 'jl', 'pl', 'pm', 'rb', 'go', 'lua', 'php',\n",
    "    'swift', 'scala', 'dart', 'asm', 'groovy',\n",
    "    'rmd', 'sage', 'nb',\n",
    "    'env', 'gradle', 'makefile', 'mak', 'mk',\n",
    "    'dockerfile', 'gitignore', 'gitattributes', 'gitmodules',\n",
    "    'cmake', 'make', 'ninja', 'build',\n",
    "    'manifest', 'manifest.json', 'vtt', 'srt', 'resx', 'strings',\n",
    "    'lang', 'po', 'mo', 'pot', 'msg', 'textbundle',\n",
    "    'rego', 'tf', 'tfvars', 'cue', 'bzl', 'bazel', 'nix', 'dhall',\n",
    "    'adoc', 'asciidoc', 'creole', 'mediawiki', 'wiki', 'org',\n",
    "    'eml', 'msg', 'mbox', 'mail', 'ics', 'vcf'])\n",
    "\n",
    "mentions2_list = []\n",
    "\n",
    "# Iterate over each row of the summary\n",
    "for _, row in filtered_ref_summary.iterrows():\n",
    "    mention_dict = {}\n",
    "\n",
    "    # For each mentioned file read file contents and create a dictionary key: location - values: file contents\n",
    "    for mentioned_file in row[\"mentions\"]:\n",
    "        if not is_readable_text_file(mentioned_file):\n",
    "            continue  # Skip unreadable formats\n",
    "\n",
    "        # Read file content\n",
    "        content = read_file(mentioned_file)\n",
    "        if content in [None, \"Failed to open file\"]:\n",
    "            continue\n",
    "\n",
    "        # Search for any file-type references using regex\n",
    "        # Match anything with a name and known file extension\n",
    "        found_files = re.findall(r'[\\w\\-/]*\\.\\w+', content)\n",
    "\n",
    "        # Filter duplicates and add to the mention_dict\n",
    "        if found_files:\n",
    "            mention_dict[mentioned_file] = list(set(found_files))\n",
    "\n",
    "    mentions2_list.append(mention_dict)\n",
    "\n",
    "# Add this as a new column\n",
    "filtered_ref_summary[\"mentions2\"] = mentions2_list\n",
    "\n",
    "# Get the list of chat files from result_df - LLM clasification\n",
    "chat_files = result_df['chats'].dropna().unique()\n",
    "\n",
    "# Get the list of source files already in filtered_ref_summary\n",
    "source_files = filtered_ref_summary['source_file'].dropna().unique()\n",
    "\n",
    "# Find missing chat files and collect chat files that are not found in source files\n",
    "existing_source_files = set(source_files)\n",
    "missing_chat_files = []\n",
    "for chat_file in chat_files:\n",
    "    if chat_file not in existing_source_files:\n",
    "        missing_chat_files.append(chat_file)\n",
    "\n",
    "# Create a new DataFrame with empty mentions and linkage columns\n",
    "new_rows = pd.DataFrame({\n",
    "    'source_file': missing_chat_files,\n",
    "})\n",
    "\n",
    "# Append the new rows to ref_summary\n",
    "filtered_ref_summary = pd.concat([filtered_ref_summary, new_rows], ignore_index=True)\n",
    "        \n",
    "# Save the updated DataFrame to CSV\n",
    "filtered_ref_summary.to_csv(\"file_reference_map_extended.csv\", index=False)\n",
    "print(filtered_ref_summary.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e01b1-5dd6-4678-b3d9-e48e859045f1",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Structuring the Data into CSV - Step 1 all MD files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37835ee2-1c35-4cf7-a08e-89d2e02c5621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9d08d1c-afe4-4cae-b359-779c917f3689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a folder and dumb all csvs\n",
    "output_dir = \"csvs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all MD files from the DataFrame\n",
    "all_file_paths = pd.unique(result_df.values.ravel('K'))\n",
    "md_files = []\n",
    "for file_path in all_file_paths:\n",
    "    if isinstance(file_path, str) and file_path.endswith(\".md\") and os.path.exists(file_path):\n",
    "        md_files.append(file_path)\n",
    "        \n",
    "# Use the LLM's clasification of MD files\n",
    "chat_files = set(result_df['chats'].dropna().astype(str))\n",
    "\n",
    "# Extract visible text + ANY file reference \n",
    "def extract_text_and_files(td):\n",
    "    text = td.get_text(strip=True)\n",
    "    links = []\n",
    "    for tag in td.find_all(['a', 'img']):\n",
    "        link = tag.get('href') or tag.get('src')\n",
    "        if link:\n",
    "            fname = os.path.basename(link.strip(\"'\\\"\"))\n",
    "            links.append(fname)\n",
    "    return text + (\" \" + \" \".join(links) if links else \"\")\n",
    "\n",
    "# This method extracts chat table from HTML\n",
    "def process_chat_md(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        rows = soup.find_all(\"tr\")[1:]\n",
    "        data = []\n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) == 4:\n",
    "                data.append({\n",
    "                    \"Time\": cols[0].get_text(strip=True),\n",
    "                    \"From\": cols[1].get_text(strip=True),\n",
    "                    \"To\": cols[2].get_text(strip=True),\n",
    "                    \"Message\": extract_text_and_files(cols[3])\n",
    "                })\n",
    "        return pd.DataFrame(data) if data else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chat file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract the contents of the other file types that the LLM classified\n",
    "def process_reference_md(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        matches = []\n",
    "        for tag in soup.find_all(['a', 'img']):\n",
    "            link = tag.get('href') or tag.get('src')\n",
    "            if link:\n",
    "                fname = os.path.basename(link.strip(\"'\\\"\"))\n",
    "                if not fname.startswith(\"0-\"):\n",
    "                    matches.append(fname)\n",
    "        matches = list(set(matches))\n",
    "        return pd.DataFrame([{\"File_Name\": f} for f in matches]) if matches else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing reference file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Loop through all MD filesand call the above methods for the afferent file type\n",
    "for file_path in md_files:\n",
    "    csv_name = os.path.basename(file_path).replace(\".md\", \".csv\")\n",
    "    csv_path = os.path.join(output_dir, csv_name)\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        if file_path in chat_files:\n",
    "            df = process_chat_md(file_path)\n",
    "        else:\n",
    "            df = process_reference_md(file_path)\n",
    "\n",
    "        if df is not None:\n",
    "            df.to_csv(csv_path, index=False)\n",
    "        else:\n",
    "            print(f\"Couldn't save to CSV: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "070f1c87-70d5-4699-8220-fecbc4f05457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f4e4289-8394-4707-b8d9-687061d28d62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>mentions</th>\n",
       "      <th>mentions2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>csvs/01cdc26f-e773-4ad7-8808-d04abf16aae7.csv</td>\n",
       "      <td>[0/png/01cdc26f-e773-4ad7-8808-d04abf16aae7_1_...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>csvs/178e3898-903d-47cf-bfbe-061e7dc18895.csv</td>\n",
       "      <td>[0/png/178e3898-903d-47cf-bfbe-061e7dc18895_8....</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>csvs/23.csv</td>\n",
       "      <td>[csvs/547aba02-6757-49c1-acb5-6df217cebfc7.csv]</td>\n",
       "      <td>{'csvs/547aba02-6757-49c1-acb5-6df217cebfc7.cs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>csvs/26.csv</td>\n",
       "      <td>[csvs/54990932-71af-48dd-9a7a-2617b1407c54.csv]</td>\n",
       "      <td>{'csvs/54990932-71af-48dd-9a7a-2617b1407c54.cs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>csvs/2db27de1-d5c5-4f89-8572-da697a6329e4.csv</td>\n",
       "      <td>[0/png/2db27de1-d5c5-4f89-8572-da697a6329e4_5_...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      source_file  \\\n",
       "0   csvs/01cdc26f-e773-4ad7-8808-d04abf16aae7.csv   \n",
       "4   csvs/178e3898-903d-47cf-bfbe-061e7dc18895.csv   \n",
       "10                                    csvs/23.csv   \n",
       "12                                    csvs/26.csv   \n",
       "13  csvs/2db27de1-d5c5-4f89-8572-da697a6329e4.csv   \n",
       "\n",
       "                                             mentions  \\\n",
       "0   [0/png/01cdc26f-e773-4ad7-8808-d04abf16aae7_1_...   \n",
       "4   [0/png/178e3898-903d-47cf-bfbe-061e7dc18895_8....   \n",
       "10    [csvs/547aba02-6757-49c1-acb5-6df217cebfc7.csv]   \n",
       "12    [csvs/54990932-71af-48dd-9a7a-2617b1407c54.csv]   \n",
       "13  [0/png/2db27de1-d5c5-4f89-8572-da697a6329e4_5_...   \n",
       "\n",
       "                                            mentions2  \n",
       "0                                                  {}  \n",
       "4                                                  {}  \n",
       "10  {'csvs/547aba02-6757-49c1-acb5-6df217cebfc7.cs...  \n",
       "12  {'csvs/54990932-71af-48dd-9a7a-2617b1407c54.cs...  \n",
       "13                                                 {}  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dir = \"csvs\"\n",
    "\n",
    "# Replace MD with CSV if the corresponding file exists\n",
    "def replace_md_with_csv(item):\n",
    "    if isinstance(item, str) and item.endswith(\".md\"):\n",
    "        base_name = Path(item).stem  # .stem extracts the filename with no extension\n",
    "        # CSV search\n",
    "        csv_path = os.path.join(csv_dir, f\"{base_name}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            return csv_path\n",
    "        else:\n",
    "            print(f\"CSV {item} not found\")\n",
    "    return item\n",
    "\n",
    "# Recursively handle strings, lists, and dictionaries\n",
    "def process_column_cell(cell):\n",
    "    if isinstance(cell, str):\n",
    "        return replace_md_with_csv(cell)\n",
    "    \n",
    "    elif isinstance(cell, list):\n",
    "        new_list = []\n",
    "        for item in cell:\n",
    "            new_item = process_column_cell(item)\n",
    "            new_list.append(new_item)\n",
    "        return new_list\n",
    "\n",
    "    elif isinstance(cell, dict):\n",
    "        new_dict = {}\n",
    "        for key, value in cell.items():\n",
    "            new_key = replace_md_with_csv(key)\n",
    "            new_value = process_column_cell(value)\n",
    "            new_dict[new_key] = new_value\n",
    "        return new_dict\n",
    "\n",
    "    else:\n",
    "        return cell\n",
    "    \n",
    "for col in filtered_ref_summary.columns:\n",
    "    filtered_ref_summary[col] = filtered_ref_summary[col].apply(process_column_cell)\n",
    "\n",
    "filtered_ref_summary.to_csv(\"filtered_ref_summary_csv_replaced.csv\", index=False)\n",
    "filtered_ref_summary.head()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d20edad-a67b-49d6-a04e-c7dd5816c91a",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Structuring the Data into CSV - Step 2 OCR on image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81eb9363-517d-4c41-bad0-a2f5057a5cee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "137d498a-c3d1-438b-817e-63973aaf66c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488\n"
     ]
    }
   ],
   "source": [
    "png_paths = []\n",
    "\n",
    "\n",
    "# Gather every png path from the df\n",
    "def gather_pngs(dataframe_name):\n",
    "    for column in dataframe_name.columns:\n",
    "\n",
    "        # Loop through each cell in that column\n",
    "        for cell in dataframe_name[column]:\n",
    "\n",
    "            # Case 1: The cell is a simple string\n",
    "            if isinstance(cell, str):\n",
    "                if cell.endswith(\".png\"):\n",
    "                    png_paths.append(cell)\n",
    "\n",
    "            # List\n",
    "            elif isinstance(cell, list):\n",
    "                for item in cell:\n",
    "                    if isinstance(item, str) and item.endswith(\".png\"):\n",
    "                        png_paths.append(item)\n",
    "\n",
    "            # Dictionary\n",
    "            elif isinstance(cell, dict):\n",
    "                for key, value in cell.items():\n",
    "                    # Check key\n",
    "                    if isinstance(key, str) and key.endswith(\".png\"):\n",
    "                        png_paths.append(key)\n",
    "\n",
    "                    # Check value\n",
    "                    if isinstance(value, str) and value.endswith(\".png\"):\n",
    "                        png_paths.append(value)\n",
    "\n",
    "                    # If the value is a list of paths\n",
    "                    elif isinstance(value, list):\n",
    "                        for sub_item in value:\n",
    "                            if isinstance(sub_item, str) and sub_item.endswith(\".png\"):\n",
    "                                png_paths.append(sub_item)\n",
    "                                \n",
    "gather_pngs(filtered_ref_summary)\n",
    "print(len(png_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb2ca5-83ac-4f57-a227-61dd217580c2",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# PaddleOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6079a28-5dbb-44fe-82ca-ff9a65092a48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /home/jovyan/.paddlex/official_models.\u001b[0m\n",
      "E0526 11:49:37.156906  2204 analysis_config.cc:169] Please use PaddlePaddle with GPU version.\n",
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[32mUsing official model (UVDoc), the model files will be automatically downloaded and saved in /home/jovyan/.paddlex/official_models.\u001b[0m\n",
      "E0526 11:49:37.669418  2204 analysis_config.cc:169] Please use PaddlePaddle with GPU version.\n",
      "\u001b[32mCreating model: ('PP-LCNet_x0_25_textline_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x0_25_textline_ori), the model files will be automatically downloaded and saved in /home/jovyan/.paddlex/official_models.\u001b[0m\n",
      "E0526 11:49:40.892796  2204 analysis_config.cc:169] Please use PaddlePaddle with GPU version.\n",
      "\u001b[32mCreating model: ('PP-OCRv5_mobile_det', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv5_mobile_det), the model files will be automatically downloaded and saved in /home/jovyan/.paddlex/official_models.\u001b[0m\n",
      "E0526 11:49:41.269107  2204 analysis_config.cc:169] Please use PaddlePaddle with GPU version.\n",
      "\u001b[32mCreating model: ('PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv5_mobile_rec), the model files will be automatically downloaded and saved in /home/jovyan/.paddlex/official_models.\u001b[0m\n",
      "E0526 11:49:42.655756  2204 analysis_config.cc:169] Please use PaddlePaddle with GPU version.\n"
     ]
    }
   ],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "import os\n",
    "\n",
    "# Use the Chinese model\n",
    "ocr_model = PaddleOCR(lang='ch')\n",
    "\n",
    "def paddle_ocr(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"File not found - {image_path}\")\n",
    "        return \"\"\n",
    "\n",
    "    result = ocr_model.ocr(image_path)\n",
    "\n",
    "    # For document structure output\n",
    "    if isinstance(result, list) and isinstance(result[0], dict) and 'rec_texts' in result[0]:\n",
    "        return \"\\n\".join(result[0]['rec_texts'])\n",
    "\n",
    "    # For basic detection output\n",
    "    extracted_text = []\n",
    "    for line in result[0]:\n",
    "        text = line[1][0]  # text, confidence\n",
    "        extracted_text.append(text)\n",
    "\n",
    "    return \"\\n\".join(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859064e-11fd-4143-ada1-16f18c5339e3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6691298c-1ae6-4a48-934a-2d56c8543584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c038d2f7-1513-4a68-ab9c-5a232476e93e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the Chinese and English model\n",
    "ocr_model = easyocr.Reader(['ch_sim'], gpu = True)  # Load once\n",
    "\n",
    "def easy_ocr(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"File not found - {image_path}\")\n",
    "        return \"\"\n",
    "\n",
    "    results = ocr_model.readtext(image_path, detail=0)  # Only return text, not boxes/confidences\n",
    "\n",
    "    return \"\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e735e605-9db2-40cb-89b9-b3de0a99de6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'ocr' does not exist.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Directory to delete\n",
    "EXTRACT_DIR = 'ocr'\n",
    "\n",
    "# Check if the directory exists and delete it\n",
    "if os.path.isdir(EXTRACT_DIR):\n",
    "    shutil.rmtree(EXTRACT_DIR)\n",
    "    print(f\"Directory '{EXTRACT_DIR}' has been deleted.\")\n",
    "else:\n",
    "    print(f\"Directory '{EXTRACT_DIR}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93149a3a-4b10-4316-bca7-92c5574e94d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a22a2-9790-46bd-85ca-437f60db484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "output_dir = 'ocr'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Number of threads to use — adjust based on your CPU\n",
    "MAX_WORKERS = 2\n",
    "\n",
    "def process_image(image_path):\n",
    "    try:\n",
    "        # Skip if output already exists\n",
    "        base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        output_csv_path = os.path.join(output_dir, f\"{base_name}.csv\")\n",
    "        if os.path.exists(output_csv_path):\n",
    "            return (image_path, \"Skipped (already exists)\")\n",
    "\n",
    "        ocr_result = easy_ocr(image_path)\n",
    "        combined_text = \" \".join(ocr_result.splitlines())\n",
    "\n",
    "        with open(output_csv_path, mode='w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['ocr_transf'])\n",
    "            writer.writerow([combined_text])\n",
    "\n",
    "        return (image_path, \"Success\")\n",
    "    except Exception as e:\n",
    "        return (image_path, f\"Error: {e}\")\n",
    "\n",
    "# Submit tasks to the thread pool\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = [executor.submit(process_image, path) for path in png_paths]\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Parallel OCR with EasyOCR\"):\n",
    "        image_path, status = future.result()\n",
    "        if \"Error\" in status:\n",
    "            print(f\"[!] Failed: {image_path} -> {status}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (IsoonAI)",
   "language": "python",
   "name": "isoonai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
