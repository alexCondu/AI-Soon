{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6559c645-340c-4646-b646-39b14bcd9932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'test' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Directory to delete\n",
    "EXTRACT_DIR = 'test'\n",
    "\n",
    "# Check if the directory exists and delete it\n",
    "if os.path.isdir(EXTRACT_DIR):\n",
    "    shutil.rmtree(EXTRACT_DIR)\n",
    "    print(f\"Directory '{EXTRACT_DIR}' has been deleted.\")\n",
    "else:\n",
    "    print(f\"Directory '{EXTRACT_DIR}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5776e2db-42ed-4aa2-a497-1a545810a7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores:  144\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "print(\"CPU cores: \", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2befe2f0-40aa-4ffa-8a73-7443ff5760b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted to: 0\n"
     ]
    }
   ],
   "source": [
    "def extract_zip(zip_path):\n",
    "\n",
    "    if not zipfile.is_zipfile(zip_path):\n",
    "        raise ValueError(f\"The file at {zip_path} is not a valid ZIP archive.\")\n",
    "    \n",
    "    # Determine the output directory name from the zip file name\n",
    "    base_dir = os.path.dirname(zip_path)\n",
    "    zip_filename = os.path.basename(zip_path)\n",
    "    folder_name = os.path.splitext(zip_filename)[0]\n",
    "    extract_to = os.path.join(base_dir, folder_name)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(extract_to):\n",
    "        os.makedirs(extract_to)\n",
    "\n",
    "    # Extract the ZIP file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(path=extract_to)\n",
    "\n",
    "    return extract_to\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    zip_file_path = '0.zip' \n",
    "    extracted_dir = extract_zip(zip_file_path)\n",
    "    print(f\"Extracted to: {extracted_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "950a34b4-50f3-486e-b781-a1dc9e43c458",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted - 0/__MACOSX\n"
     ]
    }
   ],
   "source": [
    "leak_directory = \"0\"\n",
    "\n",
    "# Path to the __MACOSX folder\n",
    "macosx_folder = os.path.join(leak_directory, \"__MACOSX\")\n",
    "\n",
    "# Check if __MACOSX exists and remove it - creates issues when analyzing the data, and its not needed, made automatically by MacOS\n",
    "if os.path.exists(macosx_folder) and os.path.isdir(macosx_folder):\n",
    "    shutil.rmtree(macosx_folder)\n",
    "    print(f\"Deleted - {macosx_folder}\")\n",
    "else:\n",
    "    print(f\"Folder not found - {macosx_folder}\")\n",
    "\n",
    "# Organize files by extension into subfolders\n",
    "for root, dirs, files in os.walk(leak_directory):\n",
    "    for file in files:\n",
    "        # Skip hidden files and __MACOSX if any reappear\n",
    "        if file.startswith('.') or '__MACOSX' in root:\n",
    "            continue\n",
    "\n",
    "        # Get the file extension (in lowercase, without the dot)\n",
    "        file_extension = os.path.splitext(file)[1].lower().lstrip('.')\n",
    "        if not file_extension:\n",
    "            file_extension = \"no_extension\"\n",
    "\n",
    "        # Define the new subfolder path\n",
    "        subfolder_path = os.path.join(leak_directory, file_extension)\n",
    "\n",
    "        # Create the subfolder if it doesn't exist\n",
    "        os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "        # Define source and destination paths\n",
    "        source_path = os.path.join(root, file)\n",
    "        destination_path = os.path.join(subfolder_path, file)\n",
    "\n",
    "        # Move the file if source and destination are not the same\n",
    "        if os.path.abspath(source_path) != os.path.abspath(destination_path):\n",
    "            shutil.move(source_path, destination_path)\n",
    "\n",
    "# Remove any empty folders within the parent directory\n",
    "for dirpath, dirnames, filenames in os.walk(leak_directory, topdown=False):\n",
    "    if not dirnames and not filenames:\n",
    "        try:\n",
    "            os.rmdir(dirpath)\n",
    "            print(f\"Removed empty folder: {dirpath}\")\n",
    "        except OSError:\n",
    "            pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ab45fc5-01f0-4dd6-9c70-1ad25b1212a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '0/0'deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete Duplicate of 0 which is empty\n",
    "\n",
    "# Directory to delete\n",
    "EXTRACT_DIR = '0/0'\n",
    "\n",
    "# Check if the directory exists and delete it\n",
    "if os.path.isdir(EXTRACT_DIR):\n",
    "    shutil.rmtree(EXTRACT_DIR)\n",
    "    print(f\"Directory '{EXTRACT_DIR}'deleted\")\n",
    "else:\n",
    "    print(f\"Directory '{EXTRACT_DIR}' does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb18a8-64e3-44d3-915e-7158104d0c8f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# **DATA PARSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f80802-63db-4c12-9401-d9d6246f556a",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Dataframe with all file location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88ee5d4f-ee82-49b3-bb69-9b052f1952f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_folder_file_dataframe(root_dir):\n",
    "    folders = []\n",
    "    for file_name in os.listdir(root_dir):\n",
    "        full_path = os.path.join(root_dir, file_name)\n",
    "        if os.path.isdir(full_path):\n",
    "            folders.append(file_name)\n",
    "\n",
    "    series_list = []\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        files = [\n",
    "            os.path.join(folder_path, file)\n",
    "            for file in os.listdir(folder_path)\n",
    "            if os.path.isfile(os.path.join(folder_path, file)) and not file.startswith('.')\n",
    "        ]\n",
    "        s = pd.Series(files, name=folder)\n",
    "        series_list.append(s)\n",
    "\n",
    "    df = pd.concat(series_list, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94163880-07e2-4205-8302-b5ead2a1f488",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>md</th>\n",
       "      <th>png</th>\n",
       "      <th>log</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md</td>\n",
       "      <td>0/png/64bba692-d430-440c-9f1e-2575f45770af_6.png</td>\n",
       "      <td>0/log/77010155050.log</td>\n",
       "      <td>0/txt/IDNET.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0/md/28.md</td>\n",
       "      <td>0/png/12756724-394c-4576-b373-7c53f1abbd94_0.png</td>\n",
       "      <td>0/log/77753527617.log</td>\n",
       "      <td>0/txt/IDTV.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0/md/5.md</td>\n",
       "      <td>0/png/f179eb06-0c53-44df-a13f-570be23355bb_1.png</td>\n",
       "      <td>0/log/tele2-lbs.log</td>\n",
       "      <td>0/txt/beeline-77774042222.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0/md/38.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_24.png</td>\n",
       "      <td>0/log/tele2-cdr.log</td>\n",
       "      <td>0/txt/beeline-77051056626.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_30.png</td>\n",
       "      <td>0/log/tele2-crm.log</td>\n",
       "      <td>0/txt/beeline-crm.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0/md/18.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_18.png</td>\n",
       "      <td>0/log/77783030133.log</td>\n",
       "      <td>0/txt/UBSCRIBER.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md</td>\n",
       "      <td>0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/beeline-cdr.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md</td>\n",
       "      <td>0/png/5387a301-0af8-4e24-a197-20189f87b9ef_8.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/CRM.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0/md/1.md</td>\n",
       "      <td>0/png/0-32eb7662-f212-4811-a7c1-1cfeb121cd99.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/LAC.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0/md/19.md</td>\n",
       "      <td>0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_11.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/beeline-lbs.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md</td>\n",
       "      <td>0/png/0-7150f512-e7a2-4f2c-86bc-58b671b25ba9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/Φ»¥σìò.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0/md/29.md</td>\n",
       "      <td>0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_8.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md</td>\n",
       "      <td>0/png/eda5b003-9250-4913-b724-74cca86240af_13.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0/md/585875ff-f8c5-4a02-acd7-fef37dc9ff11.md</td>\n",
       "      <td>0/png/eda5b003-9250-4913-b724-74cca86240af_12.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0/md/4.md</td>\n",
       "      <td>0/png/adaf869e-920a-4a17-91bd-e2ef3125c10e.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0/md/39.md</td>\n",
       "      <td>0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0/md/16.md</td>\n",
       "      <td>0/png/32eb7662-f212-4811-a7c1-1cfeb121cd99.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0/md/64bba692-d430-440c-9f1e-2575f45770af.md</td>\n",
       "      <td>0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_10.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0/md/178e3898-903d-47cf-bfbe-061e7dc18895.md</td>\n",
       "      <td>0/png/5387a301-0af8-4e24-a197-20189f87b9ef_9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0/md/22.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_19.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0/md/32.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_31.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0/md/26.md</td>\n",
       "      <td>0/png/f179eb06-0c53-44df-a13f-570be23355bb_0.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0/md/9fe6b262-9944-417d-a0c4-9f2de1de2994.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_25.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0/md/f7205881-3904-42ec-ab2c-04f36fa24785.md</td>\n",
       "      <td>0/png/7150f512-e7a2-4f2c-86bc-58b671b25ba9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0/md/54990932-71af-48dd-9a7a-2617b1407c54.md</td>\n",
       "      <td>0/png/12756724-394c-4576-b373-7c53f1abbd94_1.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0/md/12.md</td>\n",
       "      <td>0/png/64bba692-d430-440c-9f1e-2575f45770af_7.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0/md/36.md</td>\n",
       "      <td>0/png/6848748d-2881-4c26-b153-fcd5373d2f1c.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0/md/eda5b003-9250-4913-b724-74cca86240af.md</td>\n",
       "      <td>0/png/12756724-394c-4576-b373-7c53f1abbd94_3.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0/md/27.md</td>\n",
       "      <td>0/png/64bba692-d430-440c-9f1e-2575f45770af_5.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0/md/13.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_33.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              md  \\\n",
       "0   0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md   \n",
       "1                                     0/md/28.md   \n",
       "2                                      0/md/5.md   \n",
       "3                                     0/md/38.md   \n",
       "4   0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md   \n",
       "5                                     0/md/18.md   \n",
       "6   0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md   \n",
       "7   0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md   \n",
       "8                                      0/md/1.md   \n",
       "9                                     0/md/19.md   \n",
       "10  0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md   \n",
       "11                                    0/md/29.md   \n",
       "12  0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md   \n",
       "13  0/md/585875ff-f8c5-4a02-acd7-fef37dc9ff11.md   \n",
       "14                                     0/md/4.md   \n",
       "15                                    0/md/39.md   \n",
       "16                                    0/md/16.md   \n",
       "17  0/md/64bba692-d430-440c-9f1e-2575f45770af.md   \n",
       "18  0/md/178e3898-903d-47cf-bfbe-061e7dc18895.md   \n",
       "19                                    0/md/22.md   \n",
       "20                                    0/md/32.md   \n",
       "21                                    0/md/26.md   \n",
       "22  0/md/9fe6b262-9944-417d-a0c4-9f2de1de2994.md   \n",
       "23  0/md/f7205881-3904-42ec-ab2c-04f36fa24785.md   \n",
       "24  0/md/54990932-71af-48dd-9a7a-2617b1407c54.md   \n",
       "25                                    0/md/12.md   \n",
       "26                                    0/md/36.md   \n",
       "27  0/md/eda5b003-9250-4913-b724-74cca86240af.md   \n",
       "28                                    0/md/27.md   \n",
       "29                                    0/md/13.md   \n",
       "\n",
       "                                                  png                    log  \\\n",
       "0    0/png/64bba692-d430-440c-9f1e-2575f45770af_6.png  0/log/77010155050.log   \n",
       "1    0/png/12756724-394c-4576-b373-7c53f1abbd94_0.png  0/log/77753527617.log   \n",
       "2    0/png/f179eb06-0c53-44df-a13f-570be23355bb_1.png    0/log/tele2-lbs.log   \n",
       "3   0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_24.png    0/log/tele2-cdr.log   \n",
       "4   0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_30.png    0/log/tele2-crm.log   \n",
       "5   0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_18.png  0/log/77783030133.log   \n",
       "6    0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e.png                    NaN   \n",
       "7    0/png/5387a301-0af8-4e24-a197-20189f87b9ef_8.png                    NaN   \n",
       "8    0/png/0-32eb7662-f212-4811-a7c1-1cfeb121cd99.png                    NaN   \n",
       "9   0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_11.png                    NaN   \n",
       "10   0/png/0-7150f512-e7a2-4f2c-86bc-58b671b25ba9.png                    NaN   \n",
       "11   0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_8.png                    NaN   \n",
       "12  0/png/eda5b003-9250-4913-b724-74cca86240af_13.png                    NaN   \n",
       "13  0/png/eda5b003-9250-4913-b724-74cca86240af_12.png                    NaN   \n",
       "14     0/png/adaf869e-920a-4a17-91bd-e2ef3125c10e.png                    NaN   \n",
       "15   0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_9.png                    NaN   \n",
       "16     0/png/32eb7662-f212-4811-a7c1-1cfeb121cd99.png                    NaN   \n",
       "17  0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_10.png                    NaN   \n",
       "18   0/png/5387a301-0af8-4e24-a197-20189f87b9ef_9.png                    NaN   \n",
       "19  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_19.png                    NaN   \n",
       "20  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_31.png                    NaN   \n",
       "21   0/png/f179eb06-0c53-44df-a13f-570be23355bb_0.png                    NaN   \n",
       "22  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_25.png                    NaN   \n",
       "23     0/png/7150f512-e7a2-4f2c-86bc-58b671b25ba9.png                    NaN   \n",
       "24   0/png/12756724-394c-4576-b373-7c53f1abbd94_1.png                    NaN   \n",
       "25   0/png/64bba692-d430-440c-9f1e-2575f45770af_7.png                    NaN   \n",
       "26     0/png/6848748d-2881-4c26-b153-fcd5373d2f1c.png                    NaN   \n",
       "27   0/png/12756724-394c-4576-b373-7c53f1abbd94_3.png                    NaN   \n",
       "28   0/png/64bba692-d430-440c-9f1e-2575f45770af_5.png                    NaN   \n",
       "29  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_33.png                    NaN   \n",
       "\n",
       "                              txt  \n",
       "0                 0/txt/IDNET.txt  \n",
       "1                  0/txt/IDTV.txt  \n",
       "2   0/txt/beeline-77774042222.txt  \n",
       "3   0/txt/beeline-77051056626.txt  \n",
       "4           0/txt/beeline-crm.txt  \n",
       "5             0/txt/UBSCRIBER.txt  \n",
       "6           0/txt/beeline-cdr.txt  \n",
       "7                   0/txt/CRM.txt  \n",
       "8                   0/txt/LAC.txt  \n",
       "9           0/txt/beeline-lbs.txt  \n",
       "10               0/txt/Φ»¥σìò.txt  \n",
       "11                            NaN  \n",
       "12                            NaN  \n",
       "13                            NaN  \n",
       "14                            NaN  \n",
       "15                            NaN  \n",
       "16                            NaN  \n",
       "17                            NaN  \n",
       "18                            NaN  \n",
       "19                            NaN  \n",
       "20                            NaN  \n",
       "21                            NaN  \n",
       "22                            NaN  \n",
       "23                            NaN  \n",
       "24                            NaN  \n",
       "25                            NaN  \n",
       "26                            NaN  \n",
       "27                            NaN  \n",
       "28                            NaN  \n",
       "29                            NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_folder_file_dataframe(leak_directory)\n",
    "df.head(30)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b011fbc-09b2-4ccd-b075-62306871df42",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Use LLM to classify the source files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46d6b38-6ff3-4a79-860c-a37054e03cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ba6064-3151-4166-a432-142d9e610d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_313/23640893.py:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
      "Classifying files:   0%|          | 0/70 [00:00<?, ?it/s]/tmp/ipykernel_313/23640893.py:51: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  response = chain.run(content=content).strip().lower()\n",
      "Classifying files: 100%|██████████| 70/70 [03:58<00:00,  3.40s/it] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other</th>\n",
       "      <th>images</th>\n",
       "      <th>chats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md</td>\n",
       "      <td>0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md</td>\n",
       "      <td>0/md/28.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md</td>\n",
       "      <td>0/md/5.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md</td>\n",
       "      <td>0/md/38.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md</td>\n",
       "      <td>0/md/18.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md</td>\n",
       "      <td>0/md/1.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          other  \\\n",
       "0  0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md   \n",
       "1                                          None   \n",
       "2                                          None   \n",
       "3                                          None   \n",
       "4                                          None   \n",
       "\n",
       "                                         images       chats  \n",
       "0  0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md  0/md/28.md  \n",
       "1  0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md   0/md/5.md  \n",
       "2  0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md  0/md/38.md  \n",
       "3  0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md  0/md/18.md  \n",
       "4  0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md   0/md/1.md  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3.1:8b\")\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"\"\"\n",
    "You are analyzing the content of a file.\n",
    "\n",
    "File content:\n",
    "\\\"\\\"\\\"\n",
    "{content}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "1. Classify the content into one of the following categories ONLY: chats, images, other.\n",
    "2. State your confidence in the classification as one of: high, medium, or low.\n",
    "3. I will have to make a csv, please give me a list of headers based on the content. E.g. \"[<header_name>, <heaer_name2>, etc.]\n",
    "Respond in the following format:\n",
    "Category: <chats|images|other>\n",
    "Confidence: <high|medium|low>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Preprocess the files to the first 20 lines to shorten analysis times\n",
    "def preprocess_first_20_lines(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [line.strip() for _, line in zip(range(20), f)]\n",
    "            content = \" \".join(lines)\n",
    "        return file_path, content\n",
    "    except Exception:\n",
    "        return file_path, \"\"\n",
    "\n",
    "# Use only the first column of the DataFrame\n",
    "first_column = df.columns[0]\n",
    "file_paths = df[first_column].dropna().unique().tolist()\n",
    "\n",
    "# Preprocess the file for faster classification\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    file_data = list(executor.map(preprocess_first_20_lines, file_paths))\n",
    "\n",
    "# Classify and collect by category\n",
    "valid_categories = {\"chats\", \"images\", \"other\"}\n",
    "categorized_files = {cat: [] for cat in valid_categories}\n",
    "\n",
    "for file_path, content in tqdm(file_data, desc=\"Classifying files\"):\n",
    "    if not content:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        response = chain.run(content=content).strip().lower()\n",
    "        lines = response.splitlines()\n",
    "\n",
    "        category = \"\"\n",
    "        for line in lines:\n",
    "            if line.startswith(\"category:\"):\n",
    "                category = line.replace(\"category:\", \"\").strip()\n",
    "                break\n",
    "        if category not in valid_categories:\n",
    "            category = \"other\"\n",
    "\n",
    "        categorized_files[category].append(file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert to DataFrame (columns = categories)\n",
    "# Determine the maximum list length among all categories\n",
    "max_len = 0\n",
    "for file_list in categorized_files.values():\n",
    "    if len(file_list) > max_len:\n",
    "        max_len = len(file_list)\n",
    "\n",
    "# Pad each list with None to match max length\n",
    "padded = {}\n",
    "for category, file_list in categorized_files.items():\n",
    "    padding_needed = max_len - len(file_list)\n",
    "    padded[category] = file_list + [None] * padding_needed\n",
    "result_df = pd.DataFrame(padded)\n",
    "result_df.to_csv(\"classified_by_category.csv\", index=False)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae9479b-52e3-48d0-867b-55aa68bf1acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e96cc02-e59d-4113-b35e-857490c25736",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Script to check for cross refernced files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2960f30-2535-4cc4-b4ec-ac57315e764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e94a6ab-a1d9-4c11-b275-a753fc8182f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    source_file  \\\n",
      "0  0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md   \n",
      "1                                    0/md/10.md   \n",
      "2                                    0/md/13.md   \n",
      "3                                    0/md/15.md   \n",
      "4  0/md/178e3898-903d-47cf-bfbe-061e7dc18895.md   \n",
      "5                                    0/md/19.md   \n",
      "6                                     0/md/2.md   \n",
      "7                                    0/md/20.md   \n",
      "8                                    0/md/21.md   \n",
      "9                                    0/md/22.md   \n",
      "\n",
      "                                            mentions  \n",
      "0  [0/png/01cdc26f-e773-4ad7-8808-d04abf16aae7_1_...  \n",
      "1  [0/png/0-6bcc0131-e4ad-421e-bb1f-d8ebe5eeec7b....  \n",
      "2  [0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e....  \n",
      "3  [0/png/0-b8cea3b1-4dde-4438-9b1a-6faf690bbad0....  \n",
      "4  [0/png/178e3898-903d-47cf-bfbe-061e7dc18895_8....  \n",
      "5  [0/png/6848748d-2881-4c26-b153-fcd5373d2f1c.pn...  \n",
      "6  [0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md,...  \n",
      "7  [0/png/0-785cc8c9-1225-4f93-b633-349bc5113512....  \n",
      "8  [0/png/9c8c9989-2293-4e68-9ffe-6f7a5f14562f.pn...  \n",
      "9  [0/png/0-b0a4acaa-d768-4f6d-8e54-6d20f271bb7c....  \n",
      "Unique file paths in original DataFrame: 577\n"
     ]
    }
   ],
   "source": [
    "# Extract all unique file paths from the DataFrame \n",
    "file_paths = df.stack().dropna().unique().tolist()\n",
    "\n",
    "# Read file content\n",
    "def read_file(path):\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except:\n",
    "        return \"Failed to open file\"\n",
    "\n",
    "# Read file contents into a dictionary \n",
    "file_contents = {path: read_file(path) for path in file_paths}\n",
    "\n",
    "# Build a reference map (which files mention which) \n",
    "reference_map = defaultdict(list)\n",
    "\n",
    "for source_path, content in file_contents.items():\n",
    "    if content is None:\n",
    "        continue  # skip if content is None\n",
    "\n",
    "    for target_path in file_paths:\n",
    "        if target_path == source_path:\n",
    "            continue  # skip comparing file to itself\n",
    "\n",
    "        target_filename = os.path.basename(target_path)\n",
    "        pattern = re.escape(target_filename)\n",
    "\n",
    "        if re.search(rf'\\b{pattern}\\b', content):\n",
    "            reference_map[source_path].append(target_path)\n",
    "\n",
    "# Convert reference map to a DataFrame\n",
    "ref_df = pd.DataFrame([\n",
    "    {\"source_file\": src, \"mentions\": tgt}\n",
    "    for src, tgts in reference_map.items()\n",
    "    for tgt in tgts\n",
    "])\n",
    "\n",
    "# Group mentions into lists per source_file\n",
    "ref_summary = ref_df.groupby(\"source_file\")[\"mentions\"].apply(list).reset_index()\n",
    "\n",
    "# Some source_file names are also in mentions, therefore they aren't a source file anymore\n",
    "source_files = set(ref_summary[\"source_file\"])\n",
    "mentioned_files = set(file for mention_list in ref_summary[\"mentions\"] for file in mention_list)\n",
    "common_files = source_files.intersection(mentioned_files)\n",
    "\n",
    "# Filter out common files from the DataFrame\n",
    "filtered_ref_summary = ref_summary[~ref_summary[\"source_file\"].isin(common_files)].reset_index(drop=True)\n",
    "\n",
    "filtered_ref_summary.to_csv(\"file_reference_map.csv\", index=False)\n",
    "print(filtered_ref_summary.head(10))\n",
    "print(\"Unique file paths in original DataFrame:\", df.stack().nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0bce5a-16de-48e6-83df-240b732f1a20",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Check for more linkages in the files + add the missing source files from the chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10f92af7-5075-4d8d-9251-113f6e360f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from ast import literal_eval\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe46121c-4ff4-4e45-b248-8edd4d12be74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['source_file', 'mentions', 'mentions2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def is_readable_text_file(path):\n",
    "    return any(path.endswith(ext) for ext in ['txt', 'md', 'rst', 'tex', 'nfo', 'readme', 'rtf', 'doc', 'docx',\n",
    "    'cfg', 'conf', 'config', 'ini', 'json', 'yaml', 'yml', 'toml',\n",
    "    'log', 'lst', 'cnf', 'properties', 'prefs',\n",
    "    'csv', 'tsv', 'dat', 'db', 'dbf', 'sql', 'xml',\n",
    "    'ssv', 'psv', 'jsonl', 'parquet', 'orc',\n",
    "    'html', 'htm', 'xhtml', 'xht', 'css', 'js',\n",
    "    'jsx', 'ts', 'tsx', 'vue', 'erb', 'ejs', 'jsp',\n",
    "    'liquid', 'handlebars', 'hbs', 'mustache',\n",
    "    'py', 'pyw', 'ipynb', 'java', 'c', 'cpp', 'h', 'hpp', 'cs',\n",
    "    'sh', 'bash', 'zsh', 'ksh', 'bat', 'cmd', 'ps1',\n",
    "    'r', 'jl', 'pl', 'pm', 'rb', 'go', 'lua', 'php',\n",
    "    'swift', 'scala', 'dart', 'asm', 'groovy',\n",
    "    'rmd', 'sage', 'nb',\n",
    "    'env', 'gradle', 'makefile', 'mak', 'mk',\n",
    "    'dockerfile', 'gitignore', 'gitattributes', 'gitmodules',\n",
    "    'cmake', 'make', 'ninja', 'build',\n",
    "    'manifest', 'manifest.json', 'vtt', 'srt', 'resx', 'strings',\n",
    "    'lang', 'po', 'mo', 'pot', 'msg', 'textbundle',\n",
    "    'rego', 'tf', 'tfvars', 'cue', 'bzl', 'bazel', 'nix', 'dhall',\n",
    "    'adoc', 'asciidoc', 'creole', 'mediawiki', 'wiki', 'org',\n",
    "    'eml', 'msg', 'mbox', 'mail', 'ics', 'vcf'])\n",
    "\n",
    "mentions2_list = []\n",
    "\n",
    "# Iterate over each row of the summary\n",
    "for _, row in filtered_ref_summary.iterrows():\n",
    "    mention_dict = {}\n",
    "\n",
    "    # For each mentioned file read file contents and create a dictionary key: location - values: file contents\n",
    "    for mentioned_file in row[\"mentions\"]:\n",
    "        if not is_readable_text_file(mentioned_file):\n",
    "            continue  # Skip unreadable formats\n",
    "\n",
    "        # Read file content\n",
    "        content = read_file(mentioned_file)\n",
    "        if content in [None, \"Failed to open file\"]:\n",
    "            continue\n",
    "\n",
    "        # Search for any file-type references using regex\n",
    "        # Match anything with a name and known file extension\n",
    "        found_files = re.findall(r'[\\w\\-/]*\\.\\w+', content)\n",
    "\n",
    "        # Filter duplicates and add to the mention_dict\n",
    "        if found_files:\n",
    "            mention_dict[mentioned_file] = list(set(found_files))\n",
    "\n",
    "    mentions2_list.append(mention_dict)\n",
    "\n",
    "# Add this as a new column\n",
    "filtered_ref_summary[\"mentions2\"] = mentions2_list\n",
    "\n",
    "# Get the list of chat files from result_df - LLM clasification\n",
    "chat_files = result_df['chats'].dropna().unique()\n",
    "\n",
    "# Get the list of source files already in filtered_ref_summary\n",
    "source_files = filtered_ref_summary['source_file'].dropna().unique()\n",
    "\n",
    "# Find missing chat files and collect chat files that are not found in source files\n",
    "existing_source_files = set(source_files)\n",
    "missing_chat_files = []\n",
    "for chat_file in chat_files:\n",
    "    if chat_file not in existing_source_files:\n",
    "        missing_chat_files.append(chat_file)\n",
    "\n",
    "# Create a new DataFrame with empty mentions and linkage columns\n",
    "new_rows = pd.DataFrame({\n",
    "    'source_file': missing_chat_files,\n",
    "})\n",
    "\n",
    "# Append the new rows to ref_summary\n",
    "filtered_ref_summary = pd.concat([filtered_ref_summary, new_rows], ignore_index=True)\n",
    "        \n",
    "# Save the updated DataFrame to CSV\n",
    "filtered_ref_summary.to_csv(\"file_reference_map_extended.csv\", index=False)\n",
    "print(filtered_ref_summary.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e01b1-5dd6-4678-b3d9-e48e859045f1",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Structuring the Data into CSV - Step 1 all MD files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37835ee2-1c35-4cf7-a08e-89d2e02c5621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9d08d1c-afe4-4cae-b359-779c917f3689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a folder and dumb all csvs\n",
    "output_dir = \"csvs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all MD files from the DataFrame\n",
    "all_file_paths = pd.unique(result_df.values.ravel('K'))\n",
    "md_files = []\n",
    "for file_path in all_file_paths:\n",
    "    if isinstance(file_path, str) and file_path.endswith(\".md\") and os.path.exists(file_path):\n",
    "        md_files.append(file_path)\n",
    "        \n",
    "# Use the LLM's clasification of MD files\n",
    "chat_files = set(result_df['chats'].dropna().astype(str))\n",
    "\n",
    "# Extract visible text + ANY file reference \n",
    "def extract_text_and_files(td):\n",
    "    text = td.get_text(strip=True)\n",
    "    links = []\n",
    "    for tag in td.find_all(['a', 'img']):\n",
    "        link = tag.get('href') or tag.get('src')\n",
    "        if link:\n",
    "            fname = os.path.basename(link.strip(\"'\\\"\"))\n",
    "            links.append(fname)\n",
    "    return text + (\" \" + \" \".join(links) if links else \"\")\n",
    "\n",
    "# This method extracts chat table from HTML\n",
    "def process_chat_md(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        rows = soup.find_all(\"tr\")[1:]\n",
    "        data = []\n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) == 4:\n",
    "                data.append({\n",
    "                    \"Time\": cols[0].get_text(strip=True),\n",
    "                    \"From\": cols[1].get_text(strip=True),\n",
    "                    \"To\": cols[2].get_text(strip=True),\n",
    "                    \"Message\": extract_text_and_files(cols[3])\n",
    "                })\n",
    "        return pd.DataFrame(data) if data else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chat file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract the contents of the other file types that the LLM classified\n",
    "def process_reference_md(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        matches = []\n",
    "        for tag in soup.find_all(['a', 'img']):\n",
    "            link = tag.get('href') or tag.get('src')\n",
    "            if link:\n",
    "                fname = os.path.basename(link.strip(\"'\\\"\"))\n",
    "                if not fname.startswith(\"0-\"):\n",
    "                    matches.append(fname)\n",
    "        matches = list(set(matches))\n",
    "        return pd.DataFrame([{\"File_Name\": f} for f in matches]) if matches else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing reference file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Loop through all MD filesand call the above methods for the afferent file type\n",
    "for file_path in md_files:\n",
    "    csv_name = os.path.basename(file_path).replace(\".md\", \".csv\")\n",
    "    csv_path = os.path.join(output_dir, csv_name)\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        if file_path in chat_files:\n",
    "            df = process_chat_md(file_path)\n",
    "        else:\n",
    "            df = process_reference_md(file_path)\n",
    "\n",
    "        if df is not None:\n",
    "            df.to_csv(csv_path, index=False)\n",
    "        else:\n",
    "            print(f\"Couldn't save to CSV: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "070f1c87-70d5-4699-8220-fecbc4f05457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f4e4289-8394-4707-b8d9-687061d28d62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>mentions</th>\n",
       "      <th>mentions2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>csvs/01cdc26f-e773-4ad7-8808-d04abf16aae7.csv</td>\n",
       "      <td>[0/png/01cdc26f-e773-4ad7-8808-d04abf16aae7_1_...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>csvs/10.csv</td>\n",
       "      <td>[0/png/0-6bcc0131-e4ad-421e-bb1f-d8ebe5eeec7b....</td>\n",
       "      <td>{'csvs/12756724-394c-4576-b373-7c53f1abbd94.cs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>csvs/13.csv</td>\n",
       "      <td>[0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e....</td>\n",
       "      <td>{'csvs/585875ff-f8c5-4a02-acd7-fef37dc9ff11.cs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>csvs/15.csv</td>\n",
       "      <td>[0/png/0-b8cea3b1-4dde-4438-9b1a-6faf690bbad0....</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>csvs/178e3898-903d-47cf-bfbe-061e7dc18895.csv</td>\n",
       "      <td>[0/png/178e3898-903d-47cf-bfbe-061e7dc18895_8....</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     source_file  \\\n",
       "0  csvs/01cdc26f-e773-4ad7-8808-d04abf16aae7.csv   \n",
       "1                                    csvs/10.csv   \n",
       "2                                    csvs/13.csv   \n",
       "3                                    csvs/15.csv   \n",
       "4  csvs/178e3898-903d-47cf-bfbe-061e7dc18895.csv   \n",
       "\n",
       "                                            mentions  \\\n",
       "0  [0/png/01cdc26f-e773-4ad7-8808-d04abf16aae7_1_...   \n",
       "1  [0/png/0-6bcc0131-e4ad-421e-bb1f-d8ebe5eeec7b....   \n",
       "2  [0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e....   \n",
       "3  [0/png/0-b8cea3b1-4dde-4438-9b1a-6faf690bbad0....   \n",
       "4  [0/png/178e3898-903d-47cf-bfbe-061e7dc18895_8....   \n",
       "\n",
       "                                           mentions2  \n",
       "0                                                 {}  \n",
       "1  {'csvs/12756724-394c-4576-b373-7c53f1abbd94.cs...  \n",
       "2  {'csvs/585875ff-f8c5-4a02-acd7-fef37dc9ff11.cs...  \n",
       "3                                                 {}  \n",
       "4                                                 {}  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dir = \"csvs\"\n",
    "\n",
    "# Replace MD with CSV if the corresponding file exists\n",
    "def replace_md_with_csv(item):\n",
    "    if isinstance(item, str) and item.endswith(\".md\"):\n",
    "        base_name = Path(item).stem  # .stem extracts the filename with no extension\n",
    "        # CSV search\n",
    "        csv_path = os.path.join(csv_dir, f\"{base_name}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            return csv_path\n",
    "        else:\n",
    "            print(f\"CSV {item} not found\")\n",
    "    return item\n",
    "\n",
    "# Recursively handle strings, lists, and dictionaries\n",
    "def process_column_cell(cell):\n",
    "    if isinstance(cell, str):\n",
    "        return replace_md_with_csv(cell)\n",
    "    \n",
    "    elif isinstance(cell, list):\n",
    "        new_list = []\n",
    "        for item in cell:\n",
    "            new_item = process_column_cell(item)\n",
    "            new_list.append(new_item)\n",
    "        return new_list\n",
    "\n",
    "    elif isinstance(cell, dict):\n",
    "        new_dict = {}\n",
    "        for key, value in cell.items():\n",
    "            new_key = replace_md_with_csv(key)\n",
    "            new_value = process_column_cell(value)\n",
    "            new_dict[new_key] = new_value\n",
    "        return new_dict\n",
    "\n",
    "    else:\n",
    "        return cell\n",
    "    \n",
    "for col in filtered_ref_summary.columns:\n",
    "    filtered_ref_summary[col] = filtered_ref_summary[col].apply(process_column_cell)\n",
    "\n",
    "filtered_ref_summary.to_csv(\"filtered_ref_summary_csv_replaced.csv\", index=False)\n",
    "filtered_ref_summary.head()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d20edad-a67b-49d6-a04e-c7dd5816c91a",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Structuring the Data into CSV - Step 2 OCR on image files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a0616-c062-4478-b6ef-2fd80fe5c61f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "LIST OF FILES FOR TEST: 5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_0.png, 08a6bcd3-6477-4252-8f35-4f8f80d114f9.png, 1afcf93d-50f1-4f1e-896d-87b0da7519f7.png, 32eb7662-f212-4811-a7c1-1cfeb121cd99.png, 6bcc0131-e4ad-421e-bb1f-d8ebe5eeec7b.png, 79d9b7f2-cfe4-4615-9b75-8fea33fc0c9d.png, 4ae9bf34-c16c-4684-aa92-fec65a151275.png, 5a84cde3-7175-4044-8c88-d4c883a8fd38.png, e705d192-90ee-4fd1-9dcd-061958d1817f.png, 912204cb-8ab7-48b8-9abf-d803f3804d08_4.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a55ce6-7b7e-483b-8db8-88434d07f3f3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# **TEST-1: Extraction with LLAVA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbe48e62-26a6-411a-b0ba-3aeaaa33b381",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_0.png:\n",
      "The Chinese text on the image is: \"未来医疗技术与科学研究开展方向\".\n",
      "\n",
      "08a6bcd3-6477-4252-8f35-4f8f80d114f9.png:\n",
      "I'm unable to process images or provide translations directly from this interface. If you need assistance with the Chinese text in the image, please type out the text in a separate message, and I will be happy to help you with its translation or any other question you may have!\n",
      "\n",
      "1afcf93d-50f1-4f1e-896d-87b0da7519f7.png:\n",
      "你好！我有一个问题想要请教。我现在正在做一个简单的项目，需要购买一些电子设备。如果可以的话，能否向您请教一下价格和采购建议？谢谢！\n",
      "\n",
      "32eb7662-f212-4811-a7c1-1cfeb121cd99.png:\n",
      "The image you've provided shows a screenshot of a computer interface with several lines of Chinese text. Here is the extracted Chinese text from the visible parts of the screen:\n",
      "\n",
      "```\n",
      "发送人：陈某某\n",
      "发送时间：2014年6月8日 下午9点37分\n",
      "标题：关于一些信息的提供\n",
      "内容：\n",
      "\n",
      "这是一个测试文本。请问我应该在哪里写？\n",
      "```\n",
      "\n",
      "Please note that the text may be incomplete as it only covers part of what is visible in the image.\n",
      "\n",
      "6bcc0131-e4ad-421e-bb1f-d8ebe5eeec7b.png:\n",
      "抱歉，由于此图像的质量不够高，我无法识别并提取中文。请提供一个清晰的图片或文字，以便我为您提取中文。\n",
      "\n",
      "79d9b7f2-cfe4-4615-9b75-8fea33fc0c9d.png:\n",
      "投票数量：1000\n",
      "请选择/GM印章\n",
      "\n",
      "4ae9bf34-c16c-4684-aa92-fec65a151275.png:\n",
      "The image contains Chinese text that reads \"设置\" which means \"settings.\" It seems to be a screenshot of a software or system settings interface with some options visible. The actual content of the window, such as the names of the options and their values, is not clearly visible due to the angle and quality of the image.\n",
      "\n",
      "5a84cde3-7175-4044-8c88-d4c883a8fd38.png:\n",
      "很抱歉，我无法为您提供文字识别服务。\n",
      "\n",
      "e705d192-90ee-4fd1-9dcd-061958d1817f.png:\n",
      "哇哦，这个是我最喜欢的人！👯‍♀️\n",
      "\n",
      "912204cb-8ab7-48b8-9abf-d803f3804d08_4.png:\n",
      "Here are the lines of text in Chinese from the image:\n",
      "\n",
      "- 对于第三层，基于微服务架构。\n",
      "- 采用Spring Boot作为开发工具。\n",
      "- 使用Nacos进行服务注册与发现。\n",
      "- 使用MongoDB作为数据库存储。\n",
      "- 基于Redis实现缓存功能。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# List of image file paths\n",
    "image_paths = [\n",
    "    \"0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_0.png\", \n",
    "    \"0/png/08a6bcd3-6477-4252-8f35-4f8f80d114f9.png\", \n",
    "    \"0/png/1afcf93d-50f1-4f1e-896d-87b0da7519f7.png\", \n",
    "    \"0/png/32eb7662-f212-4811-a7c1-1cfeb121cd99.png\", \n",
    "    \"0/png/6bcc0131-e4ad-421e-bb1f-d8ebe5eeec7b.png\", \n",
    "    \"0/png/79d9b7f2-cfe4-4615-9b75-8fea33fc0c9d.png\", \n",
    "    \"0/png/4ae9bf34-c16c-4684-aa92-fec65a151275.png\", \n",
    "    \"0/png/5a84cde3-7175-4044-8c88-d4c883a8fd38.png\", \n",
    "    \"0/png/e705d192-90ee-4fd1-9dcd-061958d1817f.png\", \n",
    "    \"0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_4.png\"\n",
    "]\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"EXTRACT AND ONLY GIVE ME THE Chinese text in the given image. NO TRANSLATION, ONLY extract the TEXT IN CHINESE MANDARIN FROM THE PNG FILE\"\n",
    "\n",
    "# Process each image\n",
    "for image_path in image_paths:\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": \"llava:13b\",\n",
    "            \"prompt\": prompt,\n",
    "            \"images\": [image_b64],\n",
    "            \"stream\": False\n",
    "        },\n",
    "    )\n",
    "\n",
    "    filename = os.path.basename(image_path)\n",
    "    if response.ok:\n",
    "        output_text = response.json()[\"response\"]\n",
    "    else:\n",
    "        output_text = f\"Error: {response.text}\"\n",
    "\n",
    "    print(f\"{filename}:\\n{output_text.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9dea9d-50dd-4474-aaa2-055224ce8a0c",
   "metadata": {},
   "source": [
    "# **Data has been added from MacOS OCR**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd84d99-1674-441a-b8ca-7c5527bbefd8",
   "metadata": {},
   "source": [
    "# *Changing the files in the chats with the extracted output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa828773-c458-46f4-a0c6-eed040a10b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the csv file and for each file.md or image.png replace with the OCR output from the folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dde838-42c2-4ed1-bd55-8031cc18d077",
   "metadata": {},
   "source": [
    "# **DATA TRANSLATION Verification - 3 LLMs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8e79992-2b44-4424-98d4-c5783bd96644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff4c91a1-3686-4dc0-a215-7c5f89efd28d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           other  \\\n",
      "0  csvs/9d7bc879-3250-4013-ac04-5ff9bd6dff40.csv   \n",
      "1                                           None   \n",
      "2                                           None   \n",
      "3                                           None   \n",
      "4                                           None   \n",
      "\n",
      "                                          images        chats  \n",
      "0  csvs/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.csv  csvs/28.csv  \n",
      "1  csvs/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.csv   csvs/5.csv  \n",
      "2  csvs/3348953d-66e9-4cac-8675-65bb5f2ef929.csv  csvs/38.csv  \n",
      "3  csvs/07f179c5-5705-4dbd-94a7-66eed1e066b0.csv  csvs/18.csv  \n",
      "4  csvs/01cdc26f-e773-4ad7-8808-d04abf16aae7.csv   csvs/1.csv  \n"
     ]
    }
   ],
   "source": [
    "# Use the csvs folder and the LLM classification of MD files to create a new df that stores the csvs into categories.\n",
    "\n",
    "csv_folder = \"csvs\"\n",
    "\n",
    "csv_mapping = {}\n",
    "# Loop through each file in the CSV folder\n",
    "for filename in os.listdir(csv_folder):\n",
    "    # Check for csv\n",
    "    if filename.endswith('.csv'):\n",
    "        # Replace the \".csv\" extension with \".md\" to create the key\n",
    "        md_filename = filename.replace('.csv', '.md')\n",
    "        \n",
    "        # Set the value to the full path to the CSV file \n",
    "        csv_full_path = os.path.join(csv_folder, filename)\n",
    "        \n",
    "        # Add the key-value pair to the mapping\n",
    "        csv_mapping[md_filename] = csv_full_path\n",
    "\n",
    "# Step 2: Define function to replace .md path with .csv path\n",
    "def replace_md_with_csv(val):\n",
    "    if isinstance(val, str):\n",
    "        base = os.path.basename(val)\n",
    "        return csv_mapping.get(base, val)\n",
    "    return val\n",
    "\n",
    "df_csvs_llm = result_df.applymap(replace_md_with_csv)\n",
    "df_csvs_llm.to_csv(\"updated_with_csv_paths.csv\", index=False)\n",
    "print(df_csvs_llm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd9bec-a3c0-48d0-8366-f54ca8f4ae1c",
   "metadata": {},
   "source": [
    "# **TEST-1: gemma3:27b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703a4971-7d0b-4c8e-ae00-78706f144c00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with Ollama: 100%|██████████| 149/149 [07:36<00:00,  3.06s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "csv_path = 'csvs/10.csv'\n",
    "output_path = '3llms_10.csv'  # Output in project root directory\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "def get_context(df, index, window=5):\n",
    "    start = max(0, index - window)\n",
    "    end = min(len(df), index + window + 1)\n",
    "    return \"\\n\".join(df['Message'].iloc[start:end].dropna())\n",
    "\n",
    "def query_ollama(prompt, model=\"gemma3:27b\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=90)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama error: {e}\")\n",
    "        return \"[Translation Error]\"\n",
    "\n",
    "def translate_row(index):\n",
    "    target = df.at[index, 'Message']\n",
    "    if pd.isna(target):\n",
    "        return index, \"\"\n",
    "    context = get_context(df, index)\n",
    "    prompt = (\n",
    "        \"You are translating Chinese messages to English. Below is a series of related messages. \"\n",
    "        \"Use the full context to understand the meaning, but only translate the specific message provided.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Message to translate:\\n{target}\\n\\n\"\n",
    "        \"ONLY RETURN the English translation of the message, ANYTHING ELSE IS FORBIDDEN! \"\n",
    "        \"If you encounter a filename or file reference, preserve it exactly as it appears, DON'T add anything!\"\n",
    "    )\n",
    "    translation = query_ollama(prompt)\n",
    "    return index, translation\n",
    "\n",
    "translations = [\"\"] * len(df)\n",
    "\n",
    "futures = []\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for index in df.index:\n",
    "        futures.append(executor.submit(translate_row, index))\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Translating with Ollama\"):\n",
    "        idx, result = future.result()\n",
    "        translations[idx] = result\n",
    "\n",
    "translated_df = pd.DataFrame({'gemma3:27b': translations})\n",
    "translated_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ffc2e2-1b26-46f1-81bb-f02c5ffa8fbf",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# **TEST-2: 7shi/llama-translate:8b-q4_K_M**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11daa7c9-848a-43f1-92a8-00c1d2dfa6d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with 7shi/llama-translate:8b-q4_K_M: 100%|██████████| 149/149 [08:52<00:00,  3.57s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "source_csv_path = 'csvs/10.csv'\n",
    "existing_translation_path = '3llms_10.csv'\n",
    "new_model_name = '7shi/llama-translate:8b-q4_K_M'\n",
    "\n",
    "# Load original messages and previously translated file\n",
    "source_df = pd.read_csv(source_csv_path)\n",
    "translated_df = pd.read_csv(existing_translation_path)\n",
    "\n",
    "def get_context(df, index, window=5):\n",
    "    start = max(0, index - window)\n",
    "    end = min(len(df), index + window + 1)\n",
    "    return \"\\n\".join(df['Message'].iloc[start:end].dropna())\n",
    "\n",
    "def query_ollama(prompt, model):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=90)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama error: {e}\")\n",
    "        return \"[Translation Error]\"\n",
    "\n",
    "def translate_row(index):\n",
    "    target = source_df.at[index, 'Message']\n",
    "    if pd.isna(target):\n",
    "        return index, \"\"\n",
    "    context = get_context(source_df, index)\n",
    "    prompt = (\n",
    "        \"Transalte Mandarin to English. Below is a series of related messages. \"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Message to translate:\\n{target}\\n\\n\"\n",
    "    )\n",
    "    translation = query_ollama(prompt, model=new_model_name)\n",
    "    return index, translation\n",
    "\n",
    "# Prepare for parallel processing\n",
    "translations = [\"\"] * len(source_df)\n",
    "futures = []\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for index in source_df.index:\n",
    "        futures.append(executor.submit(translate_row, index))\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Translating with {new_model_name}\"):\n",
    "        idx, result = future.result()\n",
    "        translations[idx] = result\n",
    "\n",
    "translated_df[new_model_name] = translations\n",
    "translated_df.to_csv(existing_translation_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7476dfc-6315-4a48-a5c3-bf0fe4896651",
   "metadata": {},
   "source": [
    "# **TEST-3: yi:34b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cd03420-4ab3-40c0-8c22-dca324cf0318",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with yi:34b: 100%|██████████| 149/149 [07:20<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# File paths\n",
    "source_csv_path = 'csvs/10.csv'\n",
    "existing_translation_path = '3llms_10.csv'\n",
    "new_model_name = 'yi:34b'\n",
    "\n",
    "# Load original source messages and existing translations\n",
    "source_df = pd.read_csv(source_csv_path)\n",
    "translated_df = pd.read_csv(existing_translation_path)\n",
    "\n",
    "# Context window function\n",
    "def get_context(df, index, window=5):\n",
    "    start = max(0, index - window)\n",
    "    end = min(len(df), index + window + 1)\n",
    "    return \"\\n\".join(df['Message'].iloc[start:end].dropna())\n",
    "\n",
    "def query_ollama(prompt, model):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=90)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama error: {e}\")\n",
    "        return \"[Translation Error]\"\n",
    "\n",
    "# Translate a single row\n",
    "def translate_row(index):\n",
    "    target = source_df.at[index, 'Message']\n",
    "    if pd.isna(target):\n",
    "        return index, \"\"\n",
    "    context = get_context(source_df, index)\n",
    "    prompt = (\n",
    "        \"You are translating Chinese messages to English. Below is a series of related messages. \"\n",
    "        \"Use the full context to understand the meaning, but only translate the specific message provided.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Message to translate:\\n{target}\\n\\n\"\n",
    "        \"ONLY RETURN the English translation of the message, ANYTHING ELSE IS FORBIDDEN! \"\n",
    "        \"If you encounter a filename or file reference, preserve it exactly as it appears, DON'T add anything!\"\n",
    "    )\n",
    "    translation = query_ollama(prompt, model=new_model_name)\n",
    "    return index, translation\n",
    "\n",
    "# Run translations in parallel\n",
    "translations = [\"\"] * len(source_df)\n",
    "futures = []\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for index in source_df.index:\n",
    "        futures.append(executor.submit(translate_row, index))\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Translating with {new_model_name}\"):\n",
    "        idx, result = future.result()\n",
    "        translations[idx] = result\n",
    "\n",
    "translated_df[new_model_name] = translations\n",
    "translated_df.to_csv(existing_translation_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465aca0c-8ee6-4a66-909f-eff88a49d588",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Levenshtein Distance Between the 3 Translations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ee30282-f681-4e1e-8747-ed4c7616d232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "from itertools import combinations\n",
    "\n",
    "csv_path = '3llms_10.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Identify all translation columns \n",
    "translation_columns = [col for col in df.columns if not col.startswith('Levenshtein')]\n",
    "\n",
    "# Compute Levenshtein distance for all possible combinations\n",
    "for col1, col2 in combinations(translation_columns, 2):\n",
    "    new_col_name = f\"Levenshtein ({col1} vs {col2})\"\n",
    "    df[new_col_name] = [\n",
    "        Levenshtein.distance(str(a), str(b)) if pd.notna(a) and pd.notna(b) else None\n",
    "        for a, b in zip(df[col1], df[col2])\n",
    "    ]\n",
    "\n",
    "# Save updated DataFrame back to the same CSV\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cfb529-3a71-4da6-b4e4-9cde8065024c",
   "metadata": {},
   "source": [
    "# **CHATS TRANSLATION - GEMMA3:27B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a33b558c-25cf-451e-9ccd-92892c0ef1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: csvs/28.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/28.csv: 100%|██████████| 46/46 [02:55<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/28.csv\n",
      "\n",
      "Processing: csvs/5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/5.csv: 100%|██████████| 17/17 [00:24<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/5.csv\n",
      "\n",
      "Processing: csvs/38.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/38.csv: 100%|██████████| 21/21 [01:18<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/38.csv\n",
      "\n",
      "Processing: csvs/18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/18.csv: 100%|██████████| 84/84 [02:10<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/18.csv\n",
      "\n",
      "Processing: csvs/1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/1.csv:   7%|▋         | 550/8290 [20:25<13:53:57,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/1.csv:  68%|██████▊   | 5642/8290 [3:25:59<6:53:56,  9.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/1.csv:  69%|██████▉   | 5711/8290 [3:30:46<5:57:09,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/1.csv:  77%|███████▋  | 6419/8290 [4:01:12<50:27,  1.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/1.csv:  87%|████████▋ | 7230/8290 [4:35:32<2:34:48,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/1.csv:  88%|████████▊ | 7268/8290 [4:37:49<2:13:42,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/1.csv: 100%|██████████| 8290/8290 [5:20:43<00:00,  2.32s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/1.csv\n",
      "\n",
      "Processing: csvs/19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/19.csv: 100%|██████████| 21/21 [01:04<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/19.csv\n",
      "\n",
      "Processing: csvs/29.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/29.csv: 100%|██████████| 80/80 [03:05<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/29.csv\n",
      "\n",
      "Processing: csvs/4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/4.csv: 100%|██████████| 513/513 [17:16<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/4.csv\n",
      "\n",
      "Processing: csvs/39.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/39.csv: 100%|██████████| 821/821 [27:27<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/39.csv\n",
      "\n",
      "Processing: csvs/16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/16.csv: 100%|██████████| 149/149 [04:35<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/16.csv\n",
      "\n",
      "Processing: csvs/22.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/22.csv: 100%|██████████| 91/91 [03:16<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/22.csv\n",
      "\n",
      "Processing: csvs/32.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/32.csv: 100%|██████████| 86/86 [02:35<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/32.csv\n",
      "\n",
      "Processing: csvs/26.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/26.csv: 100%|██████████| 6/6 [00:17<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/26.csv\n",
      "\n",
      "Processing: csvs/12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/12.csv: 100%|██████████| 392/392 [13:52<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/12.csv\n",
      "\n",
      "Processing: csvs/36.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/36.csv: 100%|██████████| 199/199 [05:28<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/36.csv\n",
      "\n",
      "Processing: csvs/27.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/27.csv: 100%|██████████| 29/29 [02:44<00:00,  5.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n",
      "Finished and saved: csvs/27.csv\n",
      "\n",
      "Processing: csvs/13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/13.csv: 100%|██████████| 244/244 [08:18<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/13.csv\n",
      "\n",
      "Processing: csvs/37.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/37.csv:  10%|▉         | 32/329 [02:47<52:10, 10.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/37.csv:  16%|█▋        | 54/329 [04:46<21:03,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/37.csv:  27%|██▋       | 90/329 [07:55<16:51,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/37.csv: 100%|██████████| 329/329 [16:39<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/37.csv\n",
      "\n",
      "Processing: csvs/17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/17.csv: 100%|██████████| 55/55 [01:42<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/17.csv\n",
      "\n",
      "Processing: csvs/23.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/23.csv: 100%|██████████| 24/24 [00:45<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/23.csv\n",
      "\n",
      "Processing: csvs/33.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/33.csv: 100%|██████████| 31/31 [00:52<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/33.csv\n",
      "\n",
      "Processing: csvs/24.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/24.csv: 100%|██████████| 55/55 [01:57<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/24.csv\n",
      "\n",
      "Processing: csvs/41.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/41.csv: 100%|██████████| 322/322 [10:44<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/41.csv\n",
      "\n",
      "Processing: csvs/10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/10.csv: 100%|██████████| 149/149 [06:13<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/10.csv\n",
      "\n",
      "Processing: csvs/9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/9.csv: 100%|██████████| 235/235 [08:11<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/9.csv\n",
      "\n",
      "Processing: csvs/34.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/34.csv: 100%|██████████| 426/426 [13:25<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/34.csv\n",
      "\n",
      "Processing: csvs/14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/14.csv: 100%|██████████| 44/44 [01:18<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/14.csv\n",
      "\n",
      "Processing: csvs/20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/20.csv: 100%|██████████| 68/68 [02:06<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/20.csv\n",
      "\n",
      "Processing: csvs/30.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/30.csv: 100%|██████████| 322/322 [08:36<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/30.csv\n",
      "\n",
      "Processing: csvs/15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/15.csv: 100%|██████████| 199/199 [06:06<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/15.csv\n",
      "\n",
      "Processing: csvs/21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/21.csv: 100%|██████████| 1046/1046 [24:47<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/21.csv\n",
      "\n",
      "Processing: csvs/31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/31.csv: 100%|██████████| 44/44 [01:06<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/31.csv\n",
      "\n",
      "Processing: csvs/40.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/40.csv: 100%|██████████| 129/129 [04:50<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/40.csv\n",
      "\n",
      "Processing: csvs/11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/11.csv: 100%|██████████| 125/125 [04:12<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/11.csv\n",
      "\n",
      "Processing: csvs/35.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/35.csv: 100%|██████████| 196/196 [06:40<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/35.csv\n",
      "\n",
      "Processing: csvs/3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/3.csv: 100%|██████████| 23/23 [00:37<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/3.csv\n",
      "\n",
      "Processing: csvs/7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/7.csv: 100%|██████████| 196/196 [05:34<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/7.csv\n",
      "\n",
      "Processing: csvs/6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/6.csv: 100%|██████████| 136/136 [03:08<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/6.csv\n",
      "\n",
      "Processing: csvs/2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/2.csv: 100%|██████████| 500/500 [16:03<00:00,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/2.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Chats - CSV loop\n",
    "for csv_path in df_csvs_llm['chats']:\n",
    "    print(f\"Processing: {csv_path}\")\n",
    "    \n",
    "    # Load the individual CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['message_translation'] = \"\"\n",
    "\n",
    "    # Function to get context around the target message\n",
    "    def get_context(df, index, window=5):\n",
    "        start = max(0, index - window)\n",
    "        end = min(len(df), index + window + 1)\n",
    "        return \"\\n\".join(df['Message'].iloc[start:end].dropna())\n",
    "\n",
    "    def query_ollama(prompt, model=\"gemma3:27b\"):\n",
    "        url = \"http://localhost:11434/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(url, json=payload, timeout=90)\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"response\"].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Ollama error: {e}\")\n",
    "            return \"[Translation Error]\"\n",
    "\n",
    "    # Function to process one row\n",
    "    def translate_row(index):\n",
    "        target = df.at[index, 'Message']\n",
    "        if pd.isna(target):\n",
    "            return index, \"\"\n",
    "        context = get_context(df, index)\n",
    "        prompt = (\n",
    "            \"You are translating Chinese messages to English. Below is a series of related messages. \"\n",
    "            \"Use the full context to understand the meaning, but only translate the specific message provided.\\n\\n\"\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Message to translate:\\n{target}\\n\\n\"\n",
    "            \"ONLY RETURN the English translation of the message, ANYTHING ELSE IS FORBIDDEN! \"\n",
    "            \"If you encounter a filename or file reference, preserve it exactly as it appears, DON'T add anything!\"\n",
    "        )\n",
    "        translation = query_ollama(prompt)\n",
    "        return index, translation\n",
    "\n",
    "    # Run translations in parallel\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for index in df.index:\n",
    "            futures.append(executor.submit(translate_row, index))\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Translating: {csv_path}\"):\n",
    "            idx, result = future.result()\n",
    "            df.at[idx, 'message_translation'] = result\n",
    "\n",
    "    # Save updated DataFrame back to the original file\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Finished and saved: {csv_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d84e49b-797c-4c4b-b63e-f3d19968c59b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Columns:\n",
      "['Time', 'From', 'To', 'Message']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV\n",
    "df = pd.read_csv(\"csvs/10.csv\")\n",
    "\n",
    "# Print all column names to verify exact labels\n",
    "print(\"CSV Columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Drop the specified columns (only if they exist)\n",
    "columns_to_drop = [\"message_translation\", \"financial_detection\"]\n",
    "df_cleaned = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# Overwrite the original file\n",
    "df_cleaned.to_csv(\"csvs/10.csv\", index=False)  # Overwriting the same file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f24fa3-ebc0-454c-a5a4-6e03836fdf0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **USER ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03abf627-5c58-439b-8f57-57ae9bf4e360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c1b784d-2a5d-4830-92a8-4e3a3fee0cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages sent per user across all chat logs:\n",
      "- lengmo: 4981\n",
      "- Shutd0wn: 3675\n",
      "- wxid_5390224027312: 1409\n",
      "- wxid_7p054rmzkhqf21: 894\n",
      "- wxid_wh6x59w70y3r22: 620\n",
      "- gzp1991101: 603\n",
      "- wxid_zb45i0rc71yk21: 590\n",
      "- wxid_70w3p1jin84k22: 523\n",
      "- wxid_hlmnhsq64tt722: 483\n",
      "- adpw90: 235\n",
      "- qq78263462: 225\n",
      "- wxid_12n748um1thl21: 192\n",
      "- wxid_mgh25nentc4u22: 190\n",
      "- just910420: 154\n",
      "- wxid_c9yv0nsla3yn22: 116\n",
      "- nullroot: 103\n",
      "- wxid_icges6alg8cl21: 91\n",
      "- wei592628: 87\n",
      "- wxid_soekgggwnfgm21: 79\n",
      "- wxid_nv9bv435fz3722: 77\n",
      "- wxid_xusilpfkh31g21: 61\n",
      "- ken73224: 58\n",
      "- hack05112: 45\n",
      "- tianyi-0608: 45\n",
      "- wxid_kbys0kvzj4ta12: 38\n",
      "- SWEET5683yao: 35\n",
      "- wxid_zbytkn4qjl3r22: 29\n",
      "- wxid_blw54o1q0q5w22: 24\n",
      "- yanzi542766277: 24\n",
      "- dujijiyiqxx: 17\n",
      "- wxid_jcnxegjccqi441: 11\n",
      "- wangchao953541: 9\n",
      "- ibabaimama: 5\n",
      "- zhangxiaoyan0422: 5\n",
      "- snipersk: 5\n",
      "- wxid_q7vkst94g5u011: 5\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file with paths\n",
    "df_csvs = pd.read_csv(\"updated_with_csv_paths.csv\")\n",
    "\n",
    "def count_messages_per_user_from_csv(csv_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {csv_path}: {e}\")\n",
    "        return Counter()\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "    # Identify the 'from' column\n",
    "    from_columns = [col for col in df.columns if col in ['from', 'sender', 'user']]\n",
    "    if not from_columns:\n",
    "        return Counter()\n",
    "\n",
    "    from_col = from_columns[0]\n",
    "\n",
    "    # Drop missing senders and count messages\n",
    "    user_counts = df[from_col].dropna().astype(str).str.strip()\n",
    "    return Counter(user_counts[user_counts != ''])\n",
    "\n",
    "# Initialize a counter\n",
    "all_message_counts = Counter()\n",
    "\n",
    "# Loop through each CSV path in the 'chats' column\n",
    "for csv_path in df_csvs['chats'].dropna().unique():\n",
    "    if isinstance(csv_path, str) and csv_path.endswith('.csv') and os.path.exists(csv_path):\n",
    "        counts = count_messages_per_user_from_csv(csv_path)\n",
    "        all_message_counts.update(counts)\n",
    "\n",
    "# Convert to sorted list and display\n",
    "sorted_user_message_counts = sorted(all_message_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Messages sent per user across all chat logs:\")\n",
    "for user, count in sorted_user_message_counts:\n",
    "    print(f\"- {user}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee30322-2d21-4da7-9850-84cc6b4cf09a",
   "metadata": {},
   "source": [
    "# *RegEx for emails and IP addresses / Terms for finding the leaders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8292b702-3b9e-421d-81d8-da8aeb72bd4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ROLE] Boss, let's first look at the one on DingTalk, it's also convenient for matching and corresponding with the departments. | File: 38.csv | Line: 19\n",
      "[ROLE] It shows that Boss Tong is really awesome. | File: 1.csv | Line: 1862\n",
      "[ROLE] Oh, acting like a boss now. | File: 1.csv | Line: 2601\n",
      "[ROLE] Junli's boss has been worn out by accompanying them. | File: 1.csv | Line: 4099\n",
      "[ROLE] You, as the boss, don't worry about anything else, just focus on performance and revenue. | File: 1.csv | Line: 4285\n",
      "[ROLE] Do you think they would understand that? They'd probably just think it's a matter of the boss giving a word, so they came directly to us. | File: 1.csv | Line: 4505\n",
      "[ROLE] The boss can't undermine the financial system. | File: 1.csv | Line: 4508\n",
      "[ROLE] Their boss won't be back until the end of the month, only the property manager is here, and what he says doesn't count. | File: 1.csv | Line: 6066\n",
      "[ROLE] You should meet with their boss and communicate, the rent won't be defaulted on, but you also need to urge them to pay attention to the matter of the property ownership certificate. | File: 1.csv | Line: 6070\n",
      "[ROLE] The boss will return at the end of the month, we can only communicate by phone. | File: 1.csv | Line: 6071\n",
      "[ROLE] Boss Tong is awesome. | File: 1.csv | Line: 6178\n",
      "[ROLE] Why don't you meet with the people over there? After all, you are the major shareholder, the company boss. | File: 1.csv | Line: 6934\n",
      "[ROLE] I've looked into this, it falls into two categories: one pursuing money, and one pursuing people. For those pursuing money, if we don't count the boss in a single case, the bosses are all overseas, roughly 20- | File: 1.csv | Line: 7230\n",
      "[ROLE] Previously a vice secretary-general of the provincial government, director of the Beijing office, and Dongge's former boss. | File: 1.csv | Line: 7523\n",
      "[ROLE] Dongge is asking me about the investment funds, asking about our recent receipts and when they might be available. His idea, as discussed before, is bridging funds, and he might get them from Anpin. He means that if our investment funds haven't arrived yet, he feels it would be awkward to talk to the boss of Anpin. | File: 1.csv | Line: 8005\n",
      "[ROLE] Mm-hmm, I'm running the market myself, and the boss doesn't bother with small projects worth only a few hundred thousand, and he wouldn't be able to represent them anyway. | File: 39.csv | Line: 32\n",
      "[ROLE] I left, and everyone I had contact with before was still acting normal, except for C总, my close boss, who didn’t say a single word to me. | File: 39.csv | Line: 554\n",
      "[ROLE] Sigh, it's impossible to guess what the boss is thinking. | File: 39.csv | Line: 555\n",
      "[ROLE] ?? What does the boss post on WeChat Moments now that he bought a car? | File: 39.csv | Line: 695\n",
      "[ROLE] The boss bought another car. | File: 39.csv | Line: 706\n",
      "[ROLE] Working hard to contribute to the boss's luxury cars and villas. | File: 39.csv | Line: 744\n",
      "[ROLE] The product was thought up on a whim by the boss, it's unrealistic, how could it be any good? | File: 39.csv | Line: 803\n",
      "[ROLE] This was requested by Duck Boss himself. | File: 32.csv | Line: 41\n",
      "[ROLE] If I were the boss, I'd fire them all. | File: 32.csv | Line: 49\n",
      "[ROLE] Ai... the performance is not good, the products are unreliable, and there’s a lack of funds. The boss is really stressed out now. | File: 32.csv | Line: 51\n",
      "[ROLE] The boss took over. | File: 12.csv | Line: 4\n",
      "[ROLE] The other day I asked if anyone had done HK stuff, and damn, the boss had done twenty or thirty of them there. | File: 12.csv | Line: 45\n",
      "[ROLE] The main problem still lies with boss C. | File: 12.csv | Line: 69\n",
      "[ROLE] C boss is just targeting you sales people. | File: 12.csv | Line: 82\n",
      "[ROLE] Today, boss C and Zhou Weiwei personally took Liang Chenghui to run things in Hubei. | File: 12.csv | Line: 95\n",
      "[ROLE] I still don't understand why boss C is targeting the sales department. | File: 12.csv | Line: 99\n",
      "[ROLE] He and Boss C must be making a fortune. | File: 12.csv | Line: 108\n",
      "[ROLE] Duck Boss is now just a shareholder and has no decision-making power over Sichuan Anxun. | File: 12.csv | Line: 125\n",
      "[ROLE] Duck Boss definitely won't give up easily. | File: 12.csv | Line: 132\n",
      "[ROLE] Your boss really does things... (implying negatively) | File: 12.csv | Line: 238\n",
      "[ROLE] Boss Wei is too greedy. | File: 12.csv | Line: 262\n",
      "[ROLE] Wang Chengfen belongs to C boss, I handle Zhou Weiwei’s. | File: 12.csv | Line: 289\n",
      "[ROLE] CC and Boss Zhou didn't contribute a penny. | File: 12.csv | Line: 293\n",
      "[ROLE] I gave Gong boss to Bai Yu, who gave it to Zhu Xiaojuan. | File: 12.csv | Line: 335\n",
      "[IP] 8.218.67.52:27011 | File: 36.csv | Line: 22\n",
      "[IP] 8.218.67.52:17011 | File: 36.csv | Line: 22\n",
      "[IP] 74.120.172.10:10092 | File: 23.csv | Line: 3\n",
      "[ROLE] But he aligns with the boss's direction. | File: 33.csv | Line: 14\n",
      "[ROLE] The boss definitely knows he's not capable. | File: 33.csv | Line: 15\n",
      "[ROLE] I just reported to the boss, and he asked me to first document today's results and then see the effect. Also, I will be sending you another batch recently, around 100 or so. Let's see the activation probability and accuracy, hope you understand. | File: 10.csv | Line: 33\n",
      "[ROLE] So approximately how many would be within your capacity? The boss also wants to see how the results turn out. | File: 10.csv | Line: 35\n",
      "[ROLE] Okay, thank you. I will report to the boss tomorrow. If he still wants us to cooperate and try it out, I’ll let you know. | File: 10.csv | Line: 37\n",
      "[ROLE] I'll try to talk to the boss and limit the test to within 20 accounts, so it won't be too difficult for you, right? | File: 10.csv | Line: 39\n",
      "[ROLE] Haha, it hasn't actually been put into practice yet, and needs to move from online to a real-world phase, including the final results. Today, during the report to the big boss, he specifically proposed finding a few units to each provide some accounts to see how many can be run simultaneously, so we can measure the final results in a practical way. We also need to get him to change his thinking – technology changes lives [smirk]. | File: 10.csv | Line: 41\n",
      "[ROLE] We're all colleagues, don't say that. I'm also being pressured by the big boss and have no choice [bowing hands]. | File: 10.csv | Line: 45\n",
      "[ROLE] Okay, please send me a list of the specific tasks you can handle so I can report to my boss. | File: 10.csv | Line: 58\n",
      "[EMAIL] 576514445@qq.com | File: 9.csv | Line: 38\n",
      "[ROLE] Still, Boss Gong is amazing. | File: 9.csv | Line: 201\n",
      "[ROLE] My boss recommended them to me. | File: 15.csv | Line: 54\n",
      "[ROLE] What did the boss ask this morning [Wangcai]? | File: 31.csv | Line: 0\n",
      "[ROLE] It's just that the boss is getting anxious. | File: 31.csv | Line: 22\n",
      "[ROLE] I also don't know what the boss is thinking. | File: 40.csv | Line: 122\n",
      "[ROLE] I was just joking with the big boss at Qianxin, but I didn't expect him to actually do it. | File: 11.csv | Line: 78\n",
      "[ROLE] Do you think even if they do it, it won't come to me, and the big boss will take it for himself? | File: 35.csv | Line: 79\n",
      "[IP] 121.37.141.219 | File: 6.csv | Line: 62\n",
      "[ROLE] Ask the boss to go easy on me. | File: 2.csv | Line: 109\n",
      "[ROLE] Don't know the boss. | File: 2.csv | Line: 408\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load chat CSV paths\n",
    "df_csvs = pd.read_csv(\"updated_with_csv_paths.csv\")\n",
    "\n",
    "# Patterns\n",
    "email_pattern = re.compile(r'\\S+@\\S+\\.\\S+')\n",
    "ip_pattern = re.compile(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}(?::\\d{1,5})?\\b')\n",
    "role_keywords = [\n",
    "    \"boss\"\n",
    "]\n",
    "role_pattern = re.compile(r'\\b(?:' + '|'.join([re.escape(k) for k in role_keywords]) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Store unique matches\n",
    "seen = set()\n",
    "\n",
    "# Loop through all files\n",
    "for csv_path in df_csvs['chats'].dropna().unique():\n",
    "    if isinstance(csv_path, str) and csv_path.endswith('.csv') and os.path.exists(csv_path):\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {csv_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        text_cols = [col for col in df.columns if 'message' in col]\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            for col in text_cols:\n",
    "                if pd.notna(row.get(col)):\n",
    "                    text = str(row[col])\n",
    "\n",
    "                    for email in email_pattern.findall(text):\n",
    "                        key = (\"email\", email, csv_path, idx)\n",
    "                        if key not in seen:\n",
    "                            seen.add(key)\n",
    "                            print(f\"[EMAIL] {email} | File: {os.path.basename(csv_path)} | Line: {idx}\")\n",
    "\n",
    "                    for ip in ip_pattern.findall(text):\n",
    "                        key = (\"ip\", ip, csv_path, idx)\n",
    "                        if key not in seen:\n",
    "                            seen.add(key)\n",
    "                            print(f\"[IP] {ip} | File: {os.path.basename(csv_path)} | Line: {idx}\")\n",
    "\n",
    "                    if role_pattern.search(text):\n",
    "                        key = (\"role\", text, csv_path, idx)\n",
    "                        if key not in seen:\n",
    "                            seen.add(key)\n",
    "                            print(f\"[ROLE] {text} | File: {os.path.basename(csv_path)} | Line: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59e79b82-041e-4338-ab5a-babecde51a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_unique_users_from_csv(csv_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {csv_path}: {e}\")\n",
    "        return set()\n",
    "\n",
    "    # Try to find user-related columns\n",
    "    user_columns = [col for col in df.columns if col.lower() in ['from', 'to', 'sender', 'receiver', 'user']]\n",
    "    \n",
    "    if not user_columns:\n",
    "        user_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Flatten and clean user list\n",
    "    users = pd.unique(df[user_columns].values.ravel())\n",
    "    return {str(u).strip() for u in users if pd.notna(u) and str(u).strip() != ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3624c820-6029-4b82-b649-30e5b4e91cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users found in chat logs:\n",
      "- 25713010771@chatroom\n",
      "- SWEET5683yao\n",
      "- Shutd0wn\n",
      "- adpw90\n",
      "- dujijiyiqxx\n",
      "- gzp1991101\n",
      "- hack05112\n",
      "- ibabaimama\n",
      "- just910420\n",
      "- ken73224\n",
      "- lengmo\n",
      "- nullroot\n",
      "- qq78263462\n",
      "- snipersk\n",
      "- tianyi-0608\n",
      "- wangchao953541\n",
      "- wei592628\n",
      "- wxid_12n748um1thl21\n",
      "- wxid_5390224027312\n",
      "- wxid_70w3p1jin84k22\n",
      "- wxid_7p054rmzkhqf21\n",
      "- wxid_blw54o1q0q5w22\n",
      "- wxid_c9yv0nsla3yn22\n",
      "- wxid_hlmnhsq64tt722\n",
      "- wxid_icges6alg8cl21\n",
      "- wxid_jcnxegjccqi441\n",
      "- wxid_kbys0kvzj4ta12\n",
      "- wxid_mgh25nentc4u22\n",
      "- wxid_nv9bv435fz3722\n",
      "- wxid_q7vkst94g5u011\n",
      "- wxid_soekgggwnfgm21\n",
      "- wxid_wh6x59w70y3r22\n",
      "- wxid_xusilpfkh31g21\n",
      "- wxid_zb45i0rc71yk21\n",
      "- wxid_zbytkn4qjl3r22\n",
      "- yanzi542766277\n",
      "- zhangxiaoyan0422\n"
     ]
    }
   ],
   "source": [
    "all_users = set()\n",
    "\n",
    "# Loop through each file path in 'chats' column\n",
    "for csv_path in df_csvs_llm['chats'].dropna().unique():\n",
    "    if isinstance(csv_path, str) and csv_path.endswith('.csv') and os.path.exists(csv_path):\n",
    "        users = extract_unique_users_from_csv(csv_path)\n",
    "        all_users.update(users)\n",
    "\n",
    "unique_users_from_chats = sorted(all_users)\n",
    "print(\"Unique users found in chat logs:\")\n",
    "for user in unique_users_from_chats:\n",
    "    print(\"-\", user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2594254-fa3c-4bac-a108-125ac97a586e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 490\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_files_with_os(path):\n",
    "    return len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"ocr_to_csv\"\n",
    "file_count = count_files_with_os(folder_path)\n",
    "print(f\"Number of files: {file_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e7d53f-7376-40b2-9818-452231587964",
   "metadata": {},
   "source": [
    "# **Financial Infrastructures Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7847de-f686-4967-b334-2b9ebee7c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ba5cba-946f-4657-a3b8-d9d51eb411b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20294/3807150908.py:38: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  vector_store = Chroma(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Load a single CSV file\n",
    "csv_file = \"csvs/10.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "# Setup vector DB path\n",
    "db_location = \"./chroma_financial_chat_db\"\n",
    "add_documents = not os.path.exists(db_location)\n",
    "\n",
    "# Only add documents if the database doesn't already exist\n",
    "if add_documents:\n",
    "    documents = []\n",
    "    ids = []\n",
    "    for i, row in df.iterrows():\n",
    "        document = Document(\n",
    "            page_content=row[\"message_translation\"],\n",
    "            metadata={\n",
    "                \"sender\": row[\"From\"],\n",
    "                \"receiver\": row[\"To\"],\n",
    "                \"timestamp\": row[\"Time\"],\n",
    "                \"source_file\": os.path.basename(csv_file),   # Track source file\n",
    "                \"line_number\": i + 1                         # Track line number\n",
    "            },\n",
    "            id=f\"{os.path.basename(csv_file)}_{i}\"            # Ensure unique ID\n",
    "        )\n",
    "        documents.append(document)\n",
    "        ids.append(f\"{os.path.basename(csv_file)}_{i}\")\n",
    "\n",
    "# Create or load the Chroma vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"financial_chats\",\n",
    "    persist_directory=db_location,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "if add_documents:\n",
    "    vector_store.add_documents(documents=documents, ids=ids)\n",
    "\n",
    "# Setup retriever for use in RAG\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ad2851e-9636-4ac5-a439-27f4c1c0710e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = OllamaLLM(model=\"gemma3:27b\") \n",
    "\n",
    "template = \"\"\"\n",
    "You are a cybersecurity analyst reviewing translated chat messages for financial intelligence.\n",
    "\n",
    "Your task is to extract ONLY the messages that contain **financially relevant information**, such as:\n",
    "\n",
    "- Payments or money transfers\n",
    "- Amounts, currencies, or sums of money\n",
    "- Payment methods (cash, crypto, bank, wallets)\n",
    "- Account numbers, codes, or financial identifiers\n",
    "- Mentions of financial roles (payer, payee, client, broker)\n",
    "- References to suspicious or recurring financial activity\n",
    "\n",
    "From the messages below:\n",
    "{chats}\n",
    "\n",
    "Return ONLY the messages that meet any of the criteria above.\n",
    "\n",
    "For each relevant message, output:\n",
    "- The **timestamp** (if provided in the metadata or inferred)\n",
    "- The **translated content** of the message\n",
    "\n",
    "Do not summarize. Do not explain. Do not include non-financial content.  \n",
    "**DO NOT create or infer messages. Use only the exact text from the messages provided. If no messages match, return nothing.**\n",
    "\n",
    "Output format:\n",
    "[Time] Message\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e153ea-7ffe-4452-bb03-e3e1856130cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.csv - line 21] Account TGtadie, phone: 18510867099, name: Wang Ning.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Extract only the chat messages that refer to money, payments, currencies, accounts, financial identifiers, or financial discussions\"\n",
    "\n",
    "# Retrieve relevant chats\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "# Combine messages with metadata for context\n",
    "chat_texts = \"\\n\\n\".join([\n",
    "    f\"[{doc.metadata['source_file']} - line {doc.metadata['line_number']}] {doc.page_content}\"\n",
    "    for doc in retrieved_docs\n",
    "])\n",
    "\n",
    "# Run through the LLM\n",
    "response = chain.invoke({\"chats\": chat_texts})\n",
    "\n",
    "# Output the result\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b3b93f-c97f-4882-9497-ea7f06935ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb56c2cf-16c4-4845-a94a-d2c358d51ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Intelligence Report - Potential Illicit Software/Service Procurement\n",
      "\n",
      "**Date:** October 26, 2023\n",
      "**Subject:** Potential Procurement of WhatsApp Exploitation/Access Services\n",
      "**Source:** Financial Communication Analysis - 10.csv (Lines 107-137)\n",
      "**Analyst:** Financial Crime Investigator\n",
      "\n",
      "**1. Executive Summary:**\n",
      "\n",
      "This report details a concerning exchange indicating potential interest in acquiring WhatsApp access or exploitation capabilities. The conversation centers around inquiries about \"WhatsApp products\" and suggests a willingness to pay for such services, potentially for illicit purposes. The emphasis on meeting in person, coupled with references to a \"leader\" and securing a \"project,\" suggests a coordinated effort and potentially larger scale operation.\n",
      "\n",
      "**2. Financial Structure & Entities Involved:**\n",
      "\n",
      "* **Mr. Zhou:**  Represents a company that *claims* to not offer WhatsApp services, but is being actively probed about them. Position unclear, but appears to be a potential intermediary or target for procurement.\n",
      "* **Mr. Wei:** The individual asking about WhatsApp products. Likely the client or a representative of the ultimate client. Initiates inquiries and expresses interest.\n",
      "* **\"Leader\":** Referenced by Mr. Wei, suggesting a higher authority within his organization who will be involved in potential negotiations and project approval.\n",
      "* **Unidentified Company (Mr. Wei's):**  The organization Mr. Wei represents. Their motivation for requiring WhatsApp capabilities is unknown but warrants investigation.\n",
      "\n",
      "The communication strongly implies a buyer-seller relationship, where Mr. Wei (and his \"leader\") are seeking a provider (potentially Mr. Zhou's company, or a company Mr. Zhou can connect them with) for WhatsApp related services. The discussion of \"budgets,\" \"packages,\" and \"projects\" further supports this.\n",
      "\n",
      "**3. Suspicious Behavior & Coordinated Activity:**\n",
      "\n",
      "* **Evasive Responses:** Mr. Zhou's repeated denials of offering WhatsApp services, while simultaneously not explicitly shutting down the inquiry, are suspicious. This could indicate a reluctance to admit involvement in potentially illegal activities, or a willingness to connect Mr. Wei with others who *do* offer such services.\n",
      "* **In-Person Meeting Emphasis:** The persistent requests for an in-person meeting, especially mentioning bringing the “leader”, strongly suggests that sensitive details and potential payments will be discussed offline, avoiding a documented trail. The reference to “Two Sessions” and travel restrictions points to a planned meeting being delayed due to external factors.\n",
      "* **\"Project\" & \"Unique Capabilities\" References:**  The talk of a \"project\" and requesting demonstration of \"unique capabilities\" suggest the WhatsApp access is being sought for a specific, potentially nefarious purpose beyond legitimate business use.\n",
      "* **Coded Language & Casual Tone:** The seemingly casual conversation, peppered with references to “hotpot” and laughter, could be an attempt to mask the true nature of the discussions.\n",
      "* **Geographic Focus:** The repeated mentioning of Chengdu and Beijing suggests a regional focus for the activity.  Mr. Zhou is confirmed to be in Chengdu, and Mr. Wei’s travel is restricted from Beijing. This could point to a specific operational area.\n",
      "* **Inquiry into Gmail Accounts:** The earlier question about Gmail accounts (line 117) suggests a broader interest in compromising or accessing email communication platforms, hinting at a larger-scale surveillance or data acquisition operation.\n",
      "\n",
      "\n",
      "\n",
      "**4. Potential Illicit Activity:**\n",
      "\n",
      "Based on the available information, potential illicit activities include:\n",
      "\n",
      "* **Illicit Surveillance:** Acquiring WhatsApp access to monitor communications of individuals or groups.\n",
      "* **Data Theft:** Harvesting data from WhatsApp accounts.\n",
      "* **Account Takeover:** Gaining unauthorized access to WhatsApp accounts.\n",
      "* **Spear Phishing/Social Engineering:** Utilizing WhatsApp access to facilitate targeted attacks.\n",
      "* **Espionage:** Gathering intelligence through compromised WhatsApp accounts.\n",
      "\n",
      "\n",
      "\n",
      "**5. Recommended Follow-Up Steps:**\n",
      "\n",
      "* **Entity Identification:** Conduct thorough due diligence on Mr. Zhou, Mr. Wei, and their respective companies to identify their true ownership, business activities, and any known associations with criminal or suspicious activity.\n",
      "* **Financial Transaction Monitoring:** Monitor financial transactions involving these entities for any unusual or suspicious activity.\n",
      "* **Communication Interception:**  If legally permissible, monitor further communications between these individuals.\n",
      "* **Network Analysis:** Analyze the broader communication network surrounding these individuals to identify other potential collaborators or targets.\n",
      "* **Law Enforcement Coordination:** Share this intelligence with relevant law enforcement agencies for further investigation and potential prosecution.\n",
      "* **Technical Investigation:** Investigate whether Mr. Zhou's company (or associates) has a history of developing or distributing surveillance tools or software.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**Disclaimer:** This report is based on limited information and is subject to change as further investigation is conducted.  The conclusions presented are based on the analyst’s professional judgment and are intended for informational purposes only.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from langchain.llms import Ollama as OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# === Step 1: Define the LLM and prompt chain ===\n",
    "model = OllamaLLM(model=\"gemma3:27b\")\n",
    "prompt = ChatPromptTemplate.from_template(\"{chats}\")\n",
    "chain = prompt | model\n",
    "\n",
    "# === Step 2: Define function that takes any retrieved doc ===\n",
    "def investigate_target_message(doc, chain, context_window=15, base_folder=\"csvs\"):\n",
    "    # Get metadata\n",
    "    file = doc.metadata[\"source_file\"]\n",
    "    line = doc.metadata[\"line_number\"]\n",
    "    filepath = os.path.join(base_folder, file)\n",
    "\n",
    "    # Load the CSV\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    start = max(1, line - context_window)\n",
    "    end = min(len(df), line + context_window)\n",
    "\n",
    "    # Build context window\n",
    "    context_lines = []\n",
    "    for i in range(start, end + 1):\n",
    "        row_text = str(df.iloc[i - 1][\"message_translation\"])\n",
    "        tag = \"**[TARGET]**\" if i == line else \"\"\n",
    "        context_lines.append(f\"[{file} - line {i}]{tag} {row_text}\")\n",
    "\n",
    "    full_context = \"\\n\".join(context_lines)\n",
    "\n",
    "    # Construct prompt\n",
    "    prompt_text = f\"\"\"\n",
    "You are a financial crime investigator analyzing financial communications.\n",
    "\n",
    "The following messages include a suspicious message marked **[TARGET]** and 30 nearby messages for context. Based on this, determine:\n",
    "\n",
    "- The financial structure implied (e.g., payer, broker, client, intermediary)\n",
    "- Entities involved (names, accounts, phones)\n",
    "- Any suspicious behavior or coordinated activity\n",
    "- Recommended follow-up steps\n",
    "\n",
    "Messages:\n",
    "{full_context}\n",
    "\n",
    "Generate a structured intelligence report based on the **[TARGET]** message.\n",
    "\"\"\"\n",
    "\n",
    "    # Run LLM\n",
    "    return chain.invoke({\"chats\": prompt_text})\n",
    "\n",
    "# === Step 3: Example usage ===\n",
    "# Run on any retrieved document\n",
    "doc = retrieved_docs[0]  # or loop over retrieved_docs\n",
    "response = investigate_target_message(doc, chain)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d1a58-2855-4f47-b7e7-e82a6d549969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (IsoonAI)",
   "language": "python",
   "name": "isoonai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
