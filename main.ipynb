{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6559c645-340c-4646-b646-39b14bcd9932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '0' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Directory to delete\n",
    "EXTRACT_DIR = '0'\n",
    "\n",
    "# Check if the directory exists and delete it\n",
    "if os.path.isdir(EXTRACT_DIR):\n",
    "    shutil.rmtree(EXTRACT_DIR)\n",
    "    print(f\"Directory '{EXTRACT_DIR}' has been deleted.\")\n",
    "else:\n",
    "    print(f\"Directory '{EXTRACT_DIR}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5776e2db-42ed-4aa2-a497-1a545810a7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores:  144\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "print(\"CPU cores: \", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2befe2f0-40aa-4ffa-8a73-7443ff5760b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted to: 0\n"
     ]
    }
   ],
   "source": [
    "def extract_zip(zip_path):\n",
    "\n",
    "    if not zipfile.is_zipfile(zip_path):\n",
    "        raise ValueError(f\"The file at {zip_path} is not a valid ZIP archive.\")\n",
    "    \n",
    "    # Determine the output directory name from the zip file name\n",
    "    base_dir = os.path.dirname(zip_path)\n",
    "    zip_filename = os.path.basename(zip_path)\n",
    "    folder_name = os.path.splitext(zip_filename)[0]\n",
    "    extract_to = os.path.join(base_dir, folder_name)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(extract_to):\n",
    "        os.makedirs(extract_to)\n",
    "\n",
    "    # Extract the ZIP file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(path=extract_to)\n",
    "\n",
    "    return extract_to\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    zip_file_path = '0.zip' \n",
    "    extracted_dir = extract_zip(zip_file_path)\n",
    "    print(f\"Extracted to: {extracted_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "950a34b4-50f3-486e-b781-a1dc9e43c458",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted - 0/__MACOSX\n"
     ]
    }
   ],
   "source": [
    "leak_directory = \"0\"\n",
    "\n",
    "# Path to the __MACOSX folder\n",
    "macosx_folder = os.path.join(leak_directory, \"__MACOSX\")\n",
    "\n",
    "# Check if __MACOSX exists and remove it - creates issues when analyzing the data, and its not needed, made automatically by MacOS\n",
    "if os.path.exists(macosx_folder) and os.path.isdir(macosx_folder):\n",
    "    shutil.rmtree(macosx_folder)\n",
    "    print(f\"Deleted - {macosx_folder}\")\n",
    "else:\n",
    "    print(f\"Folder not found - {macosx_folder}\")\n",
    "\n",
    "# Organize files by extension into subfolders\n",
    "for root, dirs, files in os.walk(leak_directory):\n",
    "    for file in files:\n",
    "        # Skip hidden files and __MACOSX if any reappear\n",
    "        if file.startswith('.') or '__MACOSX' in root:\n",
    "            continue\n",
    "\n",
    "        # Get the file extension (in lowercase, without the dot)\n",
    "        file_extension = os.path.splitext(file)[1].lower().lstrip('.')\n",
    "        if not file_extension:\n",
    "            file_extension = \"no_extension\"\n",
    "\n",
    "        # Define the new subfolder path\n",
    "        subfolder_path = os.path.join(leak_directory, file_extension)\n",
    "\n",
    "        # Create the subfolder if it doesn't exist\n",
    "        os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "        # Define source and destination paths\n",
    "        source_path = os.path.join(root, file)\n",
    "        destination_path = os.path.join(subfolder_path, file)\n",
    "\n",
    "        # Move the file if source and destination are not the same\n",
    "        if os.path.abspath(source_path) != os.path.abspath(destination_path):\n",
    "            shutil.move(source_path, destination_path)\n",
    "\n",
    "# Remove any empty folders within the parent directory\n",
    "for dirpath, dirnames, filenames in os.walk(leak_directory, topdown=False):\n",
    "    if not dirnames and not filenames:\n",
    "        try:\n",
    "            os.rmdir(dirpath)\n",
    "            print(f\"Removed empty folder: {dirpath}\")\n",
    "        except OSError:\n",
    "            pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ab45fc5-01f0-4dd6-9c70-1ad25b1212a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '0/0'deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete Duplicate of 0 which is empty\n",
    "\n",
    "# Directory to delete\n",
    "EXTRACT_DIR = '0/0'\n",
    "\n",
    "# Check if the directory exists and delete it\n",
    "if os.path.isdir(EXTRACT_DIR):\n",
    "    shutil.rmtree(EXTRACT_DIR)\n",
    "    print(f\"Directory '{EXTRACT_DIR}'deleted\")\n",
    "else:\n",
    "    print(f\"Directory '{EXTRACT_DIR}' does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb18a8-64e3-44d3-915e-7158104d0c8f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# **DATA PARSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f80802-63db-4c12-9401-d9d6246f556a",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Dataframe with all file location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88ee5d4f-ee82-49b3-bb69-9b052f1952f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_folder_file_dataframe(root_dir):\n",
    "    folders = []\n",
    "    for file_name in os.listdir(root_dir):\n",
    "        full_path = os.path.join(root_dir, file_name)\n",
    "        if os.path.isdir(full_path):\n",
    "            folders.append(file_name)\n",
    "\n",
    "    series_list = []\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        files = [\n",
    "            os.path.join(folder_path, file)\n",
    "            for file in os.listdir(folder_path)\n",
    "            if os.path.isfile(os.path.join(folder_path, file)) and not file.startswith('.')\n",
    "        ]\n",
    "        s = pd.Series(files, name=folder)\n",
    "        series_list.append(s)\n",
    "\n",
    "    df = pd.concat(series_list, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94163880-07e2-4205-8302-b5ead2a1f488",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>md</th>\n",
       "      <th>png</th>\n",
       "      <th>log</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md</td>\n",
       "      <td>0/png/64bba692-d430-440c-9f1e-2575f45770af_6.png</td>\n",
       "      <td>0/log/77010155050.log</td>\n",
       "      <td>0/txt/IDNET.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0/md/28.md</td>\n",
       "      <td>0/png/12756724-394c-4576-b373-7c53f1abbd94_0.png</td>\n",
       "      <td>0/log/77753527617.log</td>\n",
       "      <td>0/txt/IDTV.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0/md/5.md</td>\n",
       "      <td>0/png/f179eb06-0c53-44df-a13f-570be23355bb_1.png</td>\n",
       "      <td>0/log/tele2-lbs.log</td>\n",
       "      <td>0/txt/beeline-77774042222.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0/md/38.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_24.png</td>\n",
       "      <td>0/log/tele2-cdr.log</td>\n",
       "      <td>0/txt/beeline-77051056626.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_30.png</td>\n",
       "      <td>0/log/tele2-crm.log</td>\n",
       "      <td>0/txt/beeline-crm.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0/md/18.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_18.png</td>\n",
       "      <td>0/log/77783030133.log</td>\n",
       "      <td>0/txt/UBSCRIBER.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md</td>\n",
       "      <td>0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/beeline-cdr.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md</td>\n",
       "      <td>0/png/5387a301-0af8-4e24-a197-20189f87b9ef_8.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/CRM.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0/md/1.md</td>\n",
       "      <td>0/png/0-32eb7662-f212-4811-a7c1-1cfeb121cd99.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/LAC.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0/md/19.md</td>\n",
       "      <td>0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_11.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/beeline-lbs.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md</td>\n",
       "      <td>0/png/0-7150f512-e7a2-4f2c-86bc-58b671b25ba9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/Φ»¥σìò.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0/md/29.md</td>\n",
       "      <td>0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_8.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md</td>\n",
       "      <td>0/png/eda5b003-9250-4913-b724-74cca86240af_13.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0/md/585875ff-f8c5-4a02-acd7-fef37dc9ff11.md</td>\n",
       "      <td>0/png/eda5b003-9250-4913-b724-74cca86240af_12.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0/md/4.md</td>\n",
       "      <td>0/png/adaf869e-920a-4a17-91bd-e2ef3125c10e.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0/md/39.md</td>\n",
       "      <td>0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0/md/16.md</td>\n",
       "      <td>0/png/32eb7662-f212-4811-a7c1-1cfeb121cd99.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0/md/64bba692-d430-440c-9f1e-2575f45770af.md</td>\n",
       "      <td>0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_10.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0/md/178e3898-903d-47cf-bfbe-061e7dc18895.md</td>\n",
       "      <td>0/png/5387a301-0af8-4e24-a197-20189f87b9ef_9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0/md/22.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_19.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0/md/32.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_31.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0/md/26.md</td>\n",
       "      <td>0/png/f179eb06-0c53-44df-a13f-570be23355bb_0.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0/md/9fe6b262-9944-417d-a0c4-9f2de1de2994.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_25.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0/md/f7205881-3904-42ec-ab2c-04f36fa24785.md</td>\n",
       "      <td>0/png/7150f512-e7a2-4f2c-86bc-58b671b25ba9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0/md/54990932-71af-48dd-9a7a-2617b1407c54.md</td>\n",
       "      <td>0/png/12756724-394c-4576-b373-7c53f1abbd94_1.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0/md/12.md</td>\n",
       "      <td>0/png/64bba692-d430-440c-9f1e-2575f45770af_7.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0/md/36.md</td>\n",
       "      <td>0/png/6848748d-2881-4c26-b153-fcd5373d2f1c.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0/md/eda5b003-9250-4913-b724-74cca86240af.md</td>\n",
       "      <td>0/png/12756724-394c-4576-b373-7c53f1abbd94_3.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0/md/27.md</td>\n",
       "      <td>0/png/64bba692-d430-440c-9f1e-2575f45770af_5.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0/md/13.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_33.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              md  \\\n",
       "0   0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md   \n",
       "1                                     0/md/28.md   \n",
       "2                                      0/md/5.md   \n",
       "3                                     0/md/38.md   \n",
       "4   0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md   \n",
       "5                                     0/md/18.md   \n",
       "6   0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md   \n",
       "7   0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md   \n",
       "8                                      0/md/1.md   \n",
       "9                                     0/md/19.md   \n",
       "10  0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md   \n",
       "11                                    0/md/29.md   \n",
       "12  0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md   \n",
       "13  0/md/585875ff-f8c5-4a02-acd7-fef37dc9ff11.md   \n",
       "14                                     0/md/4.md   \n",
       "15                                    0/md/39.md   \n",
       "16                                    0/md/16.md   \n",
       "17  0/md/64bba692-d430-440c-9f1e-2575f45770af.md   \n",
       "18  0/md/178e3898-903d-47cf-bfbe-061e7dc18895.md   \n",
       "19                                    0/md/22.md   \n",
       "20                                    0/md/32.md   \n",
       "21                                    0/md/26.md   \n",
       "22  0/md/9fe6b262-9944-417d-a0c4-9f2de1de2994.md   \n",
       "23  0/md/f7205881-3904-42ec-ab2c-04f36fa24785.md   \n",
       "24  0/md/54990932-71af-48dd-9a7a-2617b1407c54.md   \n",
       "25                                    0/md/12.md   \n",
       "26                                    0/md/36.md   \n",
       "27  0/md/eda5b003-9250-4913-b724-74cca86240af.md   \n",
       "28                                    0/md/27.md   \n",
       "29                                    0/md/13.md   \n",
       "\n",
       "                                                  png                    log  \\\n",
       "0    0/png/64bba692-d430-440c-9f1e-2575f45770af_6.png  0/log/77010155050.log   \n",
       "1    0/png/12756724-394c-4576-b373-7c53f1abbd94_0.png  0/log/77753527617.log   \n",
       "2    0/png/f179eb06-0c53-44df-a13f-570be23355bb_1.png    0/log/tele2-lbs.log   \n",
       "3   0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_24.png    0/log/tele2-cdr.log   \n",
       "4   0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_30.png    0/log/tele2-crm.log   \n",
       "5   0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_18.png  0/log/77783030133.log   \n",
       "6    0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e.png                    NaN   \n",
       "7    0/png/5387a301-0af8-4e24-a197-20189f87b9ef_8.png                    NaN   \n",
       "8    0/png/0-32eb7662-f212-4811-a7c1-1cfeb121cd99.png                    NaN   \n",
       "9   0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_11.png                    NaN   \n",
       "10   0/png/0-7150f512-e7a2-4f2c-86bc-58b671b25ba9.png                    NaN   \n",
       "11   0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_8.png                    NaN   \n",
       "12  0/png/eda5b003-9250-4913-b724-74cca86240af_13.png                    NaN   \n",
       "13  0/png/eda5b003-9250-4913-b724-74cca86240af_12.png                    NaN   \n",
       "14     0/png/adaf869e-920a-4a17-91bd-e2ef3125c10e.png                    NaN   \n",
       "15   0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_9.png                    NaN   \n",
       "16     0/png/32eb7662-f212-4811-a7c1-1cfeb121cd99.png                    NaN   \n",
       "17  0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_10.png                    NaN   \n",
       "18   0/png/5387a301-0af8-4e24-a197-20189f87b9ef_9.png                    NaN   \n",
       "19  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_19.png                    NaN   \n",
       "20  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_31.png                    NaN   \n",
       "21   0/png/f179eb06-0c53-44df-a13f-570be23355bb_0.png                    NaN   \n",
       "22  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_25.png                    NaN   \n",
       "23     0/png/7150f512-e7a2-4f2c-86bc-58b671b25ba9.png                    NaN   \n",
       "24   0/png/12756724-394c-4576-b373-7c53f1abbd94_1.png                    NaN   \n",
       "25   0/png/64bba692-d430-440c-9f1e-2575f45770af_7.png                    NaN   \n",
       "26     0/png/6848748d-2881-4c26-b153-fcd5373d2f1c.png                    NaN   \n",
       "27   0/png/12756724-394c-4576-b373-7c53f1abbd94_3.png                    NaN   \n",
       "28   0/png/64bba692-d430-440c-9f1e-2575f45770af_5.png                    NaN   \n",
       "29  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_33.png                    NaN   \n",
       "\n",
       "                              txt  \n",
       "0                 0/txt/IDNET.txt  \n",
       "1                  0/txt/IDTV.txt  \n",
       "2   0/txt/beeline-77774042222.txt  \n",
       "3   0/txt/beeline-77051056626.txt  \n",
       "4           0/txt/beeline-crm.txt  \n",
       "5             0/txt/UBSCRIBER.txt  \n",
       "6           0/txt/beeline-cdr.txt  \n",
       "7                   0/txt/CRM.txt  \n",
       "8                   0/txt/LAC.txt  \n",
       "9           0/txt/beeline-lbs.txt  \n",
       "10               0/txt/Φ»¥σìò.txt  \n",
       "11                            NaN  \n",
       "12                            NaN  \n",
       "13                            NaN  \n",
       "14                            NaN  \n",
       "15                            NaN  \n",
       "16                            NaN  \n",
       "17                            NaN  \n",
       "18                            NaN  \n",
       "19                            NaN  \n",
       "20                            NaN  \n",
       "21                            NaN  \n",
       "22                            NaN  \n",
       "23                            NaN  \n",
       "24                            NaN  \n",
       "25                            NaN  \n",
       "26                            NaN  \n",
       "27                            NaN  \n",
       "28                            NaN  \n",
       "29                            NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_folder_file_dataframe(leak_directory)\n",
    "df.head(30)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b011fbc-09b2-4ccd-b075-62306871df42",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Use LLM to classify the source files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46d6b38-6ff3-4a79-860c-a37054e03cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ba6064-3151-4166-a432-142d9e610d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_313/23640893.py:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
      "Classifying files:   0%|          | 0/70 [00:00<?, ?it/s]/tmp/ipykernel_313/23640893.py:51: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  response = chain.run(content=content).strip().lower()\n",
      "Classifying files: 100%|██████████| 70/70 [03:58<00:00,  3.40s/it] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other</th>\n",
       "      <th>images</th>\n",
       "      <th>chats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md</td>\n",
       "      <td>0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md</td>\n",
       "      <td>0/md/28.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md</td>\n",
       "      <td>0/md/5.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md</td>\n",
       "      <td>0/md/38.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md</td>\n",
       "      <td>0/md/18.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md</td>\n",
       "      <td>0/md/1.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          other  \\\n",
       "0  0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md   \n",
       "1                                          None   \n",
       "2                                          None   \n",
       "3                                          None   \n",
       "4                                          None   \n",
       "\n",
       "                                         images       chats  \n",
       "0  0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md  0/md/28.md  \n",
       "1  0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md   0/md/5.md  \n",
       "2  0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md  0/md/38.md  \n",
       "3  0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md  0/md/18.md  \n",
       "4  0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md   0/md/1.md  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3.1:8b\")\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"\"\"\n",
    "You are analyzing the content of a file.\n",
    "\n",
    "File content:\n",
    "\\\"\\\"\\\"\n",
    "{content}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "1. Classify the content into one of the following categories ONLY: chats, images, other.\n",
    "2. State your confidence in the classification as one of: high, medium, or low.\n",
    "3. I will have to make a csv, please give me a list of headers based on the content. E.g. \"[<header_name>, <heaer_name2>, etc.]\n",
    "Respond in the following format:\n",
    "Category: <chats|images|other>\n",
    "Confidence: <high|medium|low>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Preprocess the files to the first 20 lines to shorten analysis times\n",
    "def preprocess_first_20_lines(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [line.strip() for _, line in zip(range(20), f)]\n",
    "            content = \" \".join(lines)\n",
    "        return file_path, content\n",
    "    except Exception:\n",
    "        return file_path, \"\"\n",
    "\n",
    "# Use only the first column of the DataFrame\n",
    "first_column = df.columns[0]\n",
    "file_paths = df[first_column].dropna().unique().tolist()\n",
    "\n",
    "# Preprocess the file for faster classification\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    file_data = list(executor.map(preprocess_first_20_lines, file_paths))\n",
    "\n",
    "# Classify and collect by category\n",
    "valid_categories = {\"chats\", \"images\", \"other\"}\n",
    "categorized_files = {cat: [] for cat in valid_categories}\n",
    "\n",
    "for file_path, content in tqdm(file_data, desc=\"Classifying files\"):\n",
    "    if not content:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        response = chain.run(content=content).strip().lower()\n",
    "        lines = response.splitlines()\n",
    "\n",
    "        category = \"\"\n",
    "        for line in lines:\n",
    "            if line.startswith(\"category:\"):\n",
    "                category = line.replace(\"category:\", \"\").strip()\n",
    "                break\n",
    "        if category not in valid_categories:\n",
    "            category = \"other\"\n",
    "\n",
    "        categorized_files[category].append(file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Convert to DataFrame (columns = categories)\n",
    "# Determine the maximum list length among all categories\n",
    "max_len = 0\n",
    "for file_list in categorized_files.values():\n",
    "    if len(file_list) > max_len:\n",
    "        max_len = len(file_list)\n",
    "\n",
    "# Pad each list with None to match max length\n",
    "padded = {}\n",
    "for category, file_list in categorized_files.items():\n",
    "    padding_needed = max_len - len(file_list)\n",
    "    padded[category] = file_list + [None] * padding_needed\n",
    "result_df = pd.DataFrame(padded)\n",
    "result_df.to_csv(\"classified_by_category.csv\", index=False)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae9479b-52e3-48d0-867b-55aa68bf1acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e96cc02-e59d-4113-b35e-857490c25736",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Script to check for cross refernced files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2960f30-2535-4cc4-b4ec-ac57315e764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e94a6ab-a1d9-4c11-b275-a753fc8182f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    source_file  \\\n",
      "0  0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md   \n",
      "1                                    0/md/10.md   \n",
      "2                                    0/md/13.md   \n",
      "3                                    0/md/15.md   \n",
      "4  0/md/178e3898-903d-47cf-bfbe-061e7dc18895.md   \n",
      "5                                    0/md/19.md   \n",
      "6                                     0/md/2.md   \n",
      "7                                    0/md/20.md   \n",
      "8                                    0/md/21.md   \n",
      "9                                    0/md/22.md   \n",
      "\n",
      "                                            mentions  \n",
      "0  [0/png/01cdc26f-e773-4ad7-8808-d04abf16aae7_1_...  \n",
      "1  [0/png/0-6bcc0131-e4ad-421e-bb1f-d8ebe5eeec7b....  \n",
      "2  [0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e....  \n",
      "3  [0/png/0-b8cea3b1-4dde-4438-9b1a-6faf690bbad0....  \n",
      "4  [0/png/178e3898-903d-47cf-bfbe-061e7dc18895_8....  \n",
      "5  [0/png/6848748d-2881-4c26-b153-fcd5373d2f1c.pn...  \n",
      "6  [0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md,...  \n",
      "7  [0/png/0-785cc8c9-1225-4f93-b633-349bc5113512....  \n",
      "8  [0/png/9c8c9989-2293-4e68-9ffe-6f7a5f14562f.pn...  \n",
      "9  [0/png/0-b0a4acaa-d768-4f6d-8e54-6d20f271bb7c....  \n",
      "Unique file paths in original DataFrame: 577\n"
     ]
    }
   ],
   "source": [
    "# Extract all unique file paths from the DataFrame \n",
    "file_paths = df.stack().dropna().unique().tolist()\n",
    "\n",
    "# Read file content\n",
    "def read_file(path):\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except:\n",
    "        return \"Failed to open file\"\n",
    "\n",
    "# Read file contents into a dictionary \n",
    "file_contents = {path: read_file(path) for path in file_paths}\n",
    "\n",
    "# Build a reference map (which files mention which) \n",
    "reference_map = defaultdict(list)\n",
    "\n",
    "for source_path, content in file_contents.items():\n",
    "    if content is None:\n",
    "        continue  # skip if content is None\n",
    "\n",
    "    for target_path in file_paths:\n",
    "        if target_path == source_path:\n",
    "            continue  # skip comparing file to itself\n",
    "\n",
    "        target_filename = os.path.basename(target_path)\n",
    "        pattern = re.escape(target_filename)\n",
    "\n",
    "        if re.search(rf'\\b{pattern}\\b', content):\n",
    "            reference_map[source_path].append(target_path)\n",
    "\n",
    "# Convert reference map to a DataFrame\n",
    "ref_df = pd.DataFrame([\n",
    "    {\"source_file\": src, \"mentions\": tgt}\n",
    "    for src, tgts in reference_map.items()\n",
    "    for tgt in tgts\n",
    "])\n",
    "\n",
    "# Group mentions into lists per source_file\n",
    "ref_summary = ref_df.groupby(\"source_file\")[\"mentions\"].apply(list).reset_index()\n",
    "\n",
    "# Some source_file names are also in mentions, therefore they aren't a source file anymore\n",
    "source_files = set(ref_summary[\"source_file\"])\n",
    "mentioned_files = set(file for mention_list in ref_summary[\"mentions\"] for file in mention_list)\n",
    "common_files = source_files.intersection(mentioned_files)\n",
    "\n",
    "# Filter out common files from the DataFrame\n",
    "filtered_ref_summary = ref_summary[~ref_summary[\"source_file\"].isin(common_files)].reset_index(drop=True)\n",
    "\n",
    "filtered_ref_summary.to_csv(\"file_reference_map.csv\", index=False)\n",
    "print(filtered_ref_summary.head(10))\n",
    "print(\"Unique file paths in original DataFrame:\", df.stack().nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0bce5a-16de-48e6-83df-240b732f1a20",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Check for more linkages in the files + add the missing source files from the chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10f92af7-5075-4d8d-9251-113f6e360f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from ast import literal_eval\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe46121c-4ff4-4e45-b248-8edd4d12be74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['source_file', 'mentions', 'mentions2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def is_readable_text_file(path):\n",
    "    return any(path.endswith(ext) for ext in ['txt', 'md', 'rst', 'tex', 'nfo', 'readme', 'rtf', 'doc', 'docx',\n",
    "    'cfg', 'conf', 'config', 'ini', 'json', 'yaml', 'yml', 'toml',\n",
    "    'log', 'lst', 'cnf', 'properties', 'prefs',\n",
    "    'csv', 'tsv', 'dat', 'db', 'dbf', 'sql', 'xml',\n",
    "    'ssv', 'psv', 'jsonl', 'parquet', 'orc',\n",
    "    'html', 'htm', 'xhtml', 'xht', 'css', 'js',\n",
    "    'jsx', 'ts', 'tsx', 'vue', 'erb', 'ejs', 'jsp',\n",
    "    'liquid', 'handlebars', 'hbs', 'mustache',\n",
    "    'py', 'pyw', 'ipynb', 'java', 'c', 'cpp', 'h', 'hpp', 'cs',\n",
    "    'sh', 'bash', 'zsh', 'ksh', 'bat', 'cmd', 'ps1',\n",
    "    'r', 'jl', 'pl', 'pm', 'rb', 'go', 'lua', 'php',\n",
    "    'swift', 'scala', 'dart', 'asm', 'groovy',\n",
    "    'rmd', 'sage', 'nb',\n",
    "    'env', 'gradle', 'makefile', 'mak', 'mk',\n",
    "    'dockerfile', 'gitignore', 'gitattributes', 'gitmodules',\n",
    "    'cmake', 'make', 'ninja', 'build',\n",
    "    'manifest', 'manifest.json', 'vtt', 'srt', 'resx', 'strings',\n",
    "    'lang', 'po', 'mo', 'pot', 'msg', 'textbundle',\n",
    "    'rego', 'tf', 'tfvars', 'cue', 'bzl', 'bazel', 'nix', 'dhall',\n",
    "    'adoc', 'asciidoc', 'creole', 'mediawiki', 'wiki', 'org',\n",
    "    'eml', 'msg', 'mbox', 'mail', 'ics', 'vcf'])\n",
    "\n",
    "mentions2_list = []\n",
    "\n",
    "# Iterate over each row of the summary\n",
    "for _, row in filtered_ref_summary.iterrows():\n",
    "    mention_dict = {}\n",
    "\n",
    "    # For each mentioned file read file contents and create a dictionary key: location - values: file contents\n",
    "    for mentioned_file in row[\"mentions\"]:\n",
    "        if not is_readable_text_file(mentioned_file):\n",
    "            continue  # Skip unreadable formats\n",
    "\n",
    "        # Read file content\n",
    "        content = read_file(mentioned_file)\n",
    "        if content in [None, \"Failed to open file\"]:\n",
    "            continue\n",
    "\n",
    "        # Search for any file-type references using regex\n",
    "        # Match anything with a name and known file extension\n",
    "        found_files = re.findall(r'[\\w\\-/]*\\.\\w+', content)\n",
    "\n",
    "        # Filter duplicates and add to the mention_dict\n",
    "        if found_files:\n",
    "            mention_dict[mentioned_file] = list(set(found_files))\n",
    "\n",
    "    mentions2_list.append(mention_dict)\n",
    "\n",
    "# Add this as a new column\n",
    "filtered_ref_summary[\"mentions2\"] = mentions2_list\n",
    "\n",
    "# Get the list of chat files from result_df - LLM clasification\n",
    "chat_files = result_df['chats'].dropna().unique()\n",
    "\n",
    "# Get the list of source files already in filtered_ref_summary\n",
    "source_files = filtered_ref_summary['source_file'].dropna().unique()\n",
    "\n",
    "# Find missing chat files and collect chat files that are not found in source files\n",
    "existing_source_files = set(source_files)\n",
    "missing_chat_files = []\n",
    "for chat_file in chat_files:\n",
    "    if chat_file not in existing_source_files:\n",
    "        missing_chat_files.append(chat_file)\n",
    "\n",
    "# Create a new DataFrame with empty mentions and linkage columns\n",
    "new_rows = pd.DataFrame({\n",
    "    'source_file': missing_chat_files,\n",
    "})\n",
    "\n",
    "# Append the new rows to ref_summary\n",
    "filtered_ref_summary = pd.concat([filtered_ref_summary, new_rows], ignore_index=True)\n",
    "        \n",
    "# Save the updated DataFrame to CSV\n",
    "filtered_ref_summary.to_csv(\"file_reference_map_extended.csv\", index=False)\n",
    "print(filtered_ref_summary.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e01b1-5dd6-4678-b3d9-e48e859045f1",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Structuring the Data into CSV - Step 1 all MD files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37835ee2-1c35-4cf7-a08e-89d2e02c5621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9d08d1c-afe4-4cae-b359-779c917f3689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a folder and dumb all csvs\n",
    "output_dir = \"csvs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all MD files from the DataFrame\n",
    "all_file_paths = pd.unique(result_df.values.ravel('K'))\n",
    "md_files = []\n",
    "for file_path in all_file_paths:\n",
    "    if isinstance(file_path, str) and file_path.endswith(\".md\") and os.path.exists(file_path):\n",
    "        md_files.append(file_path)\n",
    "        \n",
    "# Use the LLM's clasification of MD files\n",
    "chat_files = set(result_df['chats'].dropna().astype(str))\n",
    "\n",
    "# Extract visible text + ANY file reference \n",
    "def extract_text_and_files(td):\n",
    "    text = td.get_text(strip=True)\n",
    "    links = []\n",
    "    for tag in td.find_all(['a', 'img']):\n",
    "        link = tag.get('href') or tag.get('src')\n",
    "        if link:\n",
    "            fname = os.path.basename(link.strip(\"'\\\"\"))\n",
    "            links.append(fname)\n",
    "    return text + (\" \" + \" \".join(links) if links else \"\")\n",
    "\n",
    "# This method extracts chat table from HTML\n",
    "def process_chat_md(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        rows = soup.find_all(\"tr\")[1:]\n",
    "        data = []\n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) == 4:\n",
    "                data.append({\n",
    "                    \"Time\": cols[0].get_text(strip=True),\n",
    "                    \"From\": cols[1].get_text(strip=True),\n",
    "                    \"To\": cols[2].get_text(strip=True),\n",
    "                    \"Message\": extract_text_and_files(cols[3])\n",
    "                })\n",
    "        return pd.DataFrame(data) if data else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chat file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract the contents of the other file types that the LLM classified\n",
    "def process_reference_md(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        matches = []\n",
    "        for tag in soup.find_all(['a', 'img']):\n",
    "            link = tag.get('href') or tag.get('src')\n",
    "            if link:\n",
    "                fname = os.path.basename(link.strip(\"'\\\"\"))\n",
    "                if not fname.startswith(\"0-\"):\n",
    "                    matches.append(fname)\n",
    "        matches = list(set(matches))\n",
    "        return pd.DataFrame([{\"File_Name\": f} for f in matches]) if matches else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing reference file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Loop through all MD filesand call the above methods for the afferent file type\n",
    "for file_path in md_files:\n",
    "    csv_name = os.path.basename(file_path).replace(\".md\", \".csv\")\n",
    "    csv_path = os.path.join(output_dir, csv_name)\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        if file_path in chat_files:\n",
    "            df = process_chat_md(file_path)\n",
    "        else:\n",
    "            df = process_reference_md(file_path)\n",
    "\n",
    "        if df is not None:\n",
    "            df.to_csv(csv_path, index=False)\n",
    "        else:\n",
    "            print(f\"Couldn't save to CSV: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "070f1c87-70d5-4699-8220-fecbc4f05457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f4e4289-8394-4707-b8d9-687061d28d62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>mentions</th>\n",
       "      <th>mentions2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>csvs/01cdc26f-e773-4ad7-8808-d04abf16aae7.csv</td>\n",
       "      <td>[0/png/01cdc26f-e773-4ad7-8808-d04abf16aae7_1_...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>csvs/10.csv</td>\n",
       "      <td>[0/png/0-6bcc0131-e4ad-421e-bb1f-d8ebe5eeec7b....</td>\n",
       "      <td>{'csvs/12756724-394c-4576-b373-7c53f1abbd94.cs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>csvs/13.csv</td>\n",
       "      <td>[0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e....</td>\n",
       "      <td>{'csvs/585875ff-f8c5-4a02-acd7-fef37dc9ff11.cs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>csvs/15.csv</td>\n",
       "      <td>[0/png/0-b8cea3b1-4dde-4438-9b1a-6faf690bbad0....</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>csvs/178e3898-903d-47cf-bfbe-061e7dc18895.csv</td>\n",
       "      <td>[0/png/178e3898-903d-47cf-bfbe-061e7dc18895_8....</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     source_file  \\\n",
       "0  csvs/01cdc26f-e773-4ad7-8808-d04abf16aae7.csv   \n",
       "1                                    csvs/10.csv   \n",
       "2                                    csvs/13.csv   \n",
       "3                                    csvs/15.csv   \n",
       "4  csvs/178e3898-903d-47cf-bfbe-061e7dc18895.csv   \n",
       "\n",
       "                                            mentions  \\\n",
       "0  [0/png/01cdc26f-e773-4ad7-8808-d04abf16aae7_1_...   \n",
       "1  [0/png/0-6bcc0131-e4ad-421e-bb1f-d8ebe5eeec7b....   \n",
       "2  [0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e....   \n",
       "3  [0/png/0-b8cea3b1-4dde-4438-9b1a-6faf690bbad0....   \n",
       "4  [0/png/178e3898-903d-47cf-bfbe-061e7dc18895_8....   \n",
       "\n",
       "                                           mentions2  \n",
       "0                                                 {}  \n",
       "1  {'csvs/12756724-394c-4576-b373-7c53f1abbd94.cs...  \n",
       "2  {'csvs/585875ff-f8c5-4a02-acd7-fef37dc9ff11.cs...  \n",
       "3                                                 {}  \n",
       "4                                                 {}  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dir = \"csvs\"\n",
    "\n",
    "# Replace MD with CSV if the corresponding file exists\n",
    "def replace_md_with_csv(item):\n",
    "    if isinstance(item, str) and item.endswith(\".md\"):\n",
    "        base_name = Path(item).stem  # .stem extracts the filename with no extension\n",
    "        # CSV search\n",
    "        csv_path = os.path.join(csv_dir, f\"{base_name}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            return csv_path\n",
    "        else:\n",
    "            print(f\"CSV {item} not found\")\n",
    "    return item\n",
    "\n",
    "# Recursively handle strings, lists, and dictionaries\n",
    "def process_column_cell(cell):\n",
    "    if isinstance(cell, str):\n",
    "        return replace_md_with_csv(cell)\n",
    "    \n",
    "    elif isinstance(cell, list):\n",
    "        new_list = []\n",
    "        for item in cell:\n",
    "            new_item = process_column_cell(item)\n",
    "            new_list.append(new_item)\n",
    "        return new_list\n",
    "\n",
    "    elif isinstance(cell, dict):\n",
    "        new_dict = {}\n",
    "        for key, value in cell.items():\n",
    "            new_key = replace_md_with_csv(key)\n",
    "            new_value = process_column_cell(value)\n",
    "            new_dict[new_key] = new_value\n",
    "        return new_dict\n",
    "\n",
    "    else:\n",
    "        return cell\n",
    "    \n",
    "for col in filtered_ref_summary.columns:\n",
    "    filtered_ref_summary[col] = filtered_ref_summary[col].apply(process_column_cell)\n",
    "\n",
    "filtered_ref_summary.to_csv(\"filtered_ref_summary_csv_replaced.csv\", index=False)\n",
    "filtered_ref_summary.head()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d20edad-a67b-49d6-a04e-c7dd5816c91a",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Structuring the Data into CSV - Step 2 OCR on image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81eb9363-517d-4c41-bad0-a2f5057a5cee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "137d498a-c3d1-438b-817e-63973aaf66c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488\n"
     ]
    }
   ],
   "source": [
    "png_paths = []\n",
    "\n",
    "\n",
    "# Gather every png path from the df\n",
    "def gather_pngs(dataframe_name):\n",
    "    for column in dataframe_name.columns:\n",
    "\n",
    "        # Loop through each cell in that column\n",
    "        for cell in dataframe_name[column]:\n",
    "\n",
    "            # Case 1: The cell is a simple string\n",
    "            if isinstance(cell, str):\n",
    "                if cell.endswith(\".png\"):\n",
    "                    png_paths.append(cell)\n",
    "\n",
    "            # List\n",
    "            elif isinstance(cell, list):\n",
    "                for item in cell:\n",
    "                    if isinstance(item, str) and item.endswith(\".png\"):\n",
    "                        png_paths.append(item)\n",
    "\n",
    "            # Dictionary\n",
    "            elif isinstance(cell, dict):\n",
    "                for key, value in cell.items():\n",
    "                    # Check key\n",
    "                    if isinstance(key, str) and key.endswith(\".png\"):\n",
    "                        png_paths.append(key)\n",
    "\n",
    "                    # Check value\n",
    "                    if isinstance(value, str) and value.endswith(\".png\"):\n",
    "                        png_paths.append(value)\n",
    "\n",
    "                    # If the value is a list of paths\n",
    "                    elif isinstance(value, list):\n",
    "                        for sub_item in value:\n",
    "                            if isinstance(sub_item, str) and sub_item.endswith(\".png\"):\n",
    "                                png_paths.append(sub_item)\n",
    "                                \n",
    "gather_pngs(filtered_ref_summary)\n",
    "print(len(png_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe507b-130f-4b49-a039-214c8505cbee",
   "metadata": {},
   "source": [
    "# **Extraction with LLAVA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fbe48e62-26a6-411a-b0ba-3aeaaa33b381",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 LLaVA Output:\n",
      "  The image contains several lines of text, which are written in Simplified Chinese characters. Here's the transcription of the visible text:\n",
      "\n",
      "韶华，我想跟你说些什么。其实是这样的：我们在一个困难的时代，需要不断地调整自己的思维方式和态度。只有我们通过不断的学习、探索和实践，才能真正获得人生中所需的技能和经验，并在面对困难时保持乐观和积极的态度。\n",
      "\n",
      "Translation:\n",
      "\n",
      "Xu Hong, I want to tell you something. Actually it's like this: we are in a difficult time when we need to constantly adjust our thinking and attitude. Only through continuous learning, exploration, and practice can we truly acquire the skills and experience needed for life and maintain an optimistic and positive attitude when facing difficulties.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "# Load and base64-encode PNG image\n",
    "image_path = \"0/png/300450bf-221e-4eeb-bdda-dc1115c947ea.png\"\n",
    "with open(image_path, \"rb\") as f:\n",
    "    image_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "prompt = \"EXTRACT AND ONLY GIVE ME THE Chinese text in the given image.\"\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/generate\",\n",
    "    json={\n",
    "        \"model\": \"llava:13b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"images\": [image_b64],\n",
    "        \"stream\": False\n",
    "    },\n",
    "    timeout=60\n",
    ")\n",
    "\n",
    "\n",
    "if response.ok:\n",
    "    print(\"LLaVA Output:\\n\", response.json()[\"response\"])\n",
    "else:\n",
    "    print(\"Error:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb2ca5-83ac-4f57-a227-61dd217580c2",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# PaddleOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6079a28-5dbb-44fe-82ca-ff9a65092a48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /home/jovyan/.paddlex/official_models.\u001b[0m\n",
      "E0526 11:49:37.156906  2204 analysis_config.cc:169] Please use PaddlePaddle with GPU version.\n",
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[32mUsing official model (UVDoc), the model files will be automatically downloaded and saved in /home/jovyan/.paddlex/official_models.\u001b[0m\n",
      "E0526 11:49:37.669418  2204 analysis_config.cc:169] Please use PaddlePaddle with GPU version.\n",
      "\u001b[32mCreating model: ('PP-LCNet_x0_25_textline_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x0_25_textline_ori), the model files will be automatically downloaded and saved in /home/jovyan/.paddlex/official_models.\u001b[0m\n",
      "E0526 11:49:40.892796  2204 analysis_config.cc:169] Please use PaddlePaddle with GPU version.\n",
      "\u001b[32mCreating model: ('PP-OCRv5_mobile_det', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv5_mobile_det), the model files will be automatically downloaded and saved in /home/jovyan/.paddlex/official_models.\u001b[0m\n",
      "E0526 11:49:41.269107  2204 analysis_config.cc:169] Please use PaddlePaddle with GPU version.\n",
      "\u001b[32mCreating model: ('PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv5_mobile_rec), the model files will be automatically downloaded and saved in /home/jovyan/.paddlex/official_models.\u001b[0m\n",
      "E0526 11:49:42.655756  2204 analysis_config.cc:169] Please use PaddlePaddle with GPU version.\n"
     ]
    }
   ],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "import os\n",
    "\n",
    "ocr_model = PaddleOCR(lang='ch')\n",
    "\n",
    "def paddle_ocr(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"File not found - {image_path}\")\n",
    "        return \"\"\n",
    "\n",
    "    result = ocr_model.ocr(image_path)\n",
    "\n",
    "    if isinstance(result, list) and isinstance(result[0], dict) and 'rec_texts' in result[0]:\n",
    "        return \"\\n\".join(result[0]['rec_texts'])\n",
    "\n",
    "    extracted_text = []\n",
    "    for line in result[0]:\n",
    "        text = line[1][0]  # text, confidence\n",
    "        extracted_text.append(text)\n",
    "\n",
    "    return \"\\n\".join(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859064e-11fd-4143-ada1-16f18c5339e3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6691298c-1ae6-4a48-934a-2d56c8543584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c038d2f7-1513-4a68-ab9c-5a232476e93e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the Chinese and English model\n",
    "ocr_model = easyocr.Reader(['ch_sim'], gpu = True)  # Load once\n",
    "\n",
    "def easy_ocr(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"File not found - {image_path}\")\n",
    "        return \"\"\n",
    "\n",
    "    results = ocr_model.readtext(image_path, detail=0)  # Only return text, not boxes/confidences\n",
    "\n",
    "    return \"\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e735e605-9db2-40cb-89b9-b3de0a99de6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'ocr' does not exist.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Directory to delete\n",
    "EXTRACT_DIR = 'ocr'\n",
    "\n",
    "# Check if the directory exists and delete it\n",
    "if os.path.isdir(EXTRACT_DIR):\n",
    "    shutil.rmtree(EXTRACT_DIR)\n",
    "    print(f\"Directory '{EXTRACT_DIR}' has been deleted.\")\n",
    "else:\n",
    "    print(f\"Directory '{EXTRACT_DIR}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93149a3a-4b10-4316-bca7-92c5574e94d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a22a2-9790-46bd-85ca-437f60db484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "output_dir = 'ocr'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "MAX_WORKERS = 2\n",
    "\n",
    "def process_image(image_path):\n",
    "    try:\n",
    "        base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        output_csv_path = os.path.join(output_dir, f\"{base_name}.csv\")\n",
    "        if os.path.exists(output_csv_path):\n",
    "            return (image_path, \"Skipped (already exists)\")\n",
    "\n",
    "        ocr_result = easy_ocr(image_path)\n",
    "        combined_text = \" \".join(ocr_result.splitlines())\n",
    "\n",
    "        with open(output_csv_path, mode='w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['ocr_transf'])\n",
    "            writer.writerow([combined_text])\n",
    "\n",
    "        return (image_path, \"Success\")\n",
    "    except Exception as e:\n",
    "        return (image_path, f\"Error: {e}\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = [executor.submit(process_image, path) for path in png_paths]\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Parallel OCR with EasyOCR\"):\n",
    "        image_path, status = future.result()\n",
    "        if \"Error\" in status:\n",
    "            print(f\"[!] Failed: {image_path} -> {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d7832-00e2-4fff-85af-d3d9ffc2aea5",
   "metadata": {},
   "source": [
    "# **DATA TRANSLATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8e79992-2b44-4424-98d4-c5783bd96644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff4c91a1-3686-4dc0-a215-7c5f89efd28d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           other  \\\n",
      "0  csvs/9d7bc879-3250-4013-ac04-5ff9bd6dff40.csv   \n",
      "1                                           None   \n",
      "2                                           None   \n",
      "3                                           None   \n",
      "4                                           None   \n",
      "\n",
      "                                          images        chats  \n",
      "0  csvs/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.csv  csvs/28.csv  \n",
      "1  csvs/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.csv   csvs/5.csv  \n",
      "2  csvs/3348953d-66e9-4cac-8675-65bb5f2ef929.csv  csvs/38.csv  \n",
      "3  csvs/07f179c5-5705-4dbd-94a7-66eed1e066b0.csv  csvs/18.csv  \n",
      "4  csvs/01cdc26f-e773-4ad7-8808-d04abf16aae7.csv   csvs/1.csv  \n"
     ]
    }
   ],
   "source": [
    "# Use the csvs folder and the LLM classification of MD files to create a new df that stores the csvs into categories.\n",
    "\n",
    "csv_folder = \"csvs\"\n",
    "\n",
    "csv_mapping = {}\n",
    "# Loop through each file in the CSV folder\n",
    "for filename in os.listdir(csv_folder):\n",
    "    # Check for csv\n",
    "    if filename.endswith('.csv'):\n",
    "        # Replace the \".csv\" extension with \".md\" to create the key\n",
    "        md_filename = filename.replace('.csv', '.md')\n",
    "        \n",
    "        # Set the value to the full path to the CSV file \n",
    "        csv_full_path = os.path.join(csv_folder, filename)\n",
    "        \n",
    "        # Add the key-value pair to the mapping\n",
    "        csv_mapping[md_filename] = csv_full_path\n",
    "\n",
    "# Step 2: Define function to replace .md path with .csv path\n",
    "def replace_md_with_csv(val):\n",
    "    if isinstance(val, str):\n",
    "        base = os.path.basename(val)\n",
    "        return csv_mapping.get(base, val)\n",
    "    return val\n",
    "\n",
    "df_csvs_llm = result_df.applymap(replace_md_with_csv)\n",
    "df_csvs_llm.to_csv(\"updated_with_csv_paths.csv\", index=False)\n",
    "print(df_csvs_llm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a832e86-3fc0-4e62-a49f-8a56d42d5ac2",
   "metadata": {},
   "source": [
    "# **Larger Model for Translation - Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703a4971-7d0b-4c8e-ae00-78706f144c00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with Ollama: 100%|██████████| 149/149 [07:10<00:00,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "csv_path = 'csvs/10.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Create translation column in the df\n",
    "df['message_translation'] = \"\"\n",
    "\n",
    "# Make a function to ffed context to the LLM for a better translation, takes 5 messages up and down\n",
    "def get_context(df, index, window=5):\n",
    "    start = max(0, index - window)\n",
    "    end = min(len(df), index + window + 1)\n",
    "    return \"\\n\".join(df['Message'].iloc[start:end].dropna())\n",
    "\n",
    "def query_ollama(prompt, model=\"gemma3:27b\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=90)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama error: {e}\")\n",
    "        return \"[Translation Error]\"\n",
    "\n",
    "# Method to process one row \n",
    "def translate_row(index):\n",
    "    target = df.at[index, 'Message']\n",
    "    if pd.isna(target):\n",
    "        return index, \"\"\n",
    "    context = get_context(df, index)\n",
    "    prompt = (\n",
    "        \"You are translating Chinese messages to English. Below is a series of related messages. \"\n",
    "        \"Use the full context to understand the meaning, but only translate the specific message provided.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Message to translate:\\n{target}\\n\\n\"\n",
    "        \"ONLY RETURN the English translation of the message, ANYTHING ELSE IS FORBIDDEN! \"\n",
    "        \"If you encounter a filename or file reference, preserve it exactly as it appears, DON'T add anything!\"\n",
    "    )\n",
    "    translation = query_ollama(prompt)\n",
    "    return index, translation\n",
    "\n",
    "# Run translations in parallel\n",
    "futures = []\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for index in df.index:\n",
    "        futures.append(executor.submit(translate_row, index))\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Translating with Ollama\"):\n",
    "        idx, result = future.result()\n",
    "        df.at[idx, 'message_translation'] = result\n",
    "\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d84e49b-797c-4c4b-b63e-f3d19968c59b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Columns:\n",
      "['Time', 'From', 'To', 'Message']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV\n",
    "df = pd.read_csv(\"csvs/10.csv\")\n",
    "\n",
    "# Print all column names to verify exact labels\n",
    "print(\"CSV Columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Drop the specified columns (only if they exist)\n",
    "columns_to_drop = [\"message_translation\", \"financial_detection\"]\n",
    "df_cleaned = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# Overwrite the original file\n",
    "df_cleaned.to_csv(\"csvs/10.csv\", index=False)  # Overwriting the same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a33b558c-25cf-451e-9ccd-92892c0ef1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: csvs/28.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/28.csv: 100%|██████████| 46/46 [02:55<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/28.csv\n",
      "\n",
      "Processing: csvs/5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/5.csv: 100%|██████████| 17/17 [00:24<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/5.csv\n",
      "\n",
      "Processing: csvs/38.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/38.csv: 100%|██████████| 21/21 [01:18<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/38.csv\n",
      "\n",
      "Processing: csvs/18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/18.csv: 100%|██████████| 84/84 [02:10<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/18.csv\n",
      "\n",
      "Processing: csvs/1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/1.csv:   7%|▋         | 550/8290 [20:25<13:53:57,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/1.csv:  68%|██████▊   | 5642/8290 [3:25:59<6:53:56,  9.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/1.csv:  69%|██████▉   | 5711/8290 [3:30:46<5:57:09,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/1.csv:  77%|███████▋  | 6419/8290 [4:01:12<50:27,  1.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/1.csv:  87%|████████▋ | 7230/8290 [4:35:32<2:34:48,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/1.csv:  88%|████████▊ | 7268/8290 [4:37:49<2:13:42,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/1.csv: 100%|██████████| 8290/8290 [5:20:43<00:00,  2.32s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/1.csv\n",
      "\n",
      "Processing: csvs/19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/19.csv: 100%|██████████| 21/21 [01:04<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/19.csv\n",
      "\n",
      "Processing: csvs/29.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/29.csv: 100%|██████████| 80/80 [03:05<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/29.csv\n",
      "\n",
      "Processing: csvs/4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/4.csv: 100%|██████████| 513/513 [17:16<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/4.csv\n",
      "\n",
      "Processing: csvs/39.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/39.csv: 100%|██████████| 821/821 [27:27<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/39.csv\n",
      "\n",
      "Processing: csvs/16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/16.csv: 100%|██████████| 149/149 [04:35<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/16.csv\n",
      "\n",
      "Processing: csvs/22.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/22.csv: 100%|██████████| 91/91 [03:16<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/22.csv\n",
      "\n",
      "Processing: csvs/32.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/32.csv: 100%|██████████| 86/86 [02:35<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/32.csv\n",
      "\n",
      "Processing: csvs/26.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/26.csv: 100%|██████████| 6/6 [00:17<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/26.csv\n",
      "\n",
      "Processing: csvs/12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/12.csv: 100%|██████████| 392/392 [13:52<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/12.csv\n",
      "\n",
      "Processing: csvs/36.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/36.csv: 100%|██████████| 199/199 [05:28<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/36.csv\n",
      "\n",
      "Processing: csvs/27.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/27.csv: 100%|██████████| 29/29 [02:44<00:00,  5.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n",
      "Finished and saved: csvs/27.csv\n",
      "\n",
      "Processing: csvs/13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/13.csv: 100%|██████████| 244/244 [08:18<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/13.csv\n",
      "\n",
      "Processing: csvs/37.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/37.csv:  10%|▉         | 32/329 [02:47<52:10, 10.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/37.csv:  16%|█▋        | 54/329 [04:46<21:03,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/37.csv:  27%|██▋       | 90/329 [07:55<16:51,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/37.csv: 100%|██████████| 329/329 [16:39<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/37.csv\n",
      "\n",
      "Processing: csvs/17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/17.csv: 100%|██████████| 55/55 [01:42<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/17.csv\n",
      "\n",
      "Processing: csvs/23.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/23.csv: 100%|██████████| 24/24 [00:45<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/23.csv\n",
      "\n",
      "Processing: csvs/33.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/33.csv: 100%|██████████| 31/31 [00:52<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/33.csv\n",
      "\n",
      "Processing: csvs/24.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/24.csv: 100%|██████████| 55/55 [01:57<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/24.csv\n",
      "\n",
      "Processing: csvs/41.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/41.csv: 100%|██████████| 322/322 [10:44<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/41.csv\n",
      "\n",
      "Processing: csvs/10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/10.csv: 100%|██████████| 149/149 [06:13<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/10.csv\n",
      "\n",
      "Processing: csvs/9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/9.csv: 100%|██████████| 235/235 [08:11<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/9.csv\n",
      "\n",
      "Processing: csvs/34.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/34.csv: 100%|██████████| 426/426 [13:25<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/34.csv\n",
      "\n",
      "Processing: csvs/14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/14.csv: 100%|██████████| 44/44 [01:18<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/14.csv\n",
      "\n",
      "Processing: csvs/20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/20.csv: 100%|██████████| 68/68 [02:06<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/20.csv\n",
      "\n",
      "Processing: csvs/30.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/30.csv: 100%|██████████| 322/322 [08:36<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/30.csv\n",
      "\n",
      "Processing: csvs/15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/15.csv: 100%|██████████| 199/199 [06:06<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/15.csv\n",
      "\n",
      "Processing: csvs/21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/21.csv: 100%|██████████| 1046/1046 [24:47<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/21.csv\n",
      "\n",
      "Processing: csvs/31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/31.csv: 100%|██████████| 44/44 [01:06<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/31.csv\n",
      "\n",
      "Processing: csvs/40.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/40.csv: 100%|██████████| 129/129 [04:50<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/40.csv\n",
      "\n",
      "Processing: csvs/11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/11.csv: 100%|██████████| 125/125 [04:12<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/11.csv\n",
      "\n",
      "Processing: csvs/35.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/35.csv: 100%|██████████| 196/196 [06:40<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/35.csv\n",
      "\n",
      "Processing: csvs/3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/3.csv: 100%|██████████| 23/23 [00:37<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/3.csv\n",
      "\n",
      "Processing: csvs/7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/7.csv: 100%|██████████| 196/196 [05:34<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/7.csv\n",
      "\n",
      "Processing: csvs/6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/6.csv: 100%|██████████| 136/136 [03:08<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/6.csv\n",
      "\n",
      "Processing: csvs/2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: csvs/2.csv: 100%|██████████| 500/500 [16:03<00:00,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved: csvs/2.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Chats - CSV loop\n",
    "for csv_path in df_csvs_llm['chats']:\n",
    "    print(f\"Processing: {csv_path}\")\n",
    "    \n",
    "    # Load the individual CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['message_translation'] = \"\"\n",
    "\n",
    "    # Function to get context around the target message\n",
    "    def get_context(df, index, window=5):\n",
    "        start = max(0, index - window)\n",
    "        end = min(len(df), index + window + 1)\n",
    "        return \"\\n\".join(df['Message'].iloc[start:end].dropna())\n",
    "\n",
    "    def query_ollama(prompt, model=\"gemma3:27b\"):\n",
    "        url = \"http://localhost:11434/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(url, json=payload, timeout=90)\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"response\"].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Ollama error: {e}\")\n",
    "            return \"[Translation Error]\"\n",
    "\n",
    "    # Function to process one row\n",
    "    def translate_row(index):\n",
    "        target = df.at[index, 'Message']\n",
    "        if pd.isna(target):\n",
    "            return index, \"\"\n",
    "        context = get_context(df, index)\n",
    "        prompt = (\n",
    "            \"You are translating Chinese messages to English. Below is a series of related messages. \"\n",
    "            \"Use the full context to understand the meaning, but only translate the specific message provided.\\n\\n\"\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Message to translate:\\n{target}\\n\\n\"\n",
    "            \"ONLY RETURN the English translation of the message, ANYTHING ELSE IS FORBIDDEN! \"\n",
    "            \"If you encounter a filename or file reference, preserve it exactly as it appears, DON'T add anything!\"\n",
    "        )\n",
    "        translation = query_ollama(prompt)\n",
    "        return index, translation\n",
    "\n",
    "    # Run translations in parallel\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for index in df.index:\n",
    "            futures.append(executor.submit(translate_row, index))\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Translating: {csv_path}\"):\n",
    "            idx, result = future.result()\n",
    "            df.at[idx, 'message_translation'] = result\n",
    "\n",
    "    # Save updated DataFrame back to the original file\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Finished and saved: {csv_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f24fa3-ebc0-454c-a5a4-6e03836fdf0a",
   "metadata": {},
   "source": [
    "# **USER ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59e79b82-041e-4338-ab5a-babecde51a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_unique_users_from_csv(csv_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {csv_path}: {e}\")\n",
    "        return set()\n",
    "\n",
    "    # Try to find user-related columns\n",
    "    user_columns = [col for col in df.columns if col.lower() in ['from', 'to', 'sender', 'receiver', 'user']]\n",
    "    \n",
    "    if not user_columns:\n",
    "        user_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Flatten and clean user list\n",
    "    users = pd.unique(df[user_columns].values.ravel())\n",
    "    return {str(u).strip() for u in users if pd.notna(u) and str(u).strip() != ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3624c820-6029-4b82-b649-30e5b4e91cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users found in chat logs:\n",
      "- 25713010771@chatroom\n",
      "- SWEET5683yao\n",
      "- Shutd0wn\n",
      "- adpw90\n",
      "- dujijiyiqxx\n",
      "- gzp1991101\n",
      "- hack05112\n",
      "- ibabaimama\n",
      "- just910420\n",
      "- ken73224\n",
      "- lengmo\n",
      "- nullroot\n",
      "- qq78263462\n",
      "- snipersk\n",
      "- tianyi-0608\n",
      "- wangchao953541\n",
      "- wei592628\n",
      "- wxid_12n748um1thl21\n",
      "- wxid_5390224027312\n",
      "- wxid_70w3p1jin84k22\n",
      "- wxid_7p054rmzkhqf21\n",
      "- wxid_blw54o1q0q5w22\n",
      "- wxid_c9yv0nsla3yn22\n",
      "- wxid_hlmnhsq64tt722\n",
      "- wxid_icges6alg8cl21\n",
      "- wxid_jcnxegjccqi441\n",
      "- wxid_kbys0kvzj4ta12\n",
      "- wxid_mgh25nentc4u22\n",
      "- wxid_nv9bv435fz3722\n",
      "- wxid_q7vkst94g5u011\n",
      "- wxid_soekgggwnfgm21\n",
      "- wxid_wh6x59w70y3r22\n",
      "- wxid_xusilpfkh31g21\n",
      "- wxid_zb45i0rc71yk21\n",
      "- wxid_zbytkn4qjl3r22\n",
      "- yanzi542766277\n",
      "- zhangxiaoyan0422\n"
     ]
    }
   ],
   "source": [
    "all_users = set()\n",
    "\n",
    "# Loop through each file path in 'chats' column\n",
    "for csv_path in df_csvs_llm['chats'].dropna().unique():\n",
    "    if isinstance(csv_path, str) and csv_path.endswith('.csv') and os.path.exists(csv_path):\n",
    "        users = extract_unique_users_from_csv(csv_path)\n",
    "        all_users.update(users)\n",
    "\n",
    "unique_users_from_chats = sorted(all_users)\n",
    "print(\"Unique users found in chat logs:\")\n",
    "for user in unique_users_from_chats:\n",
    "    print(\"-\", user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e7d53f-7376-40b2-9818-452231587964",
   "metadata": {},
   "source": [
    "# **Financial Infrastructures Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7847de-f686-4967-b334-2b9ebee7c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43ba5cba-946f-4657-a3b8-d9d51eb411b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_290/2611312270.py:37: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  vector_store = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# Test for 1 csv\n",
    "df = pd.read_csv(\"csvs/10.csv\")\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "# Setup vector DB path\n",
    "db_location = \"./chroma_financial_chat_db\"\n",
    "add_documents = not os.path.exists(db_location)\n",
    "\n",
    "# Only add documents if the database doesn't already exist\n",
    "if add_documents:\n",
    "    documents = []\n",
    "    ids = []\n",
    "    for i, row in df.iterrows():\n",
    "        document = Document(\n",
    "            page_content=row[\"message_translation\"],\n",
    "            metadata={\n",
    "                \"sender\": row[\"From\"],                 \n",
    "                \"receiver\": row[\"To\"],\n",
    "                \"timestamp\": row[\"Time\"] \n",
    "            },\n",
    "            id=str(i)\n",
    "        )\n",
    "        documents.append(document)\n",
    "        ids.append(str(i))\n",
    "\n",
    "# Create or load the Chroma vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"financial_chats\",\n",
    "    persist_directory=db_location,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "if add_documents:\n",
    "    vector_store.add_documents(documents=documents, ids=ids)\n",
    "\n",
    "# Setup retriever for use in RAG\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ad2851e-9636-4ac5-a439-27f4c1c0710e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = OllamaLLM(model=\"gemma3:27b\") \n",
    "\n",
    "template = \"\"\"\n",
    "You are a cybersecurity analyst reviewing translated chat messages for financial intelligence.\n",
    "\n",
    "Your task is to extract ONLY the messages that contain **financially relevant information**, such as:\n",
    "\n",
    "- Payments or money transfers\n",
    "- Amounts, currencies, or sums of money\n",
    "- Payment methods (cash, crypto, bank, wallets)\n",
    "- Account numbers, codes, or financial identifiers\n",
    "- Mentions of financial roles (payer, payee, client, broker)\n",
    "- References to suspicious or recurring financial activity\n",
    "\n",
    "From the messages below:\n",
    "{chats}\n",
    "\n",
    "Return ONLY the messages that meet any of the criteria above.\n",
    "\n",
    "For each relevant message, output:\n",
    "- The **timestamp** (if provided in the metadata or inferred)\n",
    "- The **translated content** of the message\n",
    "\n",
    "Do not summarize. Do not explain. Do not include non-financial content.  \n",
    "**DO NOT create or infer messages. Use only the exact text from the messages provided. If no messages match, return nothing.**\n",
    "\n",
    "Output format:\n",
    "[Time] Message\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12e153ea-7ffe-4452-bb03-e3e1856130cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account TGtadie, phone: 18510867099, name: Wang Ning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Extract only the chat messages that refer to money, payments, currencies, accounts, financial identifiers, or finanacial discussions\"\n",
    "\n",
    "# Retrieve relevant chats\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "# Combine into one context string\n",
    "chat_texts = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# Run through the LLM\n",
    "response = chain.invoke({\"chats\": chat_texts})\n",
    "\n",
    "# Output the result\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb56c2cf-16c4-4845-a94a-d2c358d51ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (IsoonAI)",
   "language": "python",
   "name": "isoonai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
