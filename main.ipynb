{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6d6a8f",
   "metadata": {},
   "source": [
    "# **IMPORTS & PATHS** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "879be4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Paths and model settings\\ nZIP_PATH = 'I-Soon-data.zip'      # Path to your downloaded zip file\n",
    "ZIP_PATH = '0.zip'    # Directory to extract contents\n",
    "EXTRACT_DIR = 'I-Soon-data'        # Directory to extract contents\n",
    "ORGANIZED_DIR = 'organized_data'  # Directory to group files by extension\n",
    "MODEL_NAME = 'llama3.2'             # Local Ollama model identifier\n",
    "OUTPUT_JSON = 'parsed_md.json'    # Aggregated JSON output\n",
    "OUTPUT_CSV = 'parsed_md.csv'      # CSV output for DataFrame\n",
    "\n",
    "# Initialize Ollama LLM via LangChain\n",
    "model = OllamaLLM(model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f9f3fd",
   "metadata": {},
   "source": [
    "# **ZIP FILE EXTRACTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e6feb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction directory 'I-Soon-data' already exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(EXTRACT_DIR):\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_DIR)\n",
    "    print(f\"Extracted archive to '{EXTRACT_DIR}'\")\n",
    "else:\n",
    "    print(f\"Extraction directory '{EXTRACT_DIR}' already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a01d8",
   "metadata": {},
   "source": [
    "# **DATA TYPE CATEGORIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dec65000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organized files under 'organized_data':\n",
      " • Markdown with HTML → md/html/\n",
      " • Markdown without HTML → md/non-html/\n",
      " • Other extensions → <extension>/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# regex to detect any HTML tag\n",
    "html_re = re.compile(r'<[A-Za-z/][^>]*>')\n",
    "\n",
    "for root, _, files in os.walk(EXTRACT_DIR):\n",
    "    for fname in files:\n",
    "        ext = os.path.splitext(fname)[1].lower().lstrip('.') or 'no_extension'\n",
    "        src_path = os.path.join(root, fname)\n",
    "\n",
    "        if ext == 'md':\n",
    "            # classify Markdown by content into md/html or md/non-html\n",
    "            with open(src_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "            subfolder = 'html' if html_re.search(text) else 'non-html'\n",
    "            dest_folder = os.path.join(ORGANIZED_DIR, 'md', subfolder)\n",
    "        else:\n",
    "            # preserve original extension grouping for non-MD\n",
    "            dest_folder = os.path.join(ORGANIZED_DIR, ext)\n",
    "\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        dst_path = os.path.join(dest_folder, fname)\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "print(f\"Organized files under '{ORGANIZED_DIR}':\\n\"\n",
    "      \" • Markdown with HTML → md/html/\\n\"\n",
    "      \" • Markdown without HTML → md/non-html/\\n\"\n",
    "      \" • Other extensions → <extension>/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a6aea",
   "metadata": {},
   "source": [
    "# **MD DATA TRANSFORMATIO TO JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "321522dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted: dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md → html_markdown_to_json/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac_docling.json\n",
      "✅ Converted: 28.md → html_markdown_to_json/28_docling.json\n",
      "✅ Converted: 5.md → html_markdown_to_json/5_docling.json\n",
      "✅ Converted: 38.md → html_markdown_to_json/38_docling.json\n",
      "✅ Converted: 9d7bc879-3250-4013-ac04-5ff9bd6dff40.md → html_markdown_to_json/9d7bc879-3250-4013-ac04-5ff9bd6dff40_docling.json\n",
      "✅ Converted: 18.md → html_markdown_to_json/18_docling.json\n",
      "❌ Failed to convert ._3.md: 'utf-8' codec can't decode byte 0xfb in position 37: invalid start byte (moved to failed_parsing)\n",
      "✅ Converted: 9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md → html_markdown_to_json/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b_docling.json\n",
      "✅ Converted: 3348953d-66e9-4cac-8675-65bb5f2ef929.md → html_markdown_to_json/3348953d-66e9-4cac-8675-65bb5f2ef929_docling.json\n",
      "✅ Converted: 1.md → html_markdown_to_json/1_docling.json\n",
      "✅ Converted: 19.md → html_markdown_to_json/19_docling.json\n",
      "✅ Converted: 07f179c5-5705-4dbd-94a7-66eed1e066b0.md → html_markdown_to_json/07f179c5-5705-4dbd-94a7-66eed1e066b0_docling.json\n",
      "✅ Converted: 29.md → html_markdown_to_json/29_docling.json\n",
      "✅ Converted: 01cdc26f-e773-4ad7-8808-d04abf16aae7.md → html_markdown_to_json/01cdc26f-e773-4ad7-8808-d04abf16aae7_docling.json\n",
      "✅ Converted: 585875ff-f8c5-4a02-acd7-fef37dc9ff11.md → html_markdown_to_json/585875ff-f8c5-4a02-acd7-fef37dc9ff11_docling.json\n",
      "✅ Converted: 4.md → html_markdown_to_json/4_docling.json\n",
      "✅ Converted: 39.md → html_markdown_to_json/39_docling.json\n",
      "✅ Converted: 64bba692-d430-440c-9f1e-2575f45770af.md → html_markdown_to_json/64bba692-d430-440c-9f1e-2575f45770af_docling.json\n",
      "✅ Converted: 178e3898-903d-47cf-bfbe-061e7dc18895.md → html_markdown_to_json/178e3898-903d-47cf-bfbe-061e7dc18895_docling.json\n",
      "✅ Converted: 16.md → html_markdown_to_json/16_docling.json\n",
      "✅ Converted: 22.md → html_markdown_to_json/22_docling.json\n",
      "✅ Converted: 32.md → html_markdown_to_json/32_docling.json\n",
      "✅ Converted: 9fe6b262-9944-417d-a0c4-9f2de1de2994.md → html_markdown_to_json/9fe6b262-9944-417d-a0c4-9f2de1de2994_docling.json\n",
      "✅ Converted: f7205881-3904-42ec-ab2c-04f36fa24785.md → html_markdown_to_json/f7205881-3904-42ec-ab2c-04f36fa24785_docling.json\n",
      "✅ Converted: 54990932-71af-48dd-9a7a-2617b1407c54.md → html_markdown_to_json/54990932-71af-48dd-9a7a-2617b1407c54_docling.json\n",
      "✅ Converted: 26.md → html_markdown_to_json/26_docling.json\n",
      "✅ Converted: 12.md → html_markdown_to_json/12_docling.json\n",
      "✅ Converted: 36.md → html_markdown_to_json/36_docling.json\n",
      "✅ Converted: eda5b003-9250-4913-b724-74cca86240af.md → html_markdown_to_json/eda5b003-9250-4913-b724-74cca86240af_docling.json\n",
      "✅ Converted: 27.md → html_markdown_to_json/27_docling.json\n",
      "✅ Converted: 13.md → html_markdown_to_json/13_docling.json\n",
      "✅ Converted: 37.md → html_markdown_to_json/37_docling.json\n",
      "✅ Converted: 5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0.md → html_markdown_to_json/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_docling.json\n",
      "✅ Converted: 17.md → html_markdown_to_json/17_docling.json\n",
      "✅ Converted: 23.md → html_markdown_to_json/23_docling.json\n",
      "✅ Converted: 33.md → html_markdown_to_json/33_docling.json\n",
      "✅ Converted: 5387a301-0af8-4e24-a197-20189f87b9ef.md → html_markdown_to_json/5387a301-0af8-4e24-a197-20189f87b9ef_docling.json\n",
      "✅ Converted: 48fd4c79-41ca-459e-a5a5-a3738e7a4af3.md → html_markdown_to_json/48fd4c79-41ca-459e-a5a5-a3738e7a4af3_docling.json\n",
      "✅ Converted: f179eb06-0c53-44df-a13f-570be23355bb.md → html_markdown_to_json/f179eb06-0c53-44df-a13f-570be23355bb_docling.json\n",
      "✅ Converted: aedc6a39-7862-4bbc-99e7-780ab3980282.md → html_markdown_to_json/aedc6a39-7862-4bbc-99e7-780ab3980282_docling.json\n",
      "✅ Converted: 24.md → html_markdown_to_json/24_docling.json\n",
      "✅ Converted: 41.md → html_markdown_to_json/41_docling.json\n",
      "✅ Converted: 10.md → html_markdown_to_json/10_docling.json\n",
      "✅ Converted: 9.md → html_markdown_to_json/9_docling.json\n",
      "✅ Converted: 34.md → html_markdown_to_json/34_docling.json\n",
      "✅ Converted: 14.md → html_markdown_to_json/14_docling.json\n",
      "✅ Converted: b3031e66-40b6-45e8-9bcd-891dc1a280da.md → html_markdown_to_json/b3031e66-40b6-45e8-9bcd-891dc1a280da_docling.json\n",
      "✅ Converted: 20.md → html_markdown_to_json/20_docling.json\n",
      "✅ Converted: 30.md → html_markdown_to_json/30_docling.json\n",
      "✅ Converted: d5ff8b65-db15-418a-b33e-169498d79110.md → html_markdown_to_json/d5ff8b65-db15-418a-b33e-169498d79110_docling.json\n",
      "✅ Converted: fe245192-1f9c-4f28-9b32-046fb7ce7e1e.md → html_markdown_to_json/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_docling.json\n",
      "✅ Converted: 15.md → html_markdown_to_json/15_docling.json\n",
      "✅ Converted: 21.md → html_markdown_to_json/21_docling.json\n",
      "✅ Converted: 31.md → html_markdown_to_json/31_docling.json\n",
      "✅ Converted: d410e4aa-fb52-4ed4-9078-4483267a02b3.md → html_markdown_to_json/d410e4aa-fb52-4ed4-9078-4483267a02b3_docling.json\n",
      "✅ Converted: 40.md → html_markdown_to_json/40_docling.json\n",
      "✅ Converted: 11.md → html_markdown_to_json/11_docling.json\n",
      "✅ Converted: 547aba02-6757-49c1-acb5-6df217cebfc7.md → html_markdown_to_json/547aba02-6757-49c1-acb5-6df217cebfc7_docling.json\n",
      "✅ Converted: 35.md → html_markdown_to_json/35_docling.json\n",
      "✅ Converted: 12756724-394c-4576-b373-7c53f1abbd94.md → html_markdown_to_json/12756724-394c-4576-b373-7c53f1abbd94_docling.json\n",
      "✅ Converted: 3.md → html_markdown_to_json/3_docling.json\n",
      "✅ Converted: e182d867-dc18-43fd-a418-26dcf784242f.md → html_markdown_to_json/e182d867-dc18-43fd-a418-26dcf784242f_docling.json\n",
      "✅ Converted: 6d7fc7b3-c892-4cb5-bd4b-a5713c089d88.md → html_markdown_to_json/6d7fc7b3-c892-4cb5-bd4b-a5713c089d88_docling.json\n",
      "✅ Converted: 912204cb-8ab7-48b8-9abf-d803f3804d08.md → html_markdown_to_json/912204cb-8ab7-48b8-9abf-d803f3804d08_docling.json\n",
      "✅ Converted: 2db27de1-d5c5-4f89-8572-da697a6329e4.md → html_markdown_to_json/2db27de1-d5c5-4f89-8572-da697a6329e4_docling.json\n",
      "✅ Converted: 7.md → html_markdown_to_json/7_docling.json\n",
      "✅ Converted: 5e5bd90e-60c5-402f-b488-750456a81a13.md → html_markdown_to_json/5e5bd90e-60c5-402f-b488-750456a81a13_docling.json\n",
      "✅ Converted: 3f451a52-d210-48d9-b56e-d28b9570bdc4.md → html_markdown_to_json/3f451a52-d210-48d9-b56e-d28b9570bdc4_docling.json\n",
      "✅ Converted: a1ba4d8b-f382-44c4-ac3f-746a44746bb4.md → html_markdown_to_json/a1ba4d8b-f382-44c4-ac3f-746a44746bb4_docling.json\n",
      "✅ Converted: 6.md → html_markdown_to_json/6_docling.json\n",
      "✅ Converted: 2.md → html_markdown_to_json/2_docling.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# Initialize converter\n",
    "converter = DocumentConverter()\n",
    "\n",
    "# Define input/output directories\n",
    "input_dir = \"organized_data/md/html\"\n",
    "output_dir = \"html_markdown_to_json\"\n",
    "failed_dir = \"failed_parsing\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(failed_dir, exist_ok=True)\n",
    "\n",
    "# Process each Markdown file\n",
    "for fname in os.listdir(input_dir):\n",
    "    if fname.endswith(\".md\"):\n",
    "        input_path = os.path.join(input_dir, fname)\n",
    "        \n",
    "        try:\n",
    "            result = converter.convert(input_path)\n",
    "            doc_dict = result.document.model_dump()\n",
    "            \n",
    "            # Output filename\n",
    "            base = os.path.splitext(fname)[0]\n",
    "            output_path = os.path.join(output_dir, f\"{base}_docling.json\")\n",
    "            \n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(doc_dict, f, indent=2)\n",
    "            \n",
    "            print(f\"✅ Converted: {fname} → {output_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Move to failed folder\n",
    "            shutil.move(input_path, os.path.join(failed_dir, fname))\n",
    "            print(f\"❌ Failed to convert {fname}: {e} (moved to {failed_dir})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232dc4db",
   "metadata": {},
   "source": [
    "# **JSON TO DATAFRAME WITH AI**\n",
    "# *Here we use AI to analyze all of the JSON files and identify which headers are usefull to keep. We store all the identified headers in a dictionary with the name of the json file as key and value the list contaning the headers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01e08cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 9fe6b262-9944-417d-a0c4-9f2de1de2994_docling.json: no tables found.\n",
      "Skipped 6d7fc7b3-c892-4cb5-bd4b-a5713c089d88_docling.json: no tables found.\n",
      "Skipped 912204cb-8ab7-48b8-9abf-d803f3804d08_docling.json: no tables found.\n",
      "Skipped b3031e66-40b6-45e8-9bcd-891dc1a280da_docling.json: no tables found.\n",
      "Skipped dbc9c90e-a3e6-4d71-bb93-5fb8394095ac_docling.json: no tables found.\n",
      "Skipped eda5b003-9250-4913-b724-74cca86240af_docling.json: no tables found.\n",
      "Skipped 12756724-394c-4576-b373-7c53f1abbd94_docling.json: no tables found.\n",
      "Skipped 5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_docling.json: no tables found.\n",
      "Skipped 547aba02-6757-49c1-acb5-6df217cebfc7_docling.json: no tables found.\n",
      "Skipped 54990932-71af-48dd-9a7a-2617b1407c54_docling.json: no tables found.\n",
      "Skipped 585875ff-f8c5-4a02-acd7-fef37dc9ff11_docling.json: no tables found.\n",
      "Skipped fe245192-1f9c-4f28-9b32-046fb7ce7e1e_docling.json: no tables found.\n",
      "Skipped 3f451a52-d210-48d9-b56e-d28b9570bdc4_docling.json: no tables found.\n",
      "Skipped 2db27de1-d5c5-4f89-8572-da697a6329e4_docling.json: no tables found.\n",
      "Skipped 01cdc26f-e773-4ad7-8808-d04abf16aae7_docling.json: no tables found.\n",
      "Skipped 48fd4c79-41ca-459e-a5a5-a3738e7a4af3_docling.json: no tables found.\n",
      "Skipped e182d867-dc18-43fd-a418-26dcf784242f_docling.json: no tables found.\n",
      "Skipped 64bba692-d430-440c-9f1e-2575f45770af_docling.json: no tables found.\n",
      "Skipped d5ff8b65-db15-418a-b33e-169498d79110_docling.json: no tables found.\n",
      "Skipped a1ba4d8b-f382-44c4-ac3f-746a44746bb4_docling.json: no tables found.\n",
      "Skipped 178e3898-903d-47cf-bfbe-061e7dc18895_docling.json: no tables found.\n",
      "Skipped 9d7bc879-3250-4013-ac04-5ff9bd6dff40_docling.json: no tables found.\n",
      "Skipped 07f179c5-5705-4dbd-94a7-66eed1e066b0_docling.json: no tables found.\n",
      "Skipped f7205881-3904-42ec-ab2c-04f36fa24785_docling.json: no tables found.\n",
      "Skipped 5e5bd90e-60c5-402f-b488-750456a81a13_docling.json: no tables found.\n",
      "Skipped aedc6a39-7862-4bbc-99e7-780ab3980282_docling.json: no tables found.\n",
      "Skipped f179eb06-0c53-44df-a13f-570be23355bb_docling.json: no tables found.\n",
      "Skipped d410e4aa-fb52-4ed4-9078-4483267a02b3_docling.json: no tables found.\n",
      "Skipped 3348953d-66e9-4cac-8675-65bb5f2ef929_docling.json: no tables found.\n",
      "Skipped 9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b_docling.json: no tables found.\n",
      "Skipped 5387a301-0af8-4e24-a197-20189f87b9ef_docling.json: no tables found.\n",
      "28_docling.json: ['From', 'To', 'Message']\n",
      "19_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "29_docling.json: ['column_header', 'col_span', 'row_span', 'start_row_offset_idx', 'end_row_offset_idx', 'start_col_offset_idx', 'end_col_offset_idx', 'text']\n",
      "18_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "15_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "40_docling.json: ['Start Col Offset Index', 'End Col Offset Index', 'Start Row Offset Index', 'End Row Offset Index', 'Text']\n",
      "12_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "24_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "23_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "6_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "1_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "36_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "31_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "13_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "41_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "14_docling.json: ['text', 'start_row_offset_idx', 'end_row_offset_idx', 'start_col_offset_idx', 'end_col_offset_idx']\n",
      "7_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "30_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "37_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "22_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "16_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "11_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "35_docling.json: ['From', 'To', 'Message']\n",
      "32_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "5_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "2_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "27_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "20_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "10_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "17_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "21_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "26_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "33_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "34_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "3_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "4_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "39_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "9_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "38_docling.json: ['Time', 'From', 'To', 'Message']\n",
      "\\n --- Headers Dicitonary --- \\n\n",
      "{'28_docling.json': ['From', 'To', 'Message'], '19_docling.json': ['Time', 'From', 'To', 'Message'], '29_docling.json': ['column_header', 'col_span', 'row_span', 'start_row_offset_idx', 'end_row_offset_idx', 'start_col_offset_idx', 'end_col_offset_idx', 'text'], '18_docling.json': ['Time', 'From', 'To', 'Message'], '15_docling.json': ['Time', 'From', 'To', 'Message'], '40_docling.json': ['Start Col Offset Index', 'End Col Offset Index', 'Start Row Offset Index', 'End Row Offset Index', 'Text'], '12_docling.json': ['Time', 'From', 'To', 'Message'], '24_docling.json': ['Time', 'From', 'To', 'Message'], '23_docling.json': ['Time', 'From', 'To', 'Message'], '6_docling.json': ['Time', 'From', 'To', 'Message'], '1_docling.json': ['Time', 'From', 'To', 'Message'], '36_docling.json': ['Time', 'From', 'To', 'Message'], '31_docling.json': ['Time', 'From', 'To', 'Message'], '13_docling.json': ['Time', 'From', 'To', 'Message'], '41_docling.json': ['Time', 'From', 'To', 'Message'], '14_docling.json': ['text', 'start_row_offset_idx', 'end_row_offset_idx', 'start_col_offset_idx', 'end_col_offset_idx'], '7_docling.json': ['Time', 'From', 'To', 'Message'], '30_docling.json': ['Time', 'From', 'To', 'Message'], '37_docling.json': ['Time', 'From', 'To', 'Message'], '22_docling.json': ['Time', 'From', 'To', 'Message'], '16_docling.json': ['Time', 'From', 'To', 'Message'], '11_docling.json': ['Time', 'From', 'To', 'Message'], '35_docling.json': ['From', 'To', 'Message'], '32_docling.json': ['Time', 'From', 'To', 'Message'], '5_docling.json': ['Time', 'From', 'To', 'Message'], '2_docling.json': ['Time', 'From', 'To', 'Message'], '27_docling.json': ['Time', 'From', 'To', 'Message'], '20_docling.json': ['Time', 'From', 'To', 'Message'], '10_docling.json': ['Time', 'From', 'To', 'Message'], '17_docling.json': ['Time', 'From', 'To', 'Message'], '21_docling.json': ['Time', 'From', 'To', 'Message'], '26_docling.json': ['Time', 'From', 'To', 'Message'], '33_docling.json': ['Time', 'From', 'To', 'Message'], '34_docling.json': ['Time', 'From', 'To', 'Message'], '3_docling.json': ['Time', 'From', 'To', 'Message'], '4_docling.json': ['Time', 'From', 'To', 'Message'], '39_docling.json': ['Time', 'From', 'To', 'Message'], '9_docling.json': ['Time', 'From', 'To', 'Message'], '38_docling.json': ['Time', 'From', 'To', 'Message']}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import ast  # To safely evaluate header string to list\n",
    "\n",
    "# Folder path\n",
    "folder_path = \"html_markdown_to_json\"\n",
    "\n",
    "# Store headers proposed for each file\n",
    "file_headers = {}\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                doc_json = json.load(f)\n",
    "\n",
    "            # Safely navigate the structure\n",
    "            tables = doc_json.get(\"tables\", [])\n",
    "            if not tables:\n",
    "                print(f\"Skipped {filename}: no tables found.\")\n",
    "                continue\n",
    "\n",
    "            cells = tables[0].get(\"data\", {}).get(\"table_cells\", [])\n",
    "            sample_cells = cells[:20]  # Use a subset to limit input size\n",
    "\n",
    "            # Format as JSON string\n",
    "            cell_str = json.dumps(sample_cells, indent=2, ensure_ascii=False)\n",
    "\n",
    "            # Prepare the prompt\n",
    "            prompt = f\"\"\"\n",
    "You are a law enforcement officer that specializes in investigating cybersecurity data leaks. Analyze the following JSON table cell data and infer what column headers would be appropriate\n",
    "if this were converted into a pandas DataFrame. \n",
    "\n",
    "Only return a list of column names that are relevant to the data. Do not include any other text or explanation.\n",
    "\n",
    "JSON data:\n",
    "{cell_str}\n",
    "\"\"\"\n",
    "\n",
    "            # Get headers from the model\n",
    "            headers_response = model(prompt)\n",
    "\n",
    "            # Convert stringified list to actual Python list safely\n",
    "            try:\n",
    "                headers_list = ast.literal_eval(headers_response.strip())\n",
    "                if isinstance(headers_list, list):\n",
    "                    file_headers[filename] = headers_list\n",
    "                else:\n",
    "                    print(f\"Warning: Invalid header format in {filename}\")\n",
    "            except Exception as eval_err:\n",
    "                print(f\"Header parsing error in {filename}: {eval_err}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Print the headers dictionary\n",
    "for fname, headers in file_headers.items():\n",
    "    print(f\"{fname}: {headers}\")\n",
    "\n",
    "# Print the headers dictionary\n",
    "print(\"\\\\n --- Headers Dicitonary --- \\\\n\")\n",
    "print(file_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a09057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "def load_docling_tables_with_llm_headers(json_path: str, headers_response: str) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads a Docling JSON export and returns a dict mapping\n",
    "    \"table_1\", \"table_2\", … → pandas.DataFrame for each table found.\n",
    "    Uses LLM-generated headers instead of inferring from content.\n",
    "    \"\"\"\n",
    "    # Parse LLM headers from string\n",
    "    try:\n",
    "        headers = ast.literal_eval(headers_response.strip())\n",
    "        if not isinstance(headers, list):\n",
    "            raise ValueError(\"LLM response did not evaluate to a list\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to parse headers from LLM output: {e}\")\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        doc = json.load(f)\n",
    "\n",
    "    tables = doc.get(\"tables\", [])\n",
    "    dfs: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    for idx, tbl in enumerate(tables, start=1):\n",
    "        # Flatten to DataFrame\n",
    "        cells = tbl[\"data\"][\"table_cells\"]\n",
    "        df_cells = pd.DataFrame(cells)\n",
    "\n",
    "        # Pivot into grid\n",
    "        grid = df_cells.pivot(\n",
    "            index=\"start_row_offset_idx\",\n",
    "            columns=\"start_col_offset_idx\",\n",
    "            values=\"text\"\n",
    "        )\n",
    "\n",
    "        # Remove the first row (assumed to be header row in JSON, already handled by LLM)\n",
    "        body = grid.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "        # Apply LLM-inferred headers\n",
    "        body.columns = headers\n",
    "\n",
    "        # Store result\n",
    "        dfs[f\"table_{idx}\"] = body\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7551b4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== table_1 (shape: (426, 4)) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-08 01:36:58</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>我昨天一天都在忙....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-08 01:37:12</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>你好久回呢？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-08 01:37:14</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>没事</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-08 01:37:19</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>我可能要下周</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-11-08 01:37:23</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>我擦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>2018-11-08 16:22:57</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>你不要跟别的人说起这些哈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>2018-11-08 16:23:06</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>嗯嗯不得</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>2018-11-08 16:23:19</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>尤其是一楼的女的......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>2018-11-08 16:23:21</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>哈哈哈哈哈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>2018-11-08 16:23:52</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>这种事肯定不得，张欢在这里都不晓得这些事，只是晓得今天客户这个事我怎么问你</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>426 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Time                From                  To  \\\n",
       "0    2018-11-08 01:36:58          qq78263462  wxid_5390224027312   \n",
       "1    2018-11-08 01:37:12          qq78263462  wxid_5390224027312   \n",
       "2    2018-11-08 01:37:14  wxid_5390224027312          qq78263462   \n",
       "3    2018-11-08 01:37:19  wxid_5390224027312          qq78263462   \n",
       "4    2018-11-08 01:37:23          qq78263462  wxid_5390224027312   \n",
       "..                   ...                 ...                 ...   \n",
       "421  2018-11-08 16:22:57          qq78263462  wxid_5390224027312   \n",
       "422  2018-11-08 16:23:06  wxid_5390224027312          qq78263462   \n",
       "423  2018-11-08 16:23:19          qq78263462  wxid_5390224027312   \n",
       "424  2018-11-08 16:23:21          qq78263462  wxid_5390224027312   \n",
       "425  2018-11-08 16:23:52  wxid_5390224027312          qq78263462   \n",
       "\n",
       "                                   Message  \n",
       "0                             我昨天一天都在忙....  \n",
       "1                                   你好久回呢？  \n",
       "2                                       没事  \n",
       "3                                   我可能要下周  \n",
       "4                                       我擦  \n",
       "..                                     ...  \n",
       "421                           你不要跟别的人说起这些哈  \n",
       "422                                   嗯嗯不得  \n",
       "423                         尤其是一楼的女的......  \n",
       "424                                  哈哈哈哈哈  \n",
       "425  这种事肯定不得，张欢在这里都不晓得这些事，只是晓得今天客户这个事我怎么问你  \n",
       "\n",
       "[426 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "JSON_PATH = \"34_docling.json\"\n",
    "\n",
    "# Example LLM output from earlier cell\n",
    "headers_response = '[\"Time\", \"From\", \"To\", \"Message\"]'\n",
    "\n",
    "# Load and display\n",
    "dataframes = load_docling_tables_with_llm_headers(JSON_PATH, headers_response)\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n=== {name} (shape: {df.shape}) ===\")\n",
    "    display(df)  # Displayed properly in Jupyter if this is the last line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11955b5",
   "metadata": {},
   "source": [
    "# **DATAFRAME TO CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "877f5868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: parsed_html_to_csv/table_1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define output directory\n",
    "OUTPUT_DIR = \"parsed_html_to_csv\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save each table to a CSV inside the directory\n",
    "for name, df in dataframes.items():\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"{name}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISoonAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
