{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6d6a8f",
   "metadata": {},
   "source": [
    "# **IMPORTS & PATHS** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "879be4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Paths and model settings\\ nZIP_PATH = 'I-Soon-data.zip'      # Path to your downloaded zip file\n",
    "ZIP_PATH = '0.zip'    # Directory to extract contents\n",
    "EXTRACT_DIR = 'I-Soon-data'        # Directory to extract contents\n",
    "ORGANIZED_DIR = 'organized_data'  # Directory to group files by extension\n",
    "MODEL_NAME = 'llama3.2'             # Local Ollama model identifier\n",
    "OUTPUT_JSON = 'parsed_md.json'    # Aggregated JSON output\n",
    "OUTPUT_CSV = 'parsed_md.csv'      # CSV output for DataFrame\n",
    "\n",
    "# Initialize Ollama LLM via LangChain\n",
    "model = OllamaLLM(model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f9f3fd",
   "metadata": {},
   "source": [
    "# **ZIP FILE EXTRACTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e6feb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction directory 'I-Soon-data' already exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(EXTRACT_DIR):\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_DIR)\n",
    "    print(f\"Extracted archive to '{EXTRACT_DIR}'\")\n",
    "else:\n",
    "    print(f\"Extraction directory '{EXTRACT_DIR}' already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a01d8",
   "metadata": {},
   "source": [
    "# **DATA TYPE CATEGORIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dec65000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organized files under 'organized_data':\n",
      " • Markdown with HTML → md/html/\n",
      " • Markdown without HTML → md/non-html/\n",
      " • Other extensions → <extension>/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# regex to detect any HTML tag\n",
    "html_re = re.compile(r'<[A-Za-z/][^>]*>')\n",
    "\n",
    "for root, _, files in os.walk(EXTRACT_DIR):\n",
    "    for fname in files:\n",
    "        ext = os.path.splitext(fname)[1].lower().lstrip('.') or 'no_extension'\n",
    "        src_path = os.path.join(root, fname)\n",
    "\n",
    "        if ext == 'md':\n",
    "            # classify Markdown by content into md/html or md/non-html\n",
    "            with open(src_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "            subfolder = 'html' if html_re.search(text) else 'non-html'\n",
    "            dest_folder = os.path.join(ORGANIZED_DIR, 'md', subfolder)\n",
    "        else:\n",
    "            # preserve original extension grouping for non-MD\n",
    "            dest_folder = os.path.join(ORGANIZED_DIR, ext)\n",
    "\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        dst_path = os.path.join(dest_folder, fname)\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "print(f\"Organized files under '{ORGANIZED_DIR}':\\n\"\n",
    "      \" • Markdown with HTML → md/html/\\n\"\n",
    "      \" • Markdown without HTML → md/non-html/\\n\"\n",
    "      \" • Other extensions → <extension>/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a6aea",
   "metadata": {},
   "source": [
    "# **MD DATA TRANSFORMATIO TO JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "321522dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted: dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md → html_markdown_to_json/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac_docling.json\n",
      "✅ Converted: 28.md → html_markdown_to_json/28_docling.json\n",
      "✅ Converted: 5.md → html_markdown_to_json/5_docling.json\n",
      "✅ Converted: 38.md → html_markdown_to_json/38_docling.json\n",
      "✅ Converted: 9d7bc879-3250-4013-ac04-5ff9bd6dff40.md → html_markdown_to_json/9d7bc879-3250-4013-ac04-5ff9bd6dff40_docling.json\n",
      "✅ Converted: 18.md → html_markdown_to_json/18_docling.json\n",
      "✅ Converted: 9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md → html_markdown_to_json/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b_docling.json\n",
      "✅ Converted: 3348953d-66e9-4cac-8675-65bb5f2ef929.md → html_markdown_to_json/3348953d-66e9-4cac-8675-65bb5f2ef929_docling.json\n",
      "✅ Converted: 1.md → html_markdown_to_json/1_docling.json\n",
      "✅ Converted: 19.md → html_markdown_to_json/19_docling.json\n",
      "✅ Converted: 07f179c5-5705-4dbd-94a7-66eed1e066b0.md → html_markdown_to_json/07f179c5-5705-4dbd-94a7-66eed1e066b0_docling.json\n",
      "✅ Converted: 29.md → html_markdown_to_json/29_docling.json\n",
      "✅ Converted: 01cdc26f-e773-4ad7-8808-d04abf16aae7.md → html_markdown_to_json/01cdc26f-e773-4ad7-8808-d04abf16aae7_docling.json\n",
      "✅ Converted: 585875ff-f8c5-4a02-acd7-fef37dc9ff11.md → html_markdown_to_json/585875ff-f8c5-4a02-acd7-fef37dc9ff11_docling.json\n",
      "✅ Converted: 4.md → html_markdown_to_json/4_docling.json\n",
      "✅ Converted: 39.md → html_markdown_to_json/39_docling.json\n",
      "✅ Converted: 64bba692-d430-440c-9f1e-2575f45770af.md → html_markdown_to_json/64bba692-d430-440c-9f1e-2575f45770af_docling.json\n",
      "✅ Converted: 178e3898-903d-47cf-bfbe-061e7dc18895.md → html_markdown_to_json/178e3898-903d-47cf-bfbe-061e7dc18895_docling.json\n",
      "✅ Converted: 16.md → html_markdown_to_json/16_docling.json\n",
      "✅ Converted: 22.md → html_markdown_to_json/22_docling.json\n",
      "✅ Converted: 32.md → html_markdown_to_json/32_docling.json\n",
      "✅ Converted: 9fe6b262-9944-417d-a0c4-9f2de1de2994.md → html_markdown_to_json/9fe6b262-9944-417d-a0c4-9f2de1de2994_docling.json\n",
      "✅ Converted: f7205881-3904-42ec-ab2c-04f36fa24785.md → html_markdown_to_json/f7205881-3904-42ec-ab2c-04f36fa24785_docling.json\n",
      "✅ Converted: 54990932-71af-48dd-9a7a-2617b1407c54.md → html_markdown_to_json/54990932-71af-48dd-9a7a-2617b1407c54_docling.json\n",
      "✅ Converted: 26.md → html_markdown_to_json/26_docling.json\n",
      "✅ Converted: 12.md → html_markdown_to_json/12_docling.json\n",
      "✅ Converted: 36.md → html_markdown_to_json/36_docling.json\n",
      "✅ Converted: eda5b003-9250-4913-b724-74cca86240af.md → html_markdown_to_json/eda5b003-9250-4913-b724-74cca86240af_docling.json\n",
      "✅ Converted: 27.md → html_markdown_to_json/27_docling.json\n",
      "✅ Converted: 13.md → html_markdown_to_json/13_docling.json\n",
      "✅ Converted: 37.md → html_markdown_to_json/37_docling.json\n",
      "✅ Converted: 5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0.md → html_markdown_to_json/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_docling.json\n",
      "✅ Converted: 17.md → html_markdown_to_json/17_docling.json\n",
      "✅ Converted: 23.md → html_markdown_to_json/23_docling.json\n",
      "✅ Converted: 33.md → html_markdown_to_json/33_docling.json\n",
      "✅ Converted: 5387a301-0af8-4e24-a197-20189f87b9ef.md → html_markdown_to_json/5387a301-0af8-4e24-a197-20189f87b9ef_docling.json\n",
      "✅ Converted: 48fd4c79-41ca-459e-a5a5-a3738e7a4af3.md → html_markdown_to_json/48fd4c79-41ca-459e-a5a5-a3738e7a4af3_docling.json\n",
      "✅ Converted: f179eb06-0c53-44df-a13f-570be23355bb.md → html_markdown_to_json/f179eb06-0c53-44df-a13f-570be23355bb_docling.json\n",
      "✅ Converted: aedc6a39-7862-4bbc-99e7-780ab3980282.md → html_markdown_to_json/aedc6a39-7862-4bbc-99e7-780ab3980282_docling.json\n",
      "✅ Converted: 24.md → html_markdown_to_json/24_docling.json\n",
      "✅ Converted: 41.md → html_markdown_to_json/41_docling.json\n",
      "✅ Converted: 10.md → html_markdown_to_json/10_docling.json\n",
      "✅ Converted: 9.md → html_markdown_to_json/9_docling.json\n",
      "✅ Converted: 34.md → html_markdown_to_json/34_docling.json\n",
      "✅ Converted: 14.md → html_markdown_to_json/14_docling.json\n",
      "✅ Converted: b3031e66-40b6-45e8-9bcd-891dc1a280da.md → html_markdown_to_json/b3031e66-40b6-45e8-9bcd-891dc1a280da_docling.json\n",
      "✅ Converted: 20.md → html_markdown_to_json/20_docling.json\n",
      "✅ Converted: 30.md → html_markdown_to_json/30_docling.json\n",
      "✅ Converted: d5ff8b65-db15-418a-b33e-169498d79110.md → html_markdown_to_json/d5ff8b65-db15-418a-b33e-169498d79110_docling.json\n",
      "✅ Converted: fe245192-1f9c-4f28-9b32-046fb7ce7e1e.md → html_markdown_to_json/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_docling.json\n",
      "✅ Converted: 15.md → html_markdown_to_json/15_docling.json\n",
      "✅ Converted: 21.md → html_markdown_to_json/21_docling.json\n",
      "✅ Converted: 31.md → html_markdown_to_json/31_docling.json\n",
      "✅ Converted: d410e4aa-fb52-4ed4-9078-4483267a02b3.md → html_markdown_to_json/d410e4aa-fb52-4ed4-9078-4483267a02b3_docling.json\n",
      "✅ Converted: 40.md → html_markdown_to_json/40_docling.json\n",
      "✅ Converted: 11.md → html_markdown_to_json/11_docling.json\n",
      "✅ Converted: 547aba02-6757-49c1-acb5-6df217cebfc7.md → html_markdown_to_json/547aba02-6757-49c1-acb5-6df217cebfc7_docling.json\n",
      "✅ Converted: 35.md → html_markdown_to_json/35_docling.json\n",
      "✅ Converted: 12756724-394c-4576-b373-7c53f1abbd94.md → html_markdown_to_json/12756724-394c-4576-b373-7c53f1abbd94_docling.json\n",
      "✅ Converted: 3.md → html_markdown_to_json/3_docling.json\n",
      "✅ Converted: e182d867-dc18-43fd-a418-26dcf784242f.md → html_markdown_to_json/e182d867-dc18-43fd-a418-26dcf784242f_docling.json\n",
      "✅ Converted: 6d7fc7b3-c892-4cb5-bd4b-a5713c089d88.md → html_markdown_to_json/6d7fc7b3-c892-4cb5-bd4b-a5713c089d88_docling.json\n",
      "✅ Converted: 912204cb-8ab7-48b8-9abf-d803f3804d08.md → html_markdown_to_json/912204cb-8ab7-48b8-9abf-d803f3804d08_docling.json\n",
      "✅ Converted: 2db27de1-d5c5-4f89-8572-da697a6329e4.md → html_markdown_to_json/2db27de1-d5c5-4f89-8572-da697a6329e4_docling.json\n",
      "✅ Converted: 7.md → html_markdown_to_json/7_docling.json\n",
      "✅ Converted: 5e5bd90e-60c5-402f-b488-750456a81a13.md → html_markdown_to_json/5e5bd90e-60c5-402f-b488-750456a81a13_docling.json\n",
      "✅ Converted: 3f451a52-d210-48d9-b56e-d28b9570bdc4.md → html_markdown_to_json/3f451a52-d210-48d9-b56e-d28b9570bdc4_docling.json\n",
      "✅ Converted: a1ba4d8b-f382-44c4-ac3f-746a44746bb4.md → html_markdown_to_json/a1ba4d8b-f382-44c4-ac3f-746a44746bb4_docling.json\n",
      "✅ Converted: 6.md → html_markdown_to_json/6_docling.json\n",
      "✅ Converted: 2.md → html_markdown_to_json/2_docling.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# Initialize converter\n",
    "converter = DocumentConverter()\n",
    "\n",
    "# Define input/output directories\n",
    "input_dir = \"organized_data/md/html\"\n",
    "output_dir = \"html_markdown_to_json\"\n",
    "failed_dir = \"failed_parsing\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(failed_dir, exist_ok=True)\n",
    "\n",
    "# Process each Markdown file\n",
    "for fname in os.listdir(input_dir):\n",
    "    if fname.endswith(\".md\"):\n",
    "        input_path = os.path.join(input_dir, fname)\n",
    "        \n",
    "        try:\n",
    "            result = converter.convert(input_path)\n",
    "            doc_dict = result.document.model_dump()\n",
    "            \n",
    "            # Output filename\n",
    "            base = os.path.splitext(fname)[0]\n",
    "            output_path = os.path.join(output_dir, f\"{base}_docling.json\")\n",
    "            \n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(doc_dict, f, indent=2)\n",
    "            \n",
    "            print(f\"✅ Converted: {fname} → {output_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Move to failed folder\n",
    "            shutil.move(input_path, os.path.join(failed_dir, fname))\n",
    "            print(f\"❌ Failed to convert {fname}: {e} (moved to {failed_dir})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232dc4db",
   "metadata": {},
   "source": [
    "# **JSON TO DATAFRAME WITH AI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01e08cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "def load_docling_tables_with_llm_headers(json_path: str, headers_response: str) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads a Docling JSON export and returns a dict mapping\n",
    "    \"table_1\", \"table_2\", … → pandas.DataFrame for each table found.\n",
    "    Uses LLM-generated headers instead of inferring from content.\n",
    "    \"\"\"\n",
    "    # Parse LLM headers from string\n",
    "    try:\n",
    "        headers = ast.literal_eval(headers_response.strip())\n",
    "        if not isinstance(headers, list):\n",
    "            raise ValueError(\"LLM response did not evaluate to a list\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to parse headers from LLM output: {e}\")\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        doc = json.load(f)\n",
    "\n",
    "    tables = doc.get(\"tables\", [])\n",
    "    dfs: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    for idx, tbl in enumerate(tables, start=1):\n",
    "        # Flatten to DataFrame\n",
    "        cells = tbl[\"data\"][\"table_cells\"]\n",
    "        df_cells = pd.DataFrame(cells)\n",
    "\n",
    "        # Pivot into grid\n",
    "        grid = df_cells.pivot(\n",
    "            index=\"start_row_offset_idx\",\n",
    "            columns=\"start_col_offset_idx\",\n",
    "            values=\"text\"\n",
    "        )\n",
    "\n",
    "        # Remove the first row (assumed to be header row in JSON, already handled by LLM)\n",
    "        body = grid.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "        # Apply LLM-inferred headers\n",
    "        body.columns = headers\n",
    "\n",
    "        # Store result\n",
    "        dfs[f\"table_{idx}\"] = body\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7551b4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== table_1 (shape: (426, 4)) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-08 01:36:58</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>我昨天一天都在忙....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-08 01:37:12</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>你好久回呢？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-08 01:37:14</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>没事</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-08 01:37:19</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>我可能要下周</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-11-08 01:37:23</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>我擦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>2018-11-08 16:22:57</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>你不要跟别的人说起这些哈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>2018-11-08 16:23:06</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>嗯嗯不得</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>2018-11-08 16:23:19</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>尤其是一楼的女的......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>2018-11-08 16:23:21</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>哈哈哈哈哈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>2018-11-08 16:23:52</td>\n",
       "      <td>wxid_5390224027312</td>\n",
       "      <td>qq78263462</td>\n",
       "      <td>这种事肯定不得，张欢在这里都不晓得这些事，只是晓得今天客户这个事我怎么问你</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>426 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Time                From                  To  \\\n",
       "0    2018-11-08 01:36:58          qq78263462  wxid_5390224027312   \n",
       "1    2018-11-08 01:37:12          qq78263462  wxid_5390224027312   \n",
       "2    2018-11-08 01:37:14  wxid_5390224027312          qq78263462   \n",
       "3    2018-11-08 01:37:19  wxid_5390224027312          qq78263462   \n",
       "4    2018-11-08 01:37:23          qq78263462  wxid_5390224027312   \n",
       "..                   ...                 ...                 ...   \n",
       "421  2018-11-08 16:22:57          qq78263462  wxid_5390224027312   \n",
       "422  2018-11-08 16:23:06  wxid_5390224027312          qq78263462   \n",
       "423  2018-11-08 16:23:19          qq78263462  wxid_5390224027312   \n",
       "424  2018-11-08 16:23:21          qq78263462  wxid_5390224027312   \n",
       "425  2018-11-08 16:23:52  wxid_5390224027312          qq78263462   \n",
       "\n",
       "                                   Message  \n",
       "0                             我昨天一天都在忙....  \n",
       "1                                   你好久回呢？  \n",
       "2                                       没事  \n",
       "3                                   我可能要下周  \n",
       "4                                       我擦  \n",
       "..                                     ...  \n",
       "421                           你不要跟别的人说起这些哈  \n",
       "422                                   嗯嗯不得  \n",
       "423                         尤其是一楼的女的......  \n",
       "424                                  哈哈哈哈哈  \n",
       "425  这种事肯定不得，张欢在这里都不晓得这些事，只是晓得今天客户这个事我怎么问你  \n",
       "\n",
       "[426 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "JSON_PATH = \"34_docling.json\"\n",
    "\n",
    "# Example LLM output from earlier cell\n",
    "headers_response = '[\"Time\", \"From\", \"To\", \"Message\"]'\n",
    "\n",
    "# Load and display\n",
    "dataframes = load_docling_tables_with_llm_headers(JSON_PATH, headers_response)\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n=== {name} (shape: {df.shape}) ===\")\n",
    "    display(df)  # Displayed properly in Jupyter if this is the last line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11955b5",
   "metadata": {},
   "source": [
    "# **DATAFRAME TO CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "877f5868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: parsed_html_to_csv/table_1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define output directory\n",
    "OUTPUT_DIR = \"parsed_html_to_csv\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save each table to a CSV inside the directory\n",
    "for name, df in dataframes.items():\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"{name}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISoonAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
