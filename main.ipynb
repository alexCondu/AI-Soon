{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6559c645-340c-4646-b646-39b14bcd9932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '0' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Directory to delete\n",
    "# EXTRACT_DIR = '0'\n",
    "\n",
    "# # Check if the directory exists and delete it\n",
    "# if os.path.isdir(EXTRACT_DIR):\n",
    "#     shutil.rmtree(EXTRACT_DIR)\n",
    "#     print(f\"Directory '{EXTRACT_DIR}' has been deleted.\")\n",
    "# else:\n",
    "#     print(f\"Directory '{EXTRACT_DIR}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5776e2db-42ed-4aa2-a497-1a545810a7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores:  144\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "print(\"CPU cores: \", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2befe2f0-40aa-4ffa-8a73-7443ff5760b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted to: 0\n"
     ]
    }
   ],
   "source": [
    "def extract_zip(zip_path):\n",
    "\n",
    "    if not zipfile.is_zipfile(zip_path):\n",
    "        raise ValueError(f\"The file at {zip_path} is not a valid ZIP archive.\")\n",
    "    \n",
    "    # Determine the output directory name from the zip file name\n",
    "    base_dir = os.path.dirname(zip_path)\n",
    "    zip_filename = os.path.basename(zip_path)\n",
    "    folder_name = os.path.splitext(zip_filename)[0]\n",
    "    extract_to = os.path.join(base_dir, folder_name)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(extract_to):\n",
    "        os.makedirs(extract_to)\n",
    "\n",
    "    # Extract the ZIP file\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(path=extract_to)\n",
    "\n",
    "    return extract_to\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    zip_file_path = '0.zip'  # Replace with your actual ZIP file path\n",
    "    extracted_dir = extract_zip(zip_file_path)\n",
    "    print(f\"Extracted to: {extracted_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "950a34b4-50f3-486e-b781-a1dc9e43c458",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: 0/__MACOSX\n"
     ]
    }
   ],
   "source": [
    "# Parent directory\n",
    "parent_directory = \"0\"\n",
    "\n",
    "# Full path to the __MACOSX folder\n",
    "macosx_folder = os.path.join(parent_directory, \"__MACOSX\")\n",
    "\n",
    "# Check if __MACOSX exists and remove it - creates issues when analyzing the data, and its not needed, made automatically by MacOS\n",
    "if os.path.exists(macosx_folder) and os.path.isdir(macosx_folder):\n",
    "    shutil.rmtree(macosx_folder)\n",
    "    print(f\"Deleted: {macosx_folder}\")\n",
    "else:\n",
    "    print(f\"Folder not found: {macosx_folder}\")\n",
    "\n",
    "# Organize files by extension into subfolders\n",
    "for root, dirs, files in os.walk(parent_directory):\n",
    "    for file in files:\n",
    "        # Skip hidden files and __MACOSX if any reappear\n",
    "        if file.startswith('.') or '__MACOSX' in root:\n",
    "            continue\n",
    "\n",
    "        # Get the file extension (in lowercase, without the dot)\n",
    "        file_extension = os.path.splitext(file)[1].lower().lstrip('.')\n",
    "        if not file_extension:\n",
    "            file_extension = \"no_extension\"\n",
    "\n",
    "        # Define the new subfolder path\n",
    "        subfolder_path = os.path.join(parent_directory, file_extension)\n",
    "\n",
    "        # Create the subfolder if it doesn't exist\n",
    "        os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "        # Define source and destination paths\n",
    "        source_path = os.path.join(root, file)\n",
    "        destination_path = os.path.join(subfolder_path, file)\n",
    "\n",
    "        # Move the file if source and destination are not the same\n",
    "        if os.path.abspath(source_path) != os.path.abspath(destination_path):\n",
    "            shutil.move(source_path, destination_path)\n",
    "\n",
    "# Remove any empty folders within the parent directory\n",
    "for dirpath, dirnames, filenames in os.walk(parent_directory, topdown=False):\n",
    "    if not dirnames and not filenames:\n",
    "        try:\n",
    "            os.rmdir(dirpath)\n",
    "            print(f\"Removed empty folder: {dirpath}\")\n",
    "        except OSError:\n",
    "            pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ab45fc5-01f0-4dd6-9c70-1ad25b1212a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '0/0' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "# Delete Duplicate of 0 - empty\n",
    "\n",
    "# Directory to delete\n",
    "EXTRACT_DIR = '0/0'\n",
    "\n",
    "# Check if the directory exists and delete it\n",
    "if os.path.isdir(EXTRACT_DIR):\n",
    "    shutil.rmtree(EXTRACT_DIR)\n",
    "    print(f\"Directory '{EXTRACT_DIR}' has been deleted.\")\n",
    "else:\n",
    "    print(f\"Directory '{EXTRACT_DIR}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f80802-63db-4c12-9401-d9d6246f556a",
   "metadata": {},
   "source": [
    "# Dataframe with all file location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88ee5d4f-ee82-49b3-bb69-9b052f1952f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>md</th>\n",
       "      <th>png</th>\n",
       "      <th>log</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md</td>\n",
       "      <td>0/png/64bba692-d430-440c-9f1e-2575f45770af_6.png</td>\n",
       "      <td>0/log/77010155050.log</td>\n",
       "      <td>0/txt/IDNET.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0/md/28.md</td>\n",
       "      <td>0/png/12756724-394c-4576-b373-7c53f1abbd94_0.png</td>\n",
       "      <td>0/log/77753527617.log</td>\n",
       "      <td>0/txt/IDTV.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0/md/5.md</td>\n",
       "      <td>0/png/f179eb06-0c53-44df-a13f-570be23355bb_1.png</td>\n",
       "      <td>0/log/tele2-lbs.log</td>\n",
       "      <td>0/txt/beeline-77774042222.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0/md/38.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_24.png</td>\n",
       "      <td>0/log/tele2-cdr.log</td>\n",
       "      <td>0/txt/beeline-77051056626.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_30.png</td>\n",
       "      <td>0/log/tele2-crm.log</td>\n",
       "      <td>0/txt/beeline-crm.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0/md/18.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_18.png</td>\n",
       "      <td>0/log/77783030133.log</td>\n",
       "      <td>0/txt/UBSCRIBER.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md</td>\n",
       "      <td>0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/beeline-cdr.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md</td>\n",
       "      <td>0/png/5387a301-0af8-4e24-a197-20189f87b9ef_8.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/CRM.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0/md/1.md</td>\n",
       "      <td>0/png/0-32eb7662-f212-4811-a7c1-1cfeb121cd99.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/LAC.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0/md/19.md</td>\n",
       "      <td>0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_11.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/beeline-lbs.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md</td>\n",
       "      <td>0/png/0-7150f512-e7a2-4f2c-86bc-58b671b25ba9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0/txt/Φ»¥σìò.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0/md/29.md</td>\n",
       "      <td>0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_8.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md</td>\n",
       "      <td>0/png/eda5b003-9250-4913-b724-74cca86240af_13.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0/md/585875ff-f8c5-4a02-acd7-fef37dc9ff11.md</td>\n",
       "      <td>0/png/eda5b003-9250-4913-b724-74cca86240af_12.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0/md/4.md</td>\n",
       "      <td>0/png/adaf869e-920a-4a17-91bd-e2ef3125c10e.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0/md/39.md</td>\n",
       "      <td>0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0/md/16.md</td>\n",
       "      <td>0/png/32eb7662-f212-4811-a7c1-1cfeb121cd99.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0/md/64bba692-d430-440c-9f1e-2575f45770af.md</td>\n",
       "      <td>0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_10.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0/md/178e3898-903d-47cf-bfbe-061e7dc18895.md</td>\n",
       "      <td>0/png/5387a301-0af8-4e24-a197-20189f87b9ef_9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0/md/22.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_19.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0/md/32.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_31.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0/md/26.md</td>\n",
       "      <td>0/png/f179eb06-0c53-44df-a13f-570be23355bb_0.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0/md/9fe6b262-9944-417d-a0c4-9f2de1de2994.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_25.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0/md/f7205881-3904-42ec-ab2c-04f36fa24785.md</td>\n",
       "      <td>0/png/7150f512-e7a2-4f2c-86bc-58b671b25ba9.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0/md/54990932-71af-48dd-9a7a-2617b1407c54.md</td>\n",
       "      <td>0/png/12756724-394c-4576-b373-7c53f1abbd94_1.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0/md/12.md</td>\n",
       "      <td>0/png/64bba692-d430-440c-9f1e-2575f45770af_7.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0/md/36.md</td>\n",
       "      <td>0/png/6848748d-2881-4c26-b153-fcd5373d2f1c.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0/md/eda5b003-9250-4913-b724-74cca86240af.md</td>\n",
       "      <td>0/png/12756724-394c-4576-b373-7c53f1abbd94_3.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0/md/27.md</td>\n",
       "      <td>0/png/64bba692-d430-440c-9f1e-2575f45770af_5.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0/md/13.md</td>\n",
       "      <td>0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_33.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              md  \\\n",
       "0   0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md   \n",
       "1                                     0/md/28.md   \n",
       "2                                      0/md/5.md   \n",
       "3                                     0/md/38.md   \n",
       "4   0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md   \n",
       "5                                     0/md/18.md   \n",
       "6   0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md   \n",
       "7   0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md   \n",
       "8                                      0/md/1.md   \n",
       "9                                     0/md/19.md   \n",
       "10  0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md   \n",
       "11                                    0/md/29.md   \n",
       "12  0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md   \n",
       "13  0/md/585875ff-f8c5-4a02-acd7-fef37dc9ff11.md   \n",
       "14                                     0/md/4.md   \n",
       "15                                    0/md/39.md   \n",
       "16                                    0/md/16.md   \n",
       "17  0/md/64bba692-d430-440c-9f1e-2575f45770af.md   \n",
       "18  0/md/178e3898-903d-47cf-bfbe-061e7dc18895.md   \n",
       "19                                    0/md/22.md   \n",
       "20                                    0/md/32.md   \n",
       "21                                    0/md/26.md   \n",
       "22  0/md/9fe6b262-9944-417d-a0c4-9f2de1de2994.md   \n",
       "23  0/md/f7205881-3904-42ec-ab2c-04f36fa24785.md   \n",
       "24  0/md/54990932-71af-48dd-9a7a-2617b1407c54.md   \n",
       "25                                    0/md/12.md   \n",
       "26                                    0/md/36.md   \n",
       "27  0/md/eda5b003-9250-4913-b724-74cca86240af.md   \n",
       "28                                    0/md/27.md   \n",
       "29                                    0/md/13.md   \n",
       "\n",
       "                                                  png                    log  \\\n",
       "0    0/png/64bba692-d430-440c-9f1e-2575f45770af_6.png  0/log/77010155050.log   \n",
       "1    0/png/12756724-394c-4576-b373-7c53f1abbd94_0.png  0/log/77753527617.log   \n",
       "2    0/png/f179eb06-0c53-44df-a13f-570be23355bb_1.png    0/log/tele2-lbs.log   \n",
       "3   0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_24.png    0/log/tele2-cdr.log   \n",
       "4   0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_30.png    0/log/tele2-crm.log   \n",
       "5   0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_18.png  0/log/77783030133.log   \n",
       "6    0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e.png                    NaN   \n",
       "7    0/png/5387a301-0af8-4e24-a197-20189f87b9ef_8.png                    NaN   \n",
       "8    0/png/0-32eb7662-f212-4811-a7c1-1cfeb121cd99.png                    NaN   \n",
       "9   0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_11.png                    NaN   \n",
       "10   0/png/0-7150f512-e7a2-4f2c-86bc-58b671b25ba9.png                    NaN   \n",
       "11   0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_8.png                    NaN   \n",
       "12  0/png/eda5b003-9250-4913-b724-74cca86240af_13.png                    NaN   \n",
       "13  0/png/eda5b003-9250-4913-b724-74cca86240af_12.png                    NaN   \n",
       "14     0/png/adaf869e-920a-4a17-91bd-e2ef3125c10e.png                    NaN   \n",
       "15   0/png/fe245192-1f9c-4f28-9b32-046fb7ce7e1e_9.png                    NaN   \n",
       "16     0/png/32eb7662-f212-4811-a7c1-1cfeb121cd99.png                    NaN   \n",
       "17  0/png/912204cb-8ab7-48b8-9abf-d803f3804d08_10.png                    NaN   \n",
       "18   0/png/5387a301-0af8-4e24-a197-20189f87b9ef_9.png                    NaN   \n",
       "19  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_19.png                    NaN   \n",
       "20  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_31.png                    NaN   \n",
       "21   0/png/f179eb06-0c53-44df-a13f-570be23355bb_0.png                    NaN   \n",
       "22  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_25.png                    NaN   \n",
       "23     0/png/7150f512-e7a2-4f2c-86bc-58b671b25ba9.png                    NaN   \n",
       "24   0/png/12756724-394c-4576-b373-7c53f1abbd94_1.png                    NaN   \n",
       "25   0/png/64bba692-d430-440c-9f1e-2575f45770af_7.png                    NaN   \n",
       "26     0/png/6848748d-2881-4c26-b153-fcd5373d2f1c.png                    NaN   \n",
       "27   0/png/12756724-394c-4576-b373-7c53f1abbd94_3.png                    NaN   \n",
       "28   0/png/64bba692-d430-440c-9f1e-2575f45770af_5.png                    NaN   \n",
       "29  0/png/5a6b122c-39c1-4581-8c1f-2d6f36a9f8a0_33.png                    NaN   \n",
       "\n",
       "                              txt  \n",
       "0                 0/txt/IDNET.txt  \n",
       "1                  0/txt/IDTV.txt  \n",
       "2   0/txt/beeline-77774042222.txt  \n",
       "3   0/txt/beeline-77051056626.txt  \n",
       "4           0/txt/beeline-crm.txt  \n",
       "5             0/txt/UBSCRIBER.txt  \n",
       "6           0/txt/beeline-cdr.txt  \n",
       "7                   0/txt/CRM.txt  \n",
       "8                   0/txt/LAC.txt  \n",
       "9           0/txt/beeline-lbs.txt  \n",
       "10               0/txt/Φ»¥σìò.txt  \n",
       "11                            NaN  \n",
       "12                            NaN  \n",
       "13                            NaN  \n",
       "14                            NaN  \n",
       "15                            NaN  \n",
       "16                            NaN  \n",
       "17                            NaN  \n",
       "18                            NaN  \n",
       "19                            NaN  \n",
       "20                            NaN  \n",
       "21                            NaN  \n",
       "22                            NaN  \n",
       "23                            NaN  \n",
       "24                            NaN  \n",
       "25                            NaN  \n",
       "26                            NaN  \n",
       "27                            NaN  \n",
       "28                            NaN  \n",
       "29                            NaN  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_folder_file_dataframe(root_dir):\n",
    "    folders = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]\n",
    "\n",
    "    series_list = []\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        files = [\n",
    "            os.path.join(folder_path, file)\n",
    "            for file in os.listdir(folder_path)\n",
    "            if os.path.isfile(os.path.join(folder_path, file)) and not file.startswith('.')\n",
    "        ]\n",
    "        s = pd.Series(files, name=folder)\n",
    "        series_list.append(s)\n",
    "\n",
    "    df = pd.concat(series_list, axis=1)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "root_directory = \"0\"\n",
    "df = get_folder_file_dataframe(root_directory)\n",
    "df.head(30)        # Show first 5 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b011fbc-09b2-4ccd-b075-62306871df42",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Use LLM to classify the source files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64ba6064-3151-4166-a432-142d9e610d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying files: 100%|██████████| 70/70 [02:32<00:00,  2.18s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>other</th>\n",
       "      <th>chats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md</td>\n",
       "      <td>0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md</td>\n",
       "      <td>0/md/28.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md</td>\n",
       "      <td>None</td>\n",
       "      <td>0/md/5.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md</td>\n",
       "      <td>None</td>\n",
       "      <td>0/md/38.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md</td>\n",
       "      <td>None</td>\n",
       "      <td>0/md/18.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md</td>\n",
       "      <td>None</td>\n",
       "      <td>0/md/1.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         images  \\\n",
       "0  0/md/dbc9c90e-a3e6-4d71-bb93-5fb8394095ac.md   \n",
       "1  0/md/9fd06037-11f1-4ad5-9a7d-cbfb3fa4193b.md   \n",
       "2  0/md/3348953d-66e9-4cac-8675-65bb5f2ef929.md   \n",
       "3  0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md   \n",
       "4  0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md   \n",
       "\n",
       "                                          other       chats  \n",
       "0  0/md/9d7bc879-3250-4013-ac04-5ff9bd6dff40.md  0/md/28.md  \n",
       "1                                          None   0/md/5.md  \n",
       "2                                          None  0/md/38.md  \n",
       "3                                          None  0/md/18.md  \n",
       "4                                          None   0/md/1.md  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Step 1: LLM setup ===\n",
    "llm = Ollama(model=\"llama3.1:8b\")\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"\"\"\n",
    "You are analyzing the content of a file.\n",
    "\n",
    "Markdown content:\n",
    "\\\"\\\"\\\"\n",
    "{content}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "1. Classify the content into one of the following categories ONLY: chats, images, other.\n",
    "2. State your confidence in the classification as one of: high, medium, or low.\n",
    "3. I will have to make a csv, please give me a list of headers based on the content. E.g. \"[<header_name>, <header_name2>, etc.]\"\n",
    "\n",
    "Respond in the following format:\n",
    "Category: <chats|images|other>\n",
    "Confidence: <high|medium|low>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# === Step 2: Read first 20 lines of a file ===\n",
    "def preprocess_first_20_lines(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [line.strip() for _, line in zip(range(20), f)]\n",
    "            content = \" \".join(lines)\n",
    "        return file_path, content\n",
    "    except Exception:\n",
    "        return file_path, \"\"\n",
    "\n",
    "# === Step 3: Use only the first column of the DataFrame ===\n",
    "first_column = df.columns[0]\n",
    "file_paths = df[first_column].dropna().unique().tolist()\n",
    "\n",
    "# === Step 4: Preprocess content\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    file_data = list(executor.map(preprocess_first_20_lines, file_paths))\n",
    "\n",
    "# === Step 5: Classify and collect by category\n",
    "valid_categories = {\"chats\", \"images\", \"other\"}\n",
    "categorized_files = {cat: [] for cat in valid_categories}\n",
    "\n",
    "for file_path, content in tqdm(file_data, desc=\"Classifying files\"):\n",
    "    if not content:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        response = chain.run(content=content).strip().lower()\n",
    "        lines = response.splitlines()\n",
    "\n",
    "        category = next((line.replace(\"category:\", \"\").strip() for line in lines if line.startswith(\"category:\")), \"\")\n",
    "        if category not in valid_categories:\n",
    "            category = \"other\"\n",
    "\n",
    "        categorized_files[category].append(file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# === Step 6: Convert to DataFrame (columns = categories)\n",
    "max_len = max(len(v) for v in categorized_files.values())\n",
    "padded = {k: v + [None] * (max_len - len(v)) for k, v in categorized_files.items()}\n",
    "result_df = pd.DataFrame(padded)\n",
    "\n",
    "result_df.to_csv(\"classified_by_category.csv\", index=False)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ae9479b-52e3-48d0-867b-55aa68bf1acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e96cc02-e59d-4113-b35e-857490c25736",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Script to check for cross refernced files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e94a6ab-a1d9-4c11-b275-a753fc8182f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Flatten all file paths from the DataFrame\n",
    "file_paths = df.stack().dropna().unique().tolist()\n",
    "\n",
    "# Read all readable files into memory\n",
    "def read_file(path):\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except:\n",
    "        return None  # Skip unreadable or binary files\n",
    "\n",
    "file_contents = {path: read_file(path) for path in file_paths}\n",
    "\n",
    "# Detect cross-references based on filenames\n",
    "reference_map = defaultdict(list)\n",
    "\n",
    "for source_path, content in file_contents.items():\n",
    "    if content is None:\n",
    "        continue\n",
    "\n",
    "    for target_path in file_paths:\n",
    "        if target_path == source_path:\n",
    "            continue\n",
    "\n",
    "        target_filename = os.path.basename(target_path)\n",
    "        pattern = re.escape(target_filename)\n",
    "\n",
    "        if re.search(rf'\\b{pattern}\\b', content):\n",
    "            reference_map[source_path].append(target_path)\n",
    "\n",
    "# Step 4: Build and save the reference map as a DataFrame\n",
    "ref_df = pd.DataFrame([\n",
    "    {\"source_file\": src, \"mentions\": tgt}\n",
    "    for src, tgts in reference_map.items()\n",
    "    for tgt in tgts\n",
    "])\n",
    "\n",
    "ref_summary = ref_df.groupby(\"source_file\")[\"mentions\"].apply(list).reset_index()\n",
    "ref_summary.to_csv(\"file_reference_map.csv\", index=False)\n",
    "ref_summary.head(10)\n",
    "\n",
    "print(df.stack().nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e97ef6-e8d9-4939-a5b5-65d3a627d19b",
   "metadata": {},
   "source": [
    "# Check for more linkages in the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cced8099-6c5f-4217-946b-9c23c600c015",
   "metadata": {},
   "source": [
    "The script looks at the files listed in the mentions column and:\n",
    "1. Opens the file\n",
    "2. Uses regex to detect if it mentions any other known file in your dataset\n",
    "3. If such links are found:\n",
    "4. It adds an entry to the Linkage column in the form of a dictionary:\n",
    "5. Key = the file being scanned from mentions\n",
    "6. Value = list of filenames that it references\n",
    "\n",
    "If a file doesn’t reference anything else, it’s skipped.\n",
    "If none of the files in mentions reference anything, Linkage will be an empty {}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fe46121c-4ff4-4e45-b248-8edd4d12be74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    source_file  \\\n",
      "0  0/md/01cdc26f-e773-4ad7-8808-d04abf16aae7.md   \n",
      "1  0/md/07f179c5-5705-4dbd-94a7-66eed1e066b0.md   \n",
      "2                                    0/md/10.md   \n",
      "3  0/md/12756724-394c-4576-b373-7c53f1abbd94.md   \n",
      "4                                    0/md/13.md   \n",
      "\n",
      "                                            mentions  \\\n",
      "0  [0/png/01cdc26f-e773-4ad7-8808-d04abf16aae7_1_...   \n",
      "1  [0/png/07f179c5-5705-4dbd-94a7-66eed1e066b0_0....   \n",
      "2  [0/png/0-6bcc0131-e4ad-421e-bb1f-d8ebe5eeec7b....   \n",
      "3  [0/png/12756724-394c-4576-b373-7c53f1abbd94_0....   \n",
      "4  [0/png/0-adaf869e-920a-4a17-91bd-e2ef3125c10e....   \n",
      "\n",
      "                                             Linkage  \n",
      "0                                                 {}  \n",
      "1                                                 {}  \n",
      "2  {'0/md/12756724-394c-4576-b373-7c53f1abbd94.md...  \n",
      "3                                                 {}  \n",
      "4  {'0/md/585875ff-f8c5-4a02-acd7-fef37dc9ff11.md...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Combine all soruce files and all mentioned files in a list\n",
    "file_paths = pd.Series(\n",
    "    ref_summary['mentions'].explode().dropna().tolist() + \n",
    "    ref_summary['source_file'].dropna().tolist()\n",
    ").unique()\n",
    "\n",
    "# The function reads a file and returns its text content\n",
    "def read_file(path):\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Create the dictionary key: parent value: child\n",
    "file_contents = {path: read_file(path) for path in file_paths}\n",
    "\n",
    "# Check fir mentions\n",
    "all_filenames = set(os.path.basename(p) for p in file_paths)\n",
    "\n",
    "# The function returns a list of mentioned files from a file\n",
    "def extract_mentions(file_path):\n",
    "    content = file_contents.get(file_path)\n",
    "    if not content:\n",
    "        return []\n",
    "    \n",
    "    mentioned = []\n",
    "    for fname in all_filenames:\n",
    "        pattern = rf'\\b{re.escape(fname)}\\b'\n",
    "        if re.search(pattern, content):\n",
    "            mentioned.append(fname)\n",
    "    return mentioned\n",
    "\n",
    "# Builds the dictionary for the afferent row\n",
    "def build_linkage_dict(mention_list):\n",
    "    if not isinstance(mention_list, list):\n",
    "        return {}\n",
    "    result = {}\n",
    "    for file in mention_list:\n",
    "        if isinstance(file, str):\n",
    "            mentioned_files = extract_mentions(file)\n",
    "            if mentioned_files:\n",
    "                result[file] = mentioned_files\n",
    "    return result if result else {}\n",
    "\n",
    "\n",
    "ref_summary['Linkage'] = ref_summary['mentions'].apply(build_linkage_dict)\n",
    "ref_summary.to_csv(\"ref_summary_with_file_linkages.csv\", index=False)\n",
    "\n",
    "# Preview\n",
    "print(ref_summary[['source_file', 'mentions', 'Linkage']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b00c5-bfe0-4597-bccd-a1c036719cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (IsoonAI)",
   "language": "python",
   "name": "isoonai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
